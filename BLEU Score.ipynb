{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b280d7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d8d5bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdeb9657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "references = [[['this', 'is', 'a', 'test'], ['this', 'is' 'test']]]\n",
    "candidates = [['this', 'is', 'a', 'test']]\n",
    "score = corpus_bleu(references, candidates)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6dcb58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbb56c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.9/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "reference = [['this', 'is', 'small', 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "score = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da1247e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual 1-gram: 0.778801\n",
      "Individual 2-gram: 0.519201\n",
      "Individual 3-gram: 0.389400\n",
      "Individual 4-gram: 0.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reference = [['this', 'is', 'a','small', 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "print('Individual 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Individual 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 1, 0, 0)))\n",
    "print('Individual 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 1, 0)))\n",
    "print('Individual 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0, 0, 0, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55fadeca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0547686614863434e-154\n"
     ]
    }
   ],
   "source": [
    "reference = [['this', 'is', 'small', 'test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "score = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02da7ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative 1-gram: 0.750000\n",
      "Cumulative 2-gram: 0.500000\n",
      "Cumulative 3-gram: 0.000000\n",
      "Cumulative 4-gram: 0.000000\n"
     ]
    }
   ],
   "source": [
    "reference = [['this', 'is', 'small', 'test'],['this', 'is','test']]\n",
    "candidate = ['this', 'is', 'a', 'test']\n",
    "print('Cumulative 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Cumulative 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)))\n",
    "print('Cumulative 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)))\n",
    "print('Cumulative 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65574c41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa92d422",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences:\n"
     ]
    }
   ],
   "source": [
    "def read_sentences_from_file(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        document = file.read()\n",
    "        sentences = document.split(\"<end>\")\n",
    "        # Remove any leading or trailing whitespace from each sentence\n",
    "        sentences = [sentence.strip() for sentence in sentences]\n",
    "        # Remove empty sentences\n",
    "        sentences = [sentence for sentence in sentences if sentence]\n",
    "        # Tokenize each sentence\n",
    "        tokenized_sentences = [sentence.split()[1:] for sentence in sentences]\n",
    "    return tokenized_sentences\n",
    "\n",
    "filename = \"data/cleaned_3.txt\"  # Change this to your file's name\n",
    "sentences = read_sentences_from_file(filename)\n",
    "print(\"Sentences:\")\n",
    "#for sentence in sentences:\n",
    "#    print(sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f50a183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'my', 'name', 'is']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'hello my name is'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5be30b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative 1-gram: 1.000000\n",
      "Cumulative 2-gram: 1.000000\n",
      "Cumulative 3-gram: 1.000000\n",
      "Cumulative 4-gram: 1.000000\n"
     ]
    }
   ],
   "source": [
    "reference = sentences\n",
    "inp = \"Samuel: Ok hours works anymore of the at small in doesn’t \\\n",
    "Ignore able we out txt it are I plots if finish else showing \\\n",
    "Gosh it acivation, locally connection am just it.95 accidentally \\\n",
    "worse 0.01\"\n",
    "\n",
    "#inp = \"Jeremy: care order i GrEaT and results trying rlly same? the \\\n",
    "#to pa we doing that 1.1e-4 and linear @Jonathan 5 training \\\n",
    "#item2. small don’t multiple min? faster gotta\"\n",
    "\n",
    "inp = \"Ester: works I’m would i’ve ended like a Improving that 0.72 \\\n",
    "yup with how Yuppp! like me my ill I’m don’t on I 1e-4 \\\n",
    "everything\"\n",
    "\n",
    "inp = \"Samuel: Ok hours works anymore of the at small in doesn’t \\\n",
    "Ignore able we out txt it are I plots if finish else showing \\\n",
    "Gosh it acivation, locally connection am just it.95 accidentally \\\n",
    "worse 0.01 \\\n",
    "Jeremy: care order i GrEaT and results trying rlly same? the \\\n",
    "to pa we doing that 1.1e-4 and linear @Jonathan 5 training \\\n",
    "item2. small don’t multiple min? faster gotta \\\n",
    "Ester: works I’m would i’ve ended like a Improving that 0.72 \\\n",
    "yup with how Yuppp! like me my ill I’m don’t on I 1e-4 \\\n",
    "everything\"\n",
    "\n",
    "inp = 'I can start working on pa2 again on Monday! And then work on the report Tues and Wednesday. Down to work on both Q4 and 5 if time permits, but if time is tight, I’ll piggy back off of existing Q4 and just work on Q5!'\n",
    "\n",
    "\n",
    "candidate = inp.split()\n",
    "print('Cumulative 1-gram: %f' % sentence_bleu(reference, candidate, weights=(1, 0, 0, 0)))\n",
    "print('Cumulative 2-gram: %f' % sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0)))\n",
    "print('Cumulative 3-gram: %f' % sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0)))\n",
    "print('Cumulative 4-gram: %f' % sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c605f71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77546124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install bert-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "496e870f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'evaluate' from 'bert_score' (/Users/jonathancheung/opt/anaconda3/lib/python3.8/site-packages/bert_score/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbert_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m evaluate\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[1;32m      3\u001b[0m bertscore \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbertscore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'evaluate' from 'bert_score' (/Users/jonathancheung/opt/anaconda3/lib/python3.8/site-packages/bert_score/__init__.py)"
     ]
    }
   ],
   "source": [
    "from bert_score import evaluate\n",
    "from evaluate import load\n",
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64e50ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c081293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd128fe494ee475cb08dce42c3b5cea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611cc417e1c84178b838abc08f6a7b1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.49 seconds, 4.10 sentences/sec\n",
      "Precision: 0.985129\n",
      "Recall: 0.977599\n",
      "F1 score: 0.981335\n"
     ]
    }
   ],
   "source": [
    "#from bert_score import score\n",
    "\n",
    "# Example reference and candidate sentences\n",
    "references = [\"The cat sat on the mat.\", \"I love natural language processing!\"]\n",
    "candidates = [\"The cat sat on the mat.\", \"I love deep learning!\"]\n",
    "\n",
    "# Compute BERTScore\n",
    "P, R, F1 = score(candidates, references, lang='en', verbose=True)\n",
    "\n",
    "# Print BERTScore\n",
    "print(f\"Precision: {P.mean().item():.6f}\")\n",
    "print(f\"Recall: {R.mean().item():.6f}\")\n",
    "print(f\"F1 score: {F1.mean().item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "613b78ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_from_file_2(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        document = file.read()\n",
    "        sentences = document.split(\"<end>\")\n",
    "        # Remove any leading or trailing whitespace from each sentence\n",
    "        sentences = [sentence.strip() for sentence in sentences]\n",
    "        # Remove empty sentences\n",
    "        sentences = [sentence for sentence in sentences if sentence]\n",
    "        # Tokenize each sentence\n",
    "        tokenized_sentences = [sentence[8:] for sentence in sentences]\n",
    "    return tokenized_sentences\n",
    "\n",
    "filename = \"data/cleaned_3.txt\"  # Change this to your file's name\n",
    "sentences_2 = read_sentences_from_file_2(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fac6fcd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PA2!',\n",
       " 'Thanks Ester!',\n",
       " 'Ester named the group CSE 151B PA2.',\n",
       " 'What should our group name be 😎 Ester and I used transformers for PA1',\n",
       " 'lol anything works',\n",
       " 'we could be incredibly original and use our names',\n",
       " 'Or name it according to the PA content? Like Convolution? \\U0001fae0',\n",
       " 'Okie dokie',\n",
       " 'I’ll make one convolution',\n",
       " 'Created, go join “convolution”',\n",
       " 'Also could you send me your email so I could add y’all as collaborator?',\n",
       " 'jeremyztow@gmail.com',\n",
       " 'Thank you for creating!',\n",
       " 'jonathandcheung@gmail.com',\n",
       " 'Added!',\n",
       " 'I created my own branch in our repo https://github.com/cse151bwi24/cse151b251b-wi24-pa2-convolution/tree/master',\n",
       " 'I pushed the updated util.py to master, but I need a minor fix after asking a question on piazza',\n",
       " \"Are you guys able to access a GPU of any sort? Datahub is still down and I can't get Google Colab to work either\",\n",
       " 'Thinking if we can use our data science capstone’s 🤔',\n",
       " 'I guess so!',\n",
       " 'Yeahh',\n",
       " 'Actually it seems like the issue happened because I’m still occupying a GPU for capstone',\n",
       " 'So I can only work on one project at a time',\n",
       " 'OHH I see I seeeee',\n",
       " 'Does it say something like CUDA blah blah exceeded or killed when u run two?',\n",
       " 'It says \"Request exceeds limit of 1 GPUs\" hahaa',\n",
       " 'OHHH okay I was wondering if u run it on the same session',\n",
       " 'But better not risk terminating the project you are running at the moment',\n",
       " 'In case it gets killed and u gotta restart the kernel 😅',\n",
       " 'I can just clear some space in my private folder and git clone the project there',\n",
       " 'Yeah sure that’s what I did',\n",
       " 'But yeah I ran into once when I was running two projects (ie notebooks) and it got too much to handle, and both just terminated - painful!! 🥲',\n",
       " '@Ester Tsai are you at part 2 for the programming part? The data loading section',\n",
       " 'Yes, I have the code for the baseline model but I’ll test it when I finish running something for capstone',\n",
       " '(I don’t know if my baseline model works, will likely need to debug!',\n",
       " 'Okkk! Sounds good, so have u implemented the test and val data in voc.py?',\n",
       " 'Feel free to test it by cloning my branch tho!',\n",
       " 'Yes',\n",
       " 'Ok could you push it to your branch? I don’t think I see it there just now',\n",
       " 'Also the first half of visualize.option',\n",
       " 'Visualize.ipynb *',\n",
       " 'https://github.com/cse151bwi24/cse151b251b-wi24-pa2-convolution/tree/ester',\n",
       " 'Yeah! wanna focus on the models so we don’t duplicate efforts on writing basic utils - which will be largely the same anyway',\n",
       " 'Yup thanks!',\n",
       " 'Almost done with the baseline model! Gonna keep testing it until test IoU is consistently around 0.05',\n",
       " 'Okie!! Thanks!',\n",
       " 'I got started with building UNet (5c)  ^just a relatively independent task to work on in the mean time. But I’ll build off what you have Ester, when the baseline is ready! Thank you Ester!!',\n",
       " 'Hmm not sure if this will make a difference and/or you are probably doing it already, might want to try using AdamW optimizer in train.py',\n",
       " 'I see we are using SGD optimizer currently',\n",
       " 'ok I can try!',\n",
       " 'Coding is done up to part 3, so feel free to make a branch of the current master branch and start working on part 4',\n",
       " 'You can test the model by running \"python train.py\" in the terminal',\n",
       " 'Yayyy thank you thank youuu!!',\n",
       " 'voc.py is where we should implement data augmentation (part 4b), after defining the augmentation',\n",
       " \"(this is not pushed to the master branch since I'll just experiment on my own branch first!\",\n",
       " 'Yup yupp!',\n",
       " 'I’m also looking at augmentation options in torchvision (part of UNet requires this)',\n",
       " 'Are you also looking at torchvision?',\n",
       " 'Just curious if there’s other places that offer image augmentation methods',\n",
       " 'TA gave this link as an example: https://pytorch.org/vision/main/generated/torchvision.transforms.RandomRotation.html',\n",
       " 'I see I see! Okk looks like it’s the same root library! Thanks for confirming haha',\n",
       " 'Idk if u guys r getting the same error but I can update my branch from the master I get an error',\n",
       " 'Cant*',\n",
       " 'So I just downloaded the zip file LOL',\n",
       " 'Maybe make a new branch haha',\n",
       " 'GitHub high key confusing sometimes',\n",
       " 'AGREE',\n",
       " 'What error are you getting tho?',\n",
       " 'Is it related to authentication?',\n",
       " 'I’m guessing it’s bc the pth model file is large and bc there’s some byte error',\n",
       " 'Interesting.. so u get an error when running the model?',\n",
       " 'I haven’t tried running the model yet so',\n",
       " 'It shouldn’t be large tho',\n",
       " 'ok its fixed',\n",
       " 'its because my wifi sucks',\n",
       " 'Wow good to know that wifi actually matters 😅',\n",
       " 'UNet is done, and together with a train_unet.py for that  train_unet is just a duplicate of train.py with minimal changes - if anyone would like to, feel free to combine the two, probably will have to create a TrainLoop object and initialize models given model name == FCN or UNet or more that we will explore. For now, the easiest is to have different train files for each model, which I believe is fine from the description of Part 5 in the assignment pdf  If everyone is okay, I can push to master before noon. Also, it’d be good to have a few more pairs of eyes to check the architecture',\n",
       " 'Yoo before the weekend comes, I am thinking if we want to sync a little on what we are currently doing and/or interested in doing? For the programming part, I believe part 4 and 5',\n",
       " 'I haven’t started anything after UNet. But I’m open to working on 5b / part 4b image augmentations / part 4c imbalanced class',\n",
       " 'I’m down to work on those parts too, but I’m not available the rest of today and am currently trying to implement the other code myself to make sure I’m understanding it',\n",
       " 'Okkk let’s wait and see what Ester and Jeremy are working on/ plan to work on/ have a preference for',\n",
       " 'Also, no pressure, just wanna make sure we don’t end up working on the exact same thing 😂',\n",
       " 'Oh yeah did prof say anything about moving the MT date? Since it’s going to be on lunar new year',\n",
       " 'I can start working on pa2 again on Monday! And then work on the report Tues and Wednesday. Down to work on both Q4 and 5 if time permits, but if time is tight, I’ll piggy back off of existing Q4 and just work on Q5!',\n",
       " 'There was a piazza post that said we’re not moving the midterm I think',\n",
       " 'Yupp ok sounds good!! U did parts 1-3 this week and I’m very thankful for that!!',\n",
       " 'OOOH I seee',\n",
       " 'about the same for me i’m also open to working on any of it',\n",
       " 'Ok! I’ll get started on part 4a later, I’ll keep y’all updated on my progress over the weekend',\n",
       " 'doc for sharing results  https://docs.google.com/document/d/1tPFP9Rze0i04IthIebiXCBRidNryW-q2Eq9NENvjZF8/edit?usp=sharing',\n",
       " '@Ester Tsai quick question, for the optimizer, did you choose learning rate to be 0.001 or was it given?',\n",
       " 'I chose it',\n",
       " 'i see.. okay! with learning rate 0.1, the accuracy actually went up to 75% (but the assignment pdf said expected ~65%)  IoU is fine, ~0.05',\n",
       " 'I can make a post on piazza',\n",
       " 'just came across this section in the visualize notebook  I will have to take back my words earlier on combining the train files, it looks like it is recommended to have different train files for all subparts in part 4 & 5',\n",
       " 'I am pretty much done with 4a, just re-running them to check the numbers',\n",
       " 'Make sense!',\n",
       " '4b is done! If everyone is okay, I can push to master',\n",
       " 'Thanks!!',\n",
       " 'Kong pinned a message.',\n",
       " 'I will get back on pa2 tomorrow, is there anything that I can work on?',\n",
       " 'I can get on that too after capstone tomorrow! What is left that we need to work on?',\n",
       " 'From yesterday, I know we have 4c, 5a and 5b left',\n",
       " 'I’ll get started on 4c today',\n",
       " 'Just did some research for 4c, I should be able to get 4c done today, will keep y’all posted on my progress',\n",
       " 'Attempting 5a - will update on how that goes lol',\n",
       " 'Thanks!',\n",
       " 'i’ll take a look at 5b later tonight',\n",
       " 'Thank you!',\n",
       " 'Pretty much done with 4c, just running a bunch of experiments, will let y’all know before I push',\n",
       " \"I'll start on the report\",\n",
       " 'Great! I’ll join u on the report tomorrow!',\n",
       " 'Here is the link to the report: https://www.overleaf.com/5635268337spfdxsqxhznt#1b7f16',\n",
       " 'And the checklist of requriements: https://docs.google.com/document/d/1nJQMPYDRe2u6rF9DQKSnqh1L9nu2qlguvrrr8XS9Ofc/edit',\n",
       " \"There are a lot to write, so I'll make a poll for how to divide the work if you guys are ok with that!\",\n",
       " 'Ester created a poll: PA2 Report Responsibility (par...',\n",
       " 'Ester created a poll: PA2 Report (part 2).',\n",
       " 'Ester voted for \"Methods - Baseline\" and 1 other option in the poll.',\n",
       " 'Ester voted for \"Discussion - Baseline\" and 1 other option in the poll.',\n",
       " 'Kong voted for \"Discussion - Improving on basline\" and 1 other option in the poll.',\n",
       " 'Kong voted for \"Methods - Improving on baseline\" and 2 other options in the poll.',\n",
       " 'You voted for \"Double Check everything before...\" in the poll.',\n",
       " 'I can also do experimentation bc I’m doing 5a',\n",
       " 'Experimentation (report) I’ll do 5c UNet',\n",
       " 'oo true it would be good to specify the 3 sections in Methods Experimentation and Discussion Experimentation',\n",
       " 'Has anyone done 5b yet? If not, I can look into it',\n",
       " 'i’m going to look into 5b in a bit',\n",
       " 'Ester voted for \"Abstract\" in the poll.',\n",
       " 'I will push 4c before 12am! Have been fine tuning the hyper parameters, finally got a satisfactory result 😮\\u200d💨😴',\n",
       " 'Yay!',\n",
       " 'Ester voted for \"Methods - Improving on baseline\" in the poll.',\n",
       " 'Ester voted for \"Discussion - Improving on basline\" in the poll.',\n",
       " 'I filled in the part for Cosine Annealing for both methods and discussion',\n",
       " 'Yayyyy! Thanks!',\n",
       " 'anyone getting an error like \"RuntimeError: input and target batch or spatial sizes don\\'t match: target [16, 224, 224], input [16, 21, 256, 256]\" when messing with kernel size?',\n",
       " 'how do i adjust the fix it?',\n",
       " 'something wrong with the cirterion function',\n",
       " 'Are you using your own code or Winfrey’s base code?',\n",
       " 'Did you get this from the line   Loss = criterion(outputs, labels) ?',\n",
       " 'i used my own code though it has similar sctructure to the 4 questions',\n",
       " 'I think it’s the',\n",
       " '256 not matching 224',\n",
       " 'Your output H and W should be 224',\n",
       " 'Try changing padding',\n",
       " '@Kong Xian Ying @Jeremy Tow @Jonathan Cheung Are you all free to call tomorrow at 9am to check in for PA2?',\n",
       " 'Yes!',\n",
       " 'Yeah',\n",
       " 'ye i shuld be free',\n",
       " 'I’ll implement the training and validation plot for report Results section',\n",
       " 'Ok sure, thanks!',\n",
       " 'Working on the rest of “improving the baseline” now',\n",
       " 'Could someone confirm that the number of the original training data is 209?',\n",
       " 'Oh yeah @Jeremy Tow @Jonathan Cheung when training the models for part 5, id recommend using the current train code implemented up till 4c to check if both IoU and accuracy increase.  Baseline + enhancements up till 4c gives IoU around 0.065 to 0.067 and accuracy 0.72 to 0.74  I saw a piazza post that they expect our models in part 5 to give better iou and accuracy',\n",
       " 'okok',\n",
       " 'will do',\n",
       " 'rn transfer learning with 5b is giving iou 0.071 and pixel acc 0.737 with the old train code',\n",
       " 'im going to test with the new train code rn',\n",
       " \"just tried 5b with @Kong Xian Ying's train_4_c, it took 15 minutes to run and the pixel acc was 0.79 with an iou of 0.2\",\n",
       " 'rly good numbers',\n",
       " 'oh btw where / how are we meeting tmr',\n",
       " 'or ig today',\n",
       " 'I think remotely!',\n",
       " 'This is great!!!',\n",
       " 'DANG Winfrey you got up at 5am 🥵',\n",
       " \"I'll create a Zoom link\",\n",
       " 'I normally take a VERY LONG TIME for breakfast 😂😂😂',\n",
       " 'https://ucsd.zoom.us/j/94091921602',\n",
       " '@Jonathan Cheung @Jeremy Tow Zoom?',\n",
       " 'def set_seed(seed: int = 42) -> None:     np.random.seed(seed)     random.seed(seed)     torch.manual_seed(seed)     torch.cuda.manual_seed(seed)     # When running on the CuDNN backend, two further options must be set     torch.backends.cudnn.deterministic = True     torch.backends.cudnn.benchmark = False     # Set a fixed value for the hash seed     os.environ[\"PYTHONHASHSEED\"] = str(seed)',\n",
       " 'Ester voted for \"Results (filling in the numbers)\" in the poll.',\n",
       " 'Ester voted for \"Discussion - Experimentation\" in the poll.',\n",
       " 'Kong voted for \"Related Work (at least 2 people)\" in the poll.',\n",
       " 'I’ll add UNet as one of the related works',\n",
       " 'https://www.overleaf.com/5635268337spfdxsqxhznt#1b7f16',\n",
       " 'Kong voted for \"References\" in the poll.',\n",
       " 'https://docs.google.com/document/d/1nJQMPYDRe2u6rF9DQKSnqh1L9nu2qlguvrrr8XS9Ofc/edit',\n",
       " \"We'll aim to get the report draft done by tonight!\",\n",
       " 'Yesss agreed!',\n",
       " \"@Jeremy Tow could you push your train_5_b code sometime today when you're done?\",\n",
       " 'yep sounds good',\n",
       " \"its literally just winfrey's train_4_c code lol\",\n",
       " 'i only modified the basic_fcn file',\n",
       " 'okok could I have your modified basic_fcn file haha',\n",
       " 'Ester do u plan to push ur code to master? The seed and plot that u added to each train file',\n",
       " \"i think i'll just push the fasic_fcn file to my branch and you can pull it from there?\",\n",
       " 'idk what the easiest way to do this is',\n",
       " 'sure I can try!',\n",
       " 'ok I just pushed to my branch',\n",
       " \"I've set up references.bib for  bibtex\",\n",
       " 'you can cite a paper using \\\\citet{reference_name} and it will automatically appear in the References section',\n",
       " '@Jeremy Tow if you have extra time, could you double check the architecture for UNet? For some reason it isn’t doing well, we got IoU 0.0556 and accuracy 0.75',\n",
       " 'yea fs i’ll check it in a bit',\n",
       " 'Thank you!',\n",
       " 'I’m trying 5a but data hub is not working for me right now',\n",
       " 'Oohhh might be cause there are too many people. I had to wait for quite a bit until I get assign a node (I used ssh)',\n",
       " 'if u need to i can give u access to my server',\n",
       " 'it works again!',\n",
       " 'Do we want to set a cutoff time for the models? For tomorrow.',\n",
       " 'Jonathan and I both tried 5A but couldn’t get it better than 6.6%',\n",
       " 'I’ll go to office hours tomorrow',\n",
       " 'Okkk! I’ll probably go too 😅',\n",
       " 'I’ve got something around 0.06 IOU, I’ll do my write up part but if we get something better by tmrw I can update it accordingly',\n",
       " 'I did the abstract too',\n",
       " 'Also, not exactly sure about the metric IoU but now I’m not sure if it can be expressed in percentage',\n",
       " 'I have zero prior knowledge about IoU though, just purely based on articles that I read they put it in decimal',\n",
       " 'i think it makes more sense in decimal',\n",
       " 'Sure!',\n",
       " 'ig its like the intersection is x percent of the union?',\n",
       " 'but idk',\n",
       " 'Yeah, sorry Ester,  I said percentage made sense this morning but I’m not sure anymore 🤔🤓😅',\n",
       " 'thought I’d share. So there has been a mystery with 5c where by the accuracy and IoU just stop improving at one point even when validation loss is actually decreasing.  There isn’t any bug in the architecture, even though I did add some minor changes to the architecture but honestly nothing changed in terms of the results that we are getting',\n",
       " 'And I read the original paper multiple times and many forums, and finally broke through the upper bound:  Kind of unexpected but it was because of a combination of the following: 1. Weight initialisation - I changed it to using normal distribution (which was mentioned in the paper)  2. Back to using SGD, cause for some reason the paper mentioned about using momentum',\n",
       " 'So for 5a, it might be something that’s beyond the network architecture, not exactly sure but yeah',\n",
       " 'Also one thing I didn’t really try is that, UNet paper mentioned that they use batch size of 1, but I’m not sure how helpful this will be',\n",
       " 'How’s the new test IoU after these improvements?',\n",
       " 'Still running but for val the highest so far is 0.057 (not great but I’m low-key traumatized by seeing the exact same 0.0556 for the past few hours 😂😂😂',\n",
       " 'This is so real',\n",
       " '0.0556 is interesting — I was using a very different architecture and it also got stuck at that exact number',\n",
       " 'this is so mysterious',\n",
       " 'Hmm set seed is actually not working fully. The results are different after rerunning',\n",
       " 'yea i was running into this too',\n",
       " 'i thought urs would work cuz u included the cuda bit but mb there’s another random source we’re not considering',\n",
       " 'like source of randomness lol',\n",
       " 'I was pretty sure it worked at one point \\U0001fae0\\U0001fae0\\U0001fae0',\n",
       " \"I'll try adding this\",\n",
       " '4c is still random',\n",
       " 'but train.py is deterministic',\n",
       " \"I checked that data augmentation should be reproducible. Then it's strange why not 4c\",\n",
       " 'Oohh like the random rotation and random crop?',\n",
       " 'yeah the angles come out deterministic',\n",
       " 'i tried doing the batch size of 1 thing mentioned in the unet paper and im getting better pixel acc of 0.73 but worse iou of 0.05432',\n",
       " 'I’m thinking - when batch size is 1, we will need a lot more epochs. How many epochs did you train on just now?',\n",
       " 'when i did a batch size of 1 i augmented the training data so that each \"img\" is actually a composite of 16 images, so the amount of epochs needed is the same??',\n",
       " 'In this case, is the backward based on one image or 16 images?',\n",
       " '16 i think',\n",
       " 'well like technically its 1 image that is a composite of 16 imgs',\n",
       " 'Yeah, like 15 different transformations of the same image + the original?',\n",
       " 'umm i think its 16 different images',\n",
       " 'Oh as in, one image is segmented into 16 pieces?',\n",
       " 'yea',\n",
       " 'so 1 image is like a 4x4 grid represnting 16 imgs',\n",
       " 'i tried sgd as well and its worse',\n",
       " 'Interestinggggg',\n",
       " 'Maybe I can run it on your branch? And see if I get the same',\n",
       " 'yea give me a bit to make it suitable for human viewing first lol',\n",
       " 'Yeah take ur time!',\n",
       " 'ok i just pushed its under jeremy-unet, the code is still vv scuffed but its kinda legible now. mainly i moved the batch_size argument to the voc, and i use the voc to combine the images. as a result batch_size has to be a perfect square',\n",
       " 'also idk why yet but im consistently getting iou of 0.49 now',\n",
       " 'For UNet ?',\n",
       " 'yea',\n",
       " 'Ok I’ll look into it in the morning 🤓 thanksss!!',\n",
       " 'just tried using batch sizes of 4 in the dataloader and 4 in the voc, validation in each epoch was giving good numbers but the final test iou was not as good',\n",
       " '0.49? 😮😮😮',\n",
       " 'The highest I’ve gotten is 0.0627',\n",
       " '@Ester Tsai can I confirm that u have already discussed Xavier and batch norm in the baseline method section? If so, I won’t include those in the introduction',\n",
       " 'Kong voted for \"Introduction\" in the poll.',\n",
       " 'I will try the architecture described in this lecture for 5A https://piazza.com/class_profile/get_resource/lr49jt48q953s2/ls80vuwtso635f',\n",
       " 'Ester, is there a time that you would like me to finalize UNet? For plots and the final numbers (I’ll take care of the segmentation result for one test data in visualize.ipynb',\n",
       " 'I’m thinking latest by 6pm today, but I’m flexible',\n",
       " 'I plan to put up the plots after we finalize our models ye',\n",
       " '6pm sounds good',\n",
       " 'Yeah',\n",
       " 'Okay and once you run everything, could you push your version of 5b to main',\n",
       " 'I can also fill in the segmentation result since it requires someone having all 7 models',\n",
       " 'Yeah',\n",
       " 'Sure',\n",
       " 'Ok I’ll get you the best possible UNet by 6 😎',\n",
       " 'Do you guys know how to get rid of the CUDA out of memory issue?',\n",
       " 'Restarting the server used to help, but not this time',\n",
       " 'Smaller batch size usually works for me',\n",
       " 'And restarting',\n",
       " 'for 5A I made the stride 1 and batch_size=4 and got this for epoch 1',\n",
       " '0.085 IoU',\n",
       " 'NAISEEEEEE',\n",
       " \"we'll see if it's gonna be good!\",\n",
       " 'and yeah this is interesting, I got 0.062 IoU only after changing the kernel size in UNet',\n",
       " 'changing to what kernel size?',\n",
       " 'All those 3*3 to 4*4',\n",
       " 'And final layer from 1*1 to 3*3',\n",
       " 'epoch 2 IoU: 0.1136',\n",
       " 'but I accidentally interrupted the training lol',\n",
       " 'sad, each epoch takes 8 min to train',\n",
       " 'Ahhhh',\n",
       " 'That’s quite a while',\n",
       " 'Like one hour for 6 epochs',\n",
       " 'Is the network very deep?',\n",
       " 'What we could do is have like 6-10epochs, then mention the complexity and efficiency in the discussion section',\n",
       " 'im running for 10 epochs',\n",
       " 'so ill report that result',\n",
       " 'Yes I’m doing 10 epochs too for 5A',\n",
       " 'Good news! TA just said Unet IoU will be graded very leniently',\n",
       " '0.06 is on',\n",
       " 'Ok',\n",
       " 'OH THANK YOU!!',\n",
       " 'Okay then I’ll be able to finalize before 2pm!',\n",
       " 'Did it take long to train 5b? The transfer learning model',\n",
       " 'Just curious',\n",
       " 'Yes, Jeremy said 15 min?',\n",
       " 'Possibly longer',\n",
       " 'SHEEEESH',\n",
       " 'just by changing stride',\n",
       " '5A we gucci',\n",
       " '@Jonathan Cheung could you experiment with having other layers too? TA said just changing stride is not big enough architecture change',\n",
       " 'Yeah',\n",
       " '^also remember to add a table for your architecture details like Jeremy and Winfrey did in the report',\n",
       " 'finally something close to 0.07',\n",
       " 'thanks for the batch size = 4 idea HAHHA',\n",
       " 'YOOOOO',\n",
       " 'it trains pretty fast too',\n",
       " '2 epochs gets training iou of 0.099',\n",
       " 'I modified kernel size as well',\n",
       " 'Could you see if it gets higher than 0.12?',\n",
       " '0.11 with leaky relu',\n",
       " '@Ester Tsai do you have a preference for pushing to main or pushing to my branch for you to compile the models? I’m only pushing train_5_c.py and unet.py',\n",
       " 'For me to push to main or my branch** whichever is easier for you grab',\n",
       " 'hows this',\n",
       " 'this is with some kernel, acivation, and layer changes as well comapred to just stride',\n",
       " 'Personally think that IoU greater than 0.07 IoU good enough and above 0.10 is GrEaT 🤣',\n",
       " 'ok then',\n",
       " 'As long as we can justify we made sufficient changes to the architecture in the report and have some discussion, then we r good',\n",
       " 'i can push the 5a code if thats fine with everyone',\n",
       " 'Your branch is ok! I’ll just copy paste for ease lol',\n",
       " 'pushed',\n",
       " \"ok pushed 5c to my branch as well! there's a high chance you won't get the exact same result, i believe it's the randomness that you mentioned yesterday\",\n",
       " \"master's README has been updated for submission later today!\",\n",
       " 'THANKS!',\n",
       " 'I’ve finished running 5A and will update the report with the plots for the Results section after my tutor hours',\n",
       " 'Yeah no problem! Thank you!',\n",
       " 'I can help cross check the architecture table when you are done @Jeremy Tow @Jonathan Cheung   @Jeremy Tow do you plan to add ResNet under related works? Just checking!',\n",
       " 'i’ll add the paper in',\n",
       " 'Ok coools!',\n",
       " 'Unet now has a lower accuracy',\n",
       " \"I'll rerun it maybe?\",\n",
       " 'Oh wow test IoU is 0.11?',\n",
       " 'That’s interesting',\n",
       " 'Sure',\n",
       " 'I changed train, val, and test batch size to 4',\n",
       " \"I'll double check that the IoU function is not dependent on batch size haha\",\n",
       " 'WOW',\n",
       " 'What magic did you do',\n",
       " '😂😂😂',\n",
       " 'Dang UNet was crawling and now it’s flying',\n",
       " 'LOL smaller batch size is better I learned',\n",
       " \"jk this screenshot doesn't say much\",\n",
       " 'The batch size can be understood as a trade-off between accuracy and speed. Large batch sizes can lead to faster training times but may result in lower accuracy and overfitting, while smaller batch sizes can provide better accuracy, but can be computationally expensive and time-consuming.',\n",
       " 'I seeee',\n",
       " 'Interesting',\n",
       " 'I like this sample!!',\n",
       " 'O OOP my IoU implementation is not quite right',\n",
       " 'It’s supposed to be over the entire validation set',\n",
       " 'Eh really? I thought I checked',\n",
       " 'Oh u mean the average?',\n",
       " 'Or the iou function',\n",
       " 'Hmm I think I did implement it according to the TA’s description',\n",
       " 'isn’t the iou function supposed to be of a single prediction and target',\n",
       " 'and then u average it in the train file',\n",
       " 'I suppose it worked nonetheless',\n",
       " 'But I’m averaging over each batch and then averaging all the batches’ average IoU',\n",
       " 'So the number might be a little odd',\n",
       " 'Ohhh ok we are looking at the train files not the util iou right',\n",
       " 'Oh util iou averages over the batch',\n",
       " 'Util iou is right if the input is only one image, but right now the input is a batch of images',\n",
       " 'Wait just kidding',\n",
       " 'Let me think again',\n",
       " 'Maybe the input is indeed just an image',\n",
       " 'LOL sorry for psyching out we’re probably fine?',\n",
       " 'But I discovered that when I make the test batch size smaller the IoU goes up, which it shouldn’t',\n",
       " 'enumerate batch_loader gives a batch. So inputs are 16*3*224*224',\n",
       " 'O RIGHT',\n",
       " 'hmmm let me go through the code and check',\n",
       " 'Maybe in util iou np.nanmean can be done on a specific dimension',\n",
       " 'Or do a double loop in util iou, so   loop through preds, targets:    Loop through all classes:    Np.nanmean for that particular one pred',\n",
       " 'I’ll try double loop',\n",
       " 'Iou becomes 0.2619 for train.py',\n",
       " 'I have a feeling that should be more accurate but the TA implemented it wrong? 😮 maybe maybe not \\U0001fae8',\n",
       " '0.05 felt too low',\n",
       " 'So maybe they meant over the whole validation set instead of each individual image',\n",
       " 'High key I will just revert back to what we had before because it matched the description well',\n",
       " 'Actually… we can keep it as is 😂',\n",
       " 'Yeah',\n",
       " 'Agreed',\n",
       " 'I’ll aim to be done by 9:30pm but',\n",
       " 'oh ok i’ll start working on it rn',\n",
       " 'I’ll update code to do full validation set after and see if there’s a big difference',\n",
       " 'Samuel chu said he tried and it wasn’t very different',\n",
       " 'Okkk double checking the architecture now',\n",
       " 'Oh just joking, I’ll wait a bit',\n",
       " '@Jonathan Cheung just checking, for 5a specifically, did you make any changes to  - weight initialization - data augmentation - optimizer - class weights',\n",
       " 'just added all the plots',\n",
       " 'yupp they look perfecto',\n",
       " 'thankies!',\n",
       " 'tysm tysm',\n",
       " 'Learning rate I believe is 0.001 and the other thing is 0.0005',\n",
       " 'But I think that’s default',\n",
       " 'Oh then did u use the learning rate scheduler?',\n",
       " 'I think so?',\n",
       " 'Okk',\n",
       " 'Let me add that blurb saying that everything else is kept the same',\n",
       " 'In the report',\n",
       " 'Quick question, is there supposed to be a separate model Python file for 5a and 5b?',\n",
       " 'oh shoot i didn’t make a sepearte train file for 5b',\n",
       " 'so it kinda overwrites the 4c one',\n",
       " 'Oh no worries, I think Ester got u',\n",
       " 'I got it yeah!',\n",
       " 'Ok I see it now!',\n",
       " 'I’m waiting to get one more plot updated potentially',\n",
       " 'But I will get the report done rn',\n",
       " 'Please add writing to the results section if you guys can',\n",
       " 'I just wanna get a better plot for 5A if possible',\n",
       " 'Yes on it',\n",
       " 'Ah',\n",
       " 'Ester, I am seeing train_5_c has the old version',\n",
       " 'Like it calls FCN and uses AdamW',\n",
       " 'Ohh let me see',\n",
       " 'I have the updated version locally but somehow didn’t update on GitHub',\n",
       " 'I’ll fix that rn',\n",
       " 'Resubmitted code',\n",
       " 'We still need to write some of discussion section',\n",
       " 'Yeah for each of the models in 5, need to draw insights from table and visuals',\n",
       " 'Currently we are only drawing from table',\n",
       " 'The 5A test iou is not accurate, it should be lower',\n",
       " 'I’m rerunning it but it’s too slow oop',\n",
       " 'I’m adding to the discussion right now',\n",
       " 'Hmmmm I’m kind of stuck with UNet’s discussion on the sample test segmentation, since it had worse results than the other two architecture, the test segmentation shows that UNet captures a lot of them 👀',\n",
       " 'Weird yeah \\U0001fae3',\n",
       " 'Ok I’m back home, is there anything I need or finish up',\n",
       " \"yes, could you check if 5a's discussion mentions the loss plot and the segmentation visual\",\n",
       " 'Still need to add something about the segmentation visual',\n",
       " 'I’m not even sure what to write abt the segmentation part',\n",
       " 'Like it looks like it’s much worse than the IOU and accuracy would say',\n",
       " 'i just added the resnet paper to related work',\n",
       " 'and i finished the architecture table for resnet',\n",
       " 'ok added mention on the results plot and visual in part 4 + baseline',\n",
       " 'I think some of us can start checking the entire paper',\n",
       " '@Jeremy Tow there are three layers numbered 31, is that intentional?',\n",
       " 'oh shoot good catch',\n",
       " 'fixed ty',\n",
       " 'np! thanks',\n",
       " 'ok I will stop writing for now, I think some of us might be working on the same section right now, I am starting to see some similar description 😅',\n",
       " 'also i used present tense for everything and some of us used past tense.   we can stick with one for consistency purpose - whoever is checking the entire paper.',\n",
       " 'I can chnage all the tenses to present',\n",
       " 'ok thanks!',\n",
       " 'Actually not understanding the segmentation visual, r the blobs supposed to look like the shape of the ppl?',\n",
       " '@Jeremy Tow could you check the code submission? I am not sure if the transfer learning file is actually reflecting the architecture in the report',\n",
       " 'ok yea looking rn',\n",
       " 'It might be the old version!',\n",
       " '\"More specifically, a constant learning rate can cause the model to get stuck in local optima and result in a slower convergence or sub-optimal solution.\" is this sentence supposed to be referring to local minima?',\n",
       " 'rather than \"optima\"',\n",
       " 'Yes, but optima means the same thing right',\n",
       " 'oh wait it means the same thing lol',\n",
       " 'just checked, it looks good',\n",
       " 'ok cools! I just realized that none of the encoder is actually called. sorry, got confused for a bit.',\n",
       " 'thanks for confirming!',\n",
       " 'yea the encoder is rollled up into the forward section of the FCN model',\n",
       " 'Rerunning 5A',\n",
       " 'It’s getting faster now',\n",
       " '5 min per epoch',\n",
       " 'nice nice',\n",
       " 'It was 24 min or something earlier',\n",
       " 'Everything except maybe a sentence or 2 in the last section are present tense, it should be ok because its a building off of the previous experiments?',\n",
       " 'Is there a way to make the discussion section go after the results section’s figures?',\n",
       " 'Right now discussion starts in between those figures',\n",
       " 'page breaks?',\n",
       " 'try \\\\newpage',\n",
       " 'It’s already there haha',\n",
       " 'forcing new page on new section',\n",
       " 'No idea why item doesn’t help',\n",
       " 'ohh',\n",
       " 'ok fixed!',\n",
       " 'use \\\\clearpage instead',\n",
       " 'Is it intentional we made the images smaller?',\n",
       " '2 per page seems to fit pretty well',\n",
       " 'ya I made it smaller but it was before we organize the report',\n",
       " 'so feel free to make new changes haha',\n",
       " 'is everyhting else finalized?',\n",
       " 'yes on my end',\n",
       " 'Sure!',\n",
       " 'good work!',\n",
       " 'tysm tysm',\n",
       " 'i can submit?',\n",
       " 'The time crunch is real \\U0001fae3',\n",
       " 'I’m looking through it again but feel free to submit',\n",
       " 'Wait sorry',\n",
       " 'The architecture for transfer learning',\n",
       " 'I’ll fix it, gimme 1 min',\n",
       " 'Ok done!',\n",
       " 'ok ill usbmit now',\n",
       " 'Thank you Jonathan!',\n",
       " 'ok i added u guys, check if u can see the reprot on gradescope',\n",
       " 'Yup!',\n",
       " 'I’ll need to resubmit it lolol I made some changes',\n",
       " 'lel',\n",
       " 'i can resubmit it again then, idk if it messes up if theres diff submissions from diff ppl in the ame team',\n",
       " 'lmk when ur finsihed changing',\n",
       " 'My 5A is almost done lol',\n",
       " 'Let me check the results',\n",
       " 'Dang it’s cutting real close',\n",
       " 'I’ll resubmit right now just in case',\n",
       " 'do u want me to do it?',\n",
       " 'I’m updating something rn',\n",
       " 'Ok just submitted, will add people',\n",
       " 'Done! Sorry for cutting it so close!',\n",
       " 'Thanks hehe',\n",
       " 'No no, not at all',\n",
       " 'Please check if it’s good',\n",
       " 'nice job',\n",
       " 'All good, thanks for the good work everyone!! Get some good rest for now 😴',\n",
       " 'YAY',\n",
       " 'tytyty',\n",
       " 'Group name is recurrent for PA3!',\n",
       " 'Yayay!',\n",
       " 'Joined',\n",
       " 'Ester named the group CSE 151B PA3.',\n",
       " 'Thanks for the name update that’s very thoughtful of you HAHA',\n",
       " 'Ester Tsai added Samuel Chu to the group.',\n",
       " '@Samuel Chu if you’d like to join!',\n",
       " 'I will get started on the programming part tonight, has anyone started working on it? Just wanted to see where I should start with, in case some parts have already been completed',\n",
       " 'I plan to start at 3:30pm!',\n",
       " 'Okie I can check with you tonight when I start!',\n",
       " 'i’m prob going to start working tonight or tmr, if i do start today i’ll lyk',\n",
       " 'Actually I’ll start maybe tonight! Gonna crank out some tutor stuff that’s time sensitive',\n",
       " 'Yup yuppp no worries!',\n",
       " 'Heloo, I made changes to fix the indentation issue mentioned on Piazza and pushed the main branch. None of the body content is changed, just indentation',\n",
       " 'There has been quite a few typos in the given template from what I have seen in Piazza. I fixed most of them and pushed to main  Another typo that is not on main  In util, get_random_song_slice takes in string data not list  Similarly, get_random_song_sequence_target takes in song in string format not list',\n",
       " 'https://piazza.com/class/lr49jt48q953s2/post/496   ^ the post about it',\n",
       " 'I should be able to get the baseline out today  But without the generate.py',\n",
       " 'Thank you Winfrey!',\n",
       " 'I’ll do the individual part first and then get on the code',\n",
       " 'I’ve push a working model to my branch called winfrey (should cover up till Q3)',\n",
       " 'The loss I’m getting is lower (0.06) than expected (1.9) so I’m still clarifying with the TA on loss calculation - there’s a bunch of averaging going on 🥲',\n",
       " 'I see!! I’m done with the individual part so I’ll be able to get to the code tomorrow!',\n",
       " 'Or later tonight 😮',\n",
       " 'Wowow this PA seems fun',\n",
       " 'I’ll work on it today',\n",
       " 'Just learned the ABC notation 😳',\n",
       " 'Yaaahhh heheh',\n",
       " 'The lowest loss I get so far is 2.1679, Im making changes to the optimizer to find the one that gives the closest to 1.9  Other than that, my branch is the most updated of my progress',\n",
       " '@Jeremy Tow @Jonathan Cheung @Samuel Chu any progress so far? 😃',\n",
       " 'I’ll be occupied with midterm and capstone until the weekend, so I’ll build off whatever we have by then',\n",
       " 'I’ve done literally nothing but I’ll try for part 4',\n",
       " \"I also haven't done anything but I'll start today\",\n",
       " 'little bit, just slowly working through',\n",
       " 'RuntimeError: cuDNN version incompatibility: PyTorch was compiled  against (8, 7, 0) but found runtime version (8, 6, 0). PyTorch already comes bundled with cuDNN. One option to resolving this error is to ensure PyTorch can find the bundled cuDNN.Looks like your LD_LIBRARY_PATH contains incompatible version of cudnnPlease either remove it from the path or install cudnn (8, 7, 0)',\n",
       " 'The Piazza post about this issue did not help lol',\n",
       " 'btw here is the Overleaf report link for PA3! https://www.overleaf.com/7724919614fydpcydwnbfj#9c10da',\n",
       " 'Has anyone else figured out how to solve this?',\n",
       " 'Looking at it now',\n",
       " 'You got the error when u were trying to install PyTorch?',\n",
       " 'No, when I run main.py',\n",
       " 'This is me lol',\n",
       " 'conda install does not even finish installing and bugs out haha',\n",
       " \"pip install doesn't solve the problem\",\n",
       " 'O I FIXED IT',\n",
       " 'OH how',\n",
       " 'unset LD_LIBRARY_PATH',\n",
       " 'OH LOL',\n",
       " 'Anything that has to do with PATH is very painful',\n",
       " \"Does that mean I'm not using CUDA tho?\",\n",
       " \"it's very slow oop\",\n",
       " 'No I don’t think so, the forum that I read says it’s just old version stuffs',\n",
       " 'Wifi matters a lot 😂',\n",
       " \"I'm training on cpu oops\",\n",
       " 'It’s slower when I run at school',\n",
       " 'Hmmmm',\n",
       " \"but that's what the TA suggested\",\n",
       " 'how long does it take for you?',\n",
       " 'Like one hour',\n",
       " 'For 100 epochs',\n",
       " 'oh me too then',\n",
       " \"jk it's slower for me\",\n",
       " 'Ahh cause it’s on cpu?',\n",
       " 'yeah : //',\n",
       " 'O IM OK NOW',\n",
       " \"I'm using capstone platform\",\n",
       " 'instead of datahub',\n",
       " 'YAYAYAYAYYAYAYYAYAYAYY',\n",
       " 'Oh THATS GREAT',\n",
       " 'CAPSToNE SUPREMEM',\n",
       " 'Honestly, I have never used datahub once',\n",
       " \"it's really fast, like 3 epochs per minute?\",\n",
       " 'Like this…?',\n",
       " 'Is it about the same?',\n",
       " 'Wow urs is slightly faster actually',\n",
       " 'I use n 30',\n",
       " 'It’s a bigger GPU although not sure if that affects speed?',\n",
       " 'Ooohhh I seeeee',\n",
       " 'i’m curious what gpu it is lol',\n",
       " 'it\\'s called \"a5000\"',\n",
       " 'Is 2.1679 the validation loss?',\n",
       " 'is it correct there is no test loss?',\n",
       " 'This is with using Adam, lr=0.001 and weight decay = 0.00001',\n",
       " 'Also TA suggested running with more epochs, I’ve only been running 100 epochs',\n",
       " 'And probably seed 10',\n",
       " 'And yeah no test loss, not given in the starter code',\n",
       " 'do you know if we are allowed to change other configs like SEQ_SIZE',\n",
       " 'ah that I’m not sure. I thought we are not supposed to change anything in the config until like 5 mins ago a TA said try more epochs',\n",
       " 'hmm I suppose instruction said to use SEQ_SIZE=25~30',\n",
       " 'Ohhh good catch!',\n",
       " 'true more epochs might help. the validation loss is still decreasing',\n",
       " \"I'm gonna work on generate.py for now\",\n",
       " 'coooools I’ll keep it running in the background while I do other stuffs 😂',\n",
       " 'What is this?',\n",
       " \"Don't do that haha\",\n",
       " 'it removes CUDNN basically',\n",
       " 'I’m getting the same error, you fixed it by switching to capstone stuff?',\n",
       " 'so it solves the error but you can only use CPU',\n",
       " 'yes',\n",
       " 'I ssh from terminal, not datahub',\n",
       " 'Like using the capstone jupyterhub or ssh ing?',\n",
       " 'Ok yeah',\n",
       " 'rn the code for generate.py works but the result is not functional',\n",
       " 'So.. do we have to remove the unknown characters?',\n",
       " 'In generate',\n",
       " 'I’m not sure if those bolded characters are identified as unknown',\n",
       " \"the bold shows bad syntax (system can't recognize what it wants to do)\",\n",
       " 'I see',\n",
       " 'So there’s gonna be some way we need to figure out to control/process them before putting into the converter',\n",
       " 'For generate.py, Is it correct that the model should only take in the most recent prediction as the input?',\n",
       " 'I wonder how it knows what the context is if I do that',\n",
       " 'There’s the hidden state that saves the context',\n",
       " 'Let me fix something real quick and check again',\n",
       " 'Okk',\n",
       " '@Kong Xian Ying when you finish training, could you send me the model checkpoint if possible? or I can just send you my generate.py',\n",
       " 'Yuppp!',\n",
       " '@Ester Tsai r u sshing into capstone and using cpu?',\n",
       " 'No, GPU',\n",
       " 'im getting the same error even on gpu lol',\n",
       " 'This is the extended fur elise',\n",
       " 'X:1759 T:F\\\\\"ur Elise T:Bagatelle No.25 in A, WoO.59 C:Ludwig van Beethoven O:Germany Z:Transcribed by Frank Nordberg - http://www.musicaviva.com F:http://abc.musicaviva.com/tunes/beethoven-ludwig-van/be059/be059-pno2.abc V:1 Program 1 0 Piano V:2 Program 1 0 bass Piano M:3/8 L:1/16 Q:3/8=40 K:Am V:1 e^d|e^deB=dc|A2 z CEA|B2 z E^GB|c2 z Ee^d| V:2 z2|z6|A,,E,A, z z2|E,,E,^G, z z2|A,,E,A, z z2| (DD (FABA A2c|defg abag|(3fee f2 (3gag (3gfe (3dcB BA|1 GAAF Ldeg|bagf gab|ede dcBA|adcB cBAG|FAFA FABd|1 (3BGA ABdB CAFA|d3ef d2fg|(3efe deBd| ecAd edcA|(3BAF GBAA| G2GF A2BG ABcA| AAF AFE|eAAF ABdB|cAA ABA|A GFG A2d:|',\n",
       " 'best val loss so far 2.0089',\n",
       " 'AdamW lr=1e-3',\n",
       " 'Nice niceeeee',\n",
       " 'my newest progress is on ester2 branch',\n",
       " \"haven't done hyperparamter tuning yet but I set up the plot name to reflect different hyperparameter choices\",\n",
       " 'might be helpful to make a branch from my branch',\n",
       " 'ur so amazing',\n",
       " '@Jeremy Tow @Samuel Chu does the GPU work for you guys when you run Python main.py?',\n",
       " 'Also our current val loss is ok! We can move on to hyperparameter tuning,. I;ll start that tonight',\n",
       " 'Yayyy!! Thank you!!',\n",
       " 'im trying to run the main.py on your branch on gpu but im getting this error:',\n",
       " \"is it supposed to be config['dropout']?\",\n",
       " 'O yeah small typo',\n",
       " 'okay',\n",
       " 'It’s config[“model_type”]',\n",
       " 'okay thaks',\n",
       " 'i got the same error that jonathan got and you were getting earlier',\n",
       " 'using datahub tho. i could try ssh',\n",
       " 'Piazza someone said they created a new conda environment and installed pytorch and it worked',\n",
       " 'oh okay. ill try that',\n",
       " 'lol im getting a new error after using a conda env:',\n",
       " 'for some reason torch.cuda.is_available() is false',\n",
       " 'Did you request a GPU when you ssh?',\n",
       " 'Just updated my branch more for hyper parameter tuning. Gonna run it overnight and get all the results',\n",
       " 'Does anyone understand how to do part 6 (feature evaluation)? Is the “activation of each neuron” the result of calling model.forward()?',\n",
       " 'Currently doing AdamW with lr=1e-3 and no weight decay, but you guys should try different optimizer setup and see if results are different 😮😮',\n",
       " 'run these to test hyper parameters if you clone my latest branch! python main.py python main.py --config config_RNN.json python main.py --config config_200_neurons.json python main.py --config config_250_neurons.json python main.py --config config_dropout_0.2.json python main.py --config config_dropout_0.3.json',\n",
       " 'Hold on gonna fix something small real quick',\n",
       " 'Not too sure, I only remember seeing a post on piazza about softmax and temperature',\n",
       " 'lol that’s my post',\n",
       " 'I think I figured the temperature out',\n",
       " 'But I will ask in OH today to confirm',\n",
       " 'Finished updating! Now we can know the training loss that correspond to the min Val loss',\n",
       " 'Can we call at 3pm today to do a quick check-in for what to get done over weekend?',\n",
       " '@everyone',\n",
       " 'Yasssss',\n",
       " 'I have midterm at 4 so I’ll can only stay until 3.30pm for the call',\n",
       " '@Ester Tsai @Jonathan Cheung logistic is actually a nonlinear technique, good job!! 🤣🎉',\n",
       " 'Huhhhhh',\n",
       " 'I’m working on the heat map, almost done',\n",
       " 'What’s the lowest val loss so far with hyperparameter tuning?',\n",
       " 'Baseline is still the best',\n",
       " '2.008',\n",
       " 'Interesting',\n",
       " 'Thanks!!',\n",
       " 'Reminder we will call at 3pm',\n",
       " 'I’ll go to the 2-3pm OH',\n",
       " 'The heat map has been implemented but it has a small bug',\n",
       " 'only the first 20 chars show up',\n",
       " 'Jk he moved it again so there’s no OH rn',\n",
       " 'I’m ready call anytime if you guys are',\n",
       " 'https://ucsd.zoom.us/j/93496884535',\n",
       " 'wait i’m not ready',\n",
       " 'what time works for you?',\n",
       " 'like in 5 mknjtes',\n",
       " 'Ah can we keep it at 3? I’m at work until 3pm and I have a consultation at the moment',\n",
       " 'ok!',\n",
       " 'I’ll get on soon, in capstone meeting rn',\n",
       " \"let's do 3pm still\",\n",
       " 'both winfreya nd I will need to leave by 3:30',\n",
       " 'https://docs.google.com/document/d/1nJQMPYDRe2u6rF9DQKSnqh1L9nu2qlguvrrr8XS9Ofc/edit',\n",
       " 'https://www.overleaf.com/7724919614fydpcydwnbfj#9c10da',\n",
       " '^ report',\n",
       " 'ok I pushed latest changes!',\n",
       " \"i tried ssh'ing into capstone gpu but that gave me the same error also\",\n",
       " 'To clarify the most updated branch is ester2! Ignore the original Ester',\n",
       " '@Kong Xian Ying do you think you help me fix the gpu error i’m having with capstone gpu later today?',\n",
       " 'Oh yeah ofc',\n",
       " 'Is 8.30 ok?',\n",
       " 'hmm i might not be home til 9-9:15😬',\n",
       " 'would 9:15 work?',\n",
       " 'Okiee',\n",
       " 'thanks',\n",
       " \"Samuel Chu is inviting you to a scheduled Zoom meeting.  Topic: Samuel Chu's Personal Meeting Room  Join Zoom Meeting https://us05web.zoom.us/j/4658153808?pwd=sasbq5aNFoJprBhAtUUwlrKKsOLhlP.1  Meeting ID: 465 815 3808 Passcode: meeting\",\n",
       " 'Okie coming',\n",
       " 'do you want to set a new meeting?',\n",
       " 'https://ucsd.zoom.us/j/91954366855',\n",
       " \"update: Winfrey showed me what she did on the capstone gpu, but it still isn't working on my end :| so I think she will try to train a model with new hyperparameters tmr maybe?\",\n",
       " 'Okok! You could still write the report using the plot results on my branch if needed 👌👌',\n",
       " 'okay is there stuff that you tested that is not on the report already?',\n",
       " 'I have the basic set of results that the report needs (loss plots, heatmaps, and the training/val loss are in the model checkpoint names), but we can test more models and get better results',\n",
       " 'Training with 150 sequence length right now, it’s training slower so…  … will mostly hear back from me after 2 hours 😂😂',\n",
       " 'Code is still running but I got 1.988 lowest so far with sequence length of 70',\n",
       " 'I think we can go ahead with sequence length of 70, and the training speed is acceptable',\n",
       " 'i trained with a sequence length of 300 and let it run the whole day, i’ll let you guys know the loss after i get back to my computer',\n",
       " 'Training with sequence length of 50 is even better. Got 1.97 at epoch 198, also faster for sure',\n",
       " 'okie I think I will settle with sequence length of 50 for now',\n",
       " 'will also see what Jeremy gets with sequence length of 300!',\n",
       " 'baseline model plot',\n",
       " 'its kinda not good',\n",
       " 'I’m thinking if it’s overfitting',\n",
       " 'yea :(',\n",
       " 'But also getting 2.0 something at epoch 151 is kinda okay ish?',\n",
       " 'Oh sorry my bad, I saw epoch instead of iter',\n",
       " 'Ignore this heh',\n",
       " 'Ah I see',\n",
       " 'Just very very slightly higher',\n",
       " 'def not worth doing',\n",
       " 'Would you like to try sequence length of 50? to make sure my GPU is not overly optimistic haha',\n",
       " 'ok yea fs let me do a run rn',\n",
       " 'Okkk',\n",
       " 'oh how many neurons are u using',\n",
       " '150',\n",
       " 'Epochs 260',\n",
       " 'oh wait no im stupid i ran it with 30 instead of 200 i forgot to update the config file fully uhhhhhhh',\n",
       " 'i will run it again lol',\n",
       " \"i'll run both 200 and 50 again and report back\",\n",
       " 'skulling',\n",
       " 'OH LOL',\n",
       " 'Hahaha okieee',\n",
       " 'And I’ll generate the plots for hyperparamter tuning using sequence length of 50 and all else constant except for the specific hyperparamter , I’ll take the neurons config, and would you like to take the dropout configs?',\n",
       " 'okok',\n",
       " 'do i even try 200 lol the training time is obscene',\n",
       " 'One round takes like 1.5hours to 2 hours for me (cause sometimes my wifi drops lol   So I def won’t be able to do all configs in one day',\n",
       " 'How long did it take 😂',\n",
       " 'Oh I’m joking',\n",
       " 'I mean.. only if u want to keep it running overnight 😂',\n",
       " 'like fully 4 minutes per epoch i think',\n",
       " 'I tried 150 and it got too miserable at one point I forgot how long I waited so I just interrupted it 😂😂😂',\n",
       " 'so like 17 hrs',\n",
       " 'Ouchy, wait how is ur GPU billed?',\n",
       " 'i have my own gpu lol',\n",
       " 'Ohhhhh like u own it?',\n",
       " 'its like ever so slightly slower than the datahub ones i think? but like more reliable and i dont have to worry about my connection dropping',\n",
       " 'yea its sitting in my computer next to me rn lol',\n",
       " 'when u said u have one, I was thinking that you actively rent/pay on demand from like Amazon',\n",
       " '😂 that’s very cool',\n",
       " 'oh man i could never aws is so expensive',\n",
       " 'OH REALLY dang I thought they always sell themselves as a cheaper option or like pay as you go, no hidden fee, low cost etc',\n",
       " 'like idk i dont think id want to spend any extra money for class lol',\n",
       " 'yea i think im just going to stop at 50',\n",
       " '300 is taking soo long ive gone through 3 epochs so far',\n",
       " 'training 50 takes me about 1:10 per epoch',\n",
       " 'Yeah does take a while, I think datahub might be faster but not really too significant? I haven’t really timed it properly. Anyway, if we train the full 260 epochs each time, we can probably only do like 3-4 rounds per person per day (for the amount of time that we are awake 😬',\n",
       " \"if it takes around 2 hrs for datahub then its like twice as fast lol mb i'll switch\",\n",
       " \"mb i'll do runs on datahub and my personal gpu concurrently\",\n",
       " 'I could be wrong cause I just start it and work on other stuffs 😂😂 it could actually be 3-4 hours but yeah I think running on both is a good idea if that’s manageable',\n",
       " 'i just timed a single epoch and extrapolated',\n",
       " 'Should I try running anything when I’m back or just focus on the report?',\n",
       " 'Hmmm I’d say.. we’ll see? I was thinking that you’d want to rest and settle down once u get home. I think I will be able to run 2 full rounds by afternoon (covering all of hyperparamter part b)',\n",
       " 'i will run these three',\n",
       " 'seq length of 50',\n",
       " 'Woah nice',\n",
       " 'Should we just go with the results you guys got?',\n",
       " 'Lmk if I can help run anything!',\n",
       " 'Yup yup',\n",
       " 'Yes! could you run config 250 neurons with sequence length 50?',\n",
       " 'AdamW lr=1e-3?',\n",
       " 'Yupp everything else the same',\n",
       " 'how about the existing result on report?',\n",
       " 'Ohh I pushed a small fix for train.py last weekend so u might need to check with git pull',\n",
       " 'okok!',\n",
       " 'I updated row 1',\n",
       " 'Row 2 still running',\n",
       " 'As in, the current row 1 is accurate',\n",
       " 'Which branch should I pull?',\n",
       " 'ester2',\n",
       " 'Okok!',\n",
       " 'Oo what was the bug in train.py?',\n",
       " 'Remember u said u added to SongRNN to return x0 or x1',\n",
       " 'So now the songrnn forward returns output, _ instead of just output',\n",
       " 'Got it!',\n",
       " 'Now running config_250_neurons and the RNN next if it’s not done already',\n",
       " 'im kind of confused on temperature can affect how deterministic the model is. bc if we run these values: 5, 10, 15 though a softmax no matter what we divide each element by, when we pick the element with the highest softmax value it will always be the 2nd index(15) right?',\n",
       " 'We are not picking the element with the highest soft max value',\n",
       " 'Soft max value goes into torch.multinomial to serve as a list of probabilities',\n",
       " 'Torch.multinomial randomly selects a character to output',\n",
       " 'ohhh',\n",
       " 'ok',\n",
       " \"I'm updating the related works section with citations\",\n",
       " 'feel free to double check and add more',\n",
       " \"I'm done with my written portions! Lmk if another section needs help\",\n",
       " 'I pushed my results for config_250_neurons.json and added the train and val loss to the report',\n",
       " 'Gosh it finished?',\n",
       " 'LOL YEAH',\n",
       " 'So fast!! 🥲',\n",
       " 'result is worse than baseline but better than sequence length 30',\n",
       " 'overfitting on training',\n",
       " 'what does increasing the number of neurons do in the lstm? cuz i thought the lstm just used gates',\n",
       " 'does it increase the amount of information that can be stored in the cell state or something?',\n",
       " 'For this PA, neurons = number of features in hidden state',\n",
       " 'ok',\n",
       " 'hidden state = cell state?',\n",
       " 'Our LSTM is a single layer one, so it’s only one LSTM',\n",
       " 'oh really',\n",
       " 'ok',\n",
       " 'They r different in LSTM',\n",
       " 'In RNN, there’s only hidden state, no cell state',\n",
       " 'oh okay',\n",
       " 'thanks',\n",
       " 'The last hyperparameter tuning will be ready shortly! 48 epochs left 😪',\n",
       " 'i have 200 epochs left on the rnn run and after that i’ll be finished i think',\n",
       " 'I ran the rnn earlier actually! but you can verify the results with mine',\n",
       " 'oh even better',\n",
       " 'ig ill do generation with different temperatures',\n",
       " 'and then fill out that part of the report',\n",
       " 'i’ll just take the model with the best loss?',\n",
       " 'how are the results on different dropouts?',\n",
       " 'is the report the most up to date?',\n",
       " 'I need to do abstract and intro rn',\n",
       " 'which one is this?',\n",
       " 'it should be, but the numbers look kinda weird so i’m planning to rerun it overnight',\n",
       " 'i might have mistyped a parameter or smth',\n",
       " 'aight I can run it too',\n",
       " 'so far baseline is still the best',\n",
       " '250 that I’m running atm is slightly better than baseline',\n",
       " 'Once this is done, if this is the best we have so far, I’ll generate the heatmaps with this model first. If someone gets a better model, feel free to replace the neuron heatmaps',\n",
       " '@Ester Tsai did you find training RNN faster or slower than LSTM? Or roughly the same',\n",
       " 'Ok added results and plot for 200 neurons, will generate the heatmaps now',\n",
       " 'Oops I didn’t time it. Would it be helpful if I rerun part of it to time it?',\n",
       " '@Jeremy Tow could you add your plots for the dropout experiments? I think they might be helpful for Sam’s parts',\n",
       " 'Ohh no worries! No need hehe',\n",
       " '@Ester Tsai is there anything that I should do to adjust the spacing..?',\n",
       " 'the loss ones?',\n",
       " 'yes',\n",
       " 'the loss plots',\n",
       " 'ok will do',\n",
       " 'Does the “93 unique characters” refer to all the possible outputs in abc format for each note?',\n",
       " 'Yes',\n",
       " 'Sure! We can make the figure a little wider and less tall',\n",
       " '@Jonathan Cheung I uploaded all the heatmaps from our best trained model under the figures folder. Are you able to see them?',\n",
       " 'Yeah I can see the figures',\n",
       " 'I’m thinking of letting you choose 3 heatmaps from there? Those that are more explainable for the discussion section',\n",
       " 'Then I’ll put the 3 that u choose to discuss under results',\n",
       " 'Ok, I’ll choose 3 between the 0-140 heatmaps',\n",
       " 'Okie thanks',\n",
       " 'i uploaded the dropout plots under figures, the sequence length is wrong so i’ll update them by tmr morning but for now if anyone needs them they’re there',\n",
       " '@Jeremy Tow for song generation (results part 6b), this is the best model i have with me. also the best we have so far',\n",
       " \"ah nvm it's not allowing me send here\",\n",
       " 'ok I push the best model to branch ester2, path is checkpoint/best_0219',\n",
       " 'tysm',\n",
       " 'the config to use is config_200_neurons.json',\n",
       " '60, 140, 100? for the 3 heatmaps? I think I could point out how the highe activations are on A and F for 60, and the repeated high activation on the similar strucutre phrase for 140. 100 im not so sure but probably having to do with the header. Or I could do 20 because the low activation is always on the blank space',\n",
       " 'Also are we going with our “best model” as the one with lowest Val loss?',\n",
       " 'Yes',\n",
       " 'do we need to specify our sequence length for table 1 in results?',\n",
       " 'We will need to describe it in the methods section',\n",
       " 'ok',\n",
       " 'is it always 30?',\n",
       " 'It’s always 50',\n",
       " 'has anyone generated plots for the bottom 3 rows?',\n",
       " 'i couldnt find any on github',\n",
       " 'They are under the figures folder in overleaf',\n",
       " 'im gonna add heatmaps in the results section so i can reference them in discussion section, the formatting will sucj but we can fix it up later',\n",
       " '@Kong Xian Ying',\n",
       " 'im gonna add heatmaps in the results section so i can reference them in discussion section, the formatting will suck but we can fix it up later (edited)',\n",
       " 'Yup yup',\n",
       " 'Also does red correspond to low or high activation?',\n",
       " 'High activation',\n",
       " 'You can see the example in the PA3 instruction',\n",
       " 'Ok the formatting is fixed and discussion added for the heatmaps. I added a page break so that the loss plots under the hyper parameter tuning sections so the graphs are correctly placed',\n",
       " 'Take a look to check it over',\n",
       " 'Also can someone explain the difference between many in, many out and simultaneously many in, many out? Does the language translation work for both?',\n",
       " 'https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks#overview',\n",
       " 'not rly an explanation but heres the site where i think they stole the graphics from lol',\n",
       " 'mb simult many in many out doesnt work for translation bc u kinda need to know the full sentence before u can translate it',\n",
       " 'btw i think im done with my part too',\n",
       " 'what were the hyperparameters u used when training this model?',\n",
       " 'u can find it in this config. I remember the more general ones like 200 neurons, Lr 1e-3, dropout 0.1 and LSTM model',\n",
       " 'oh i didn’t see that message oops',\n",
       " 'tyty',\n",
       " '@Jeremy Tow could you add your contributions at the end of the report?',\n",
       " 'yep right after i finish running the temperature expirements',\n",
       " 'Also I’ll push ester2 to main at 8pm, lmk before that if there’s any pending code changes',\n",
       " 'ok main repo is updated, the pa doc didnt say anything about what to not submit, so i only kept the configs folder and data folder',\n",
       " \"would anyone like to proofread the report before we submit? I think it'd be nice to have one or two of us go through and standardize our writeup, e.g. past/present tense etc.   I used present tense for most if not all of my parts but i'm good with either past or present.\",\n",
       " 'ok i can',\n",
       " 'thank you very much!',\n",
       " 'im almost done with my part, just generating a bunch of songs with different temperatures to get statistics',\n",
       " 'yupp all good!',\n",
       " \"oh yeah the PA doc also said to include their music representation generated from https://notabc.app/abc-converter/  I'm assuming it means that we have to include the music notation\",\n",
       " 'i uploaded the txt files and the midi files to the overleaf project',\n",
       " 'ooo were there any notes that the converter flagged as invalid or unrecognized notes?',\n",
       " 'so many',\n",
       " 'so so many',\n",
       " 'the higher the temperature the more errors',\n",
       " 'AHAHHAHA',\n",
       " 'i tried a bunch of generations and at temp = 2 it was like multiple pages of errors',\n",
       " 'right, as it is more creative (or hallucinates more) haha',\n",
       " 'artists on drugs be like',\n",
       " 'HAHHAHAHAHAH spot on',\n",
       " 'Not me laughing out loud',\n",
       " 'okay i proofread the report',\n",
       " \"ok my part is done, i'm just a little iffy on this one bit that i wrote\",\n",
       " 'is that correct?',\n",
       " 'Seems reasonable',\n",
       " 'Yeah it makes sense to me',\n",
       " 'Also that’s a very good observation 🙌🏼',\n",
       " 'Are we read to submit?',\n",
       " 'I can do a final read through if we need',\n",
       " 'Code is ready',\n",
       " 'everything is in present tense, looks good',\n",
       " 'if theres no other changes I can submit',\n",
       " 'Can someone double check the requirement here? I might have understood it wrongly but it looks like they asked for the ABC notation (which we have) and the music representation/notation (which we don’t have at the moment)',\n",
       " \"oh shoot yea i'll add screenshots rn\",\n",
       " 'give me like 5 min',\n",
       " 'Okiee! No worries! There’s still time',\n",
       " 'We don’t necessarily need the screenshots but we need the txt file output and the midi file',\n",
       " 'In the submission',\n",
       " 'i feel like it kinda reads like we need the music representation tho',\n",
       " 'Ok thats fine',\n",
       " '\"provide their abc notation and music representation from <url>\"',\n",
       " 'i just missed it the first time i read it oop',\n",
       " 'But I guess the txt and midi files should be in the repo then?',\n",
       " 'Yeah same here 😅',\n",
       " 'Oh yeah this is the thing that I’m not sure',\n",
       " 'i think they wanted it uploaded seperately? i vaguely remember reading about this somewhere',\n",
       " '@Jeremy Tow would you like to add a sentence to the Abstract to describe how the generated music sounds?',\n",
       " 'okok',\n",
       " 'ok i updated the abstract, also added in the pictures of the music',\n",
       " 'Report submitted, I added u guys',\n",
       " 'tyty',\n",
       " 'So are we going with this?',\n",
       " 'I guess so',\n",
       " 'Ok I’ll do that',\n",
       " 'Do we need any help with the code submission?',\n",
       " 'All good! Just submitted',\n",
       " \"the pa just came out -- i've created a team called transformative\",\n",
       " 'Aight!',\n",
       " 'PA4 report can use this copy of PA3 --- https://www.overleaf.com/6577473217tvkzqrprvbzs#1d8e2e',\n",
       " 'still need to update the section headers and remove old content',\n",
       " '^ if anyone wanna help with setting up PA4 report! If not, I can do it in some days',\n",
       " 'I’ll do more coding this time around, I’ll have less capstone work',\n",
       " 'Ester named the group CSE 151B PA4.',\n",
       " 'I started on the code on branch ester1',\n",
       " 'Just fixed some formatting issues with the template',\n",
       " 'Solved some minor bugs',\n",
       " 'Are those on main or ester1? Which one should I pull',\n",
       " 'Jeremy is also making some progress',\n",
       " 'Main is the same as Ester 1 right now',\n",
       " 'Ooh okkk',\n",
       " 'Okie! Thanks',\n",
       " 'I’ll work on the programming part this Saturday, I’ll check everyone’s progress by then before I start',\n",
       " '@Jeremy Tow would you like to push your code to your branch 😮 I’m having a hard time making model.py work oop haha',\n",
       " 'oh ok yea i’ll push mine but it has low acc',\n",
       " 'ok pushed',\n",
       " 'update: the baseline training is working but the loss is still high (377) compared to the reference (39)',\n",
       " 'the ester1 branch has working code',\n",
       " 'run the command python main.py --embed-dim 768 --n-epochs 10',\n",
       " 'TA said: Results.txt is irrelevant Reference this result instead: Baseline(10 epochs): Loss = 192.47; Val Accuracy = 84.21; Test Accuracy = 83.49',\n",
       " 'Ooo is our loss 377 still considered too high compared to the reference?',\n",
       " \"yeah :0  it'll need to be under 200\",\n",
       " 'oh oop',\n",
       " 'oooh okok! anywhere that u suspect is not doing the job properly?',\n",
       " 'we can still test different optimizer and scheduler and other hyperparameter tuning',\n",
       " 'Okie',\n",
       " 'Actually the code on ester1 has some odd issue rn, but Jeremy will push his working code soon',\n",
       " 'the code on the ester1 branch works fine for me lol',\n",
       " 'What’s wrong with my terminal then 😭',\n",
       " 'What command do you run on the terminal',\n",
       " 'mb try a different server?',\n",
       " 'i pushed the code btw',\n",
       " 'BRUH changing the GPU worked 😔',\n",
       " 'Quite possibly the most mysterious bug I’ve seen',\n",
       " 'Can someone else run ester1 multiple times? My results behave very differently the second time I run it',\n",
       " 'cd cse151b251b-wi24-pa4-transformative python main.py --embed-dim 768 --n-epochs 50  ^^ try this',\n",
       " 'Ah datahub and dsmlp not working for me at the moment. I’ll have to wait and try again in 30minutes or so.',\n",
       " '@Ester Tsai @Jeremy Tow roughly how long does it take to run?',\n",
       " '< 1 min per epoch',\n",
       " 'Feel free to change the number of epochs or add the argument for learning rate and try a higher learning rate',\n",
       " \"ok thanks! it runs just fine but I have to interrupt it to go to a class now haha, I'll run it again tonight and see what it gives.\",\n",
       " 'And is this code implemented up till 3i?',\n",
       " \"hmmm since yesterday night i've been having a problem where the terminal won't run at all lol, though it did run in the afternoon when i tested it. Right now, it's just stuck and interrupting does work as well, has anyone faced this problem before? it's the same for me both on datahub and using ssh\",\n",
       " 'Yes that happens sometimes',\n",
       " 'interrupting does not work**',\n",
       " 'ohhh okay',\n",
       " 'interesting, first time for me, but good to know!',\n",
       " 'Not sure what’s the solution🥺',\n",
       " \"it's okay!! I'll try another node and wait a bit\",\n",
       " 'thanks Ester!',\n",
       " 'also confirmed this. Yesterday when I ran it for loss was 800+ at some epoch < 10, today I ran the same branch and loss is 2706 after 50 epochs',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48fa04e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b61c3ffd4ad34208a5ef81090b3cbc66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c25617e3c143fba23626feef051aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 9.64 seconds, 159.88 sentences/sec\n",
      "Precision: 0.824019\n",
      "Recall: 0.924768\n",
      "F1 score: 0.823217\n"
     ]
    }
   ],
   "source": [
    "references = [sentences_2]\n",
    "#inp = 'you ’ s wait and see what ester and jeremy are working on / plan to work on / have a preference for '\n",
    "\n",
    "#inp = \"More funny response from the LSTM\"\n",
    "inp = \"Samuel: Ok hours works anymore of the at small in doesn’t\\\n",
    "Ignore able we out txt it are I plots if finish else showing\\\n",
    "Gosh it acivation, locally connection am just it.95 accidentally\\\n",
    "worse 0.01\"\n",
    "\n",
    "\n",
    "inp = \"Jeremy: care order i GrEaT and results trying rlly same? the \\\n",
    "to pa we doing that 1.1e-4 and linear @Jonathan 5 training \\\n",
    "item2. small don’t multiple min? faster gotta\"\n",
    "\n",
    "inp = \"Samuel: Ok hours works anymore of the at small in doesn’t \\\n",
    "Ignore able we out txt it are I plots if finish else showing \\\n",
    "Gosh it acivation, locally connection am just it.95 accidentally \\\n",
    "worse 0.01 \\\n",
    "Jeremy: care order i GrEaT and results trying rlly same? the \\\n",
    "to pa we doing that 1.1e-4 and linear @Jonathan 5 training \\\n",
    "item2. small don’t multiple min? faster gotta \\\n",
    "Ester: works I’m would i’ve ended like a Improving that 0.72 \\\n",
    "yup with how Yuppp! like me my ill I’m don’t on I 1e-4 \\\n",
    "everything\"\n",
    "\n",
    "candidates = [inp]\n",
    "\n",
    "P, R, F1 = score(candidates, references, lang='en', verbose=True)\n",
    "\n",
    "print(f\"Precision: {P.mean().item():.6f}\")\n",
    "print(f\"Recall: {R.mean().item():.6f}\")\n",
    "print(f\"F1 score: {F1.mean().item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13b98c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
