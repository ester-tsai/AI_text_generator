<start> Any progress on part 5? <end> <start> nothing significant yet, gonna work on it later tonight <end> <start> Also would anyone like to set up the report sections according to the PA instruction?     https://piazza.com/class_profile/get_resource/lr49jt48q953s2/lsw7akg438g2v https://www.overleaf.com/6577473217tvkzqrprvbzs#1d8e2e <end> <start> I will do that <end> <start> I‚Äôll also work on what is left for Part 4 this Sunday. @Jonathan Cheung @Samuel Chu lmk if you are already working on any of Part 4 (or Part 5) so I can plan my time accordingly.   Also I‚Äôll need help with getting started on the report, if any of you would like to work together with me, I‚Äôd really appreciate that!! <end> <start> I haven‚Äôt rlly looked but I can do part 5 <end> <start> We‚Äôre pretty good on part 4 since we‚Äôve done methods 1-3. Feel free to try methods 4-5 but we should aim to get part 5 done first! <end> <start> Yupp agreed!! <end> <start> Anyone going to OH tomorrow? <end> <start> Nope I won‚Äôt be <end> <start> I have a functional part 5 <end> <start> there are 2 parts to the supcon_train: the first training loop uses SupCon loss to train the model to output features that group similar data points together; the second training loop uses cross entropy loss to train the classifier to output predictions based on "features" input <end> <start> ester1 branch has the newest updates <end> <start> oh oop im still working on it rn <end> <start> oh btw did we submit the regrade request yet and if not shuld i do it <end> <start> feel free to try on your own or improve on mine! <end> <start> I can check <end> <start> tyty <end> <start> not submitted yet <end> <start> If you use ester1 code, you can run "python main.py --task supcon --embed-dim 768 --n-epochs 20 --classifier-n-epochs 10 --learning-rate 1e-4 --drop-rate 0.15 --temperature 0.07 --contrast-type SupCon" to test SupCon <end> <start> Can someone run it and see what the test accuracy is? <end> <start> It‚Äôs only 50% if I set n-epochs to 10 <end> <start> So we can try 20 <end> <start> ill run it <end> <start> rip no nodes avialble rn <end> <start> nvm its working now <end> <start> just finsihed running with 20 epochs, test acc is 0.5047 <end> <start> lol so more epochs does not help <end> <start> I've implemented util.plot <end> <start> YOOOOOOOOOOOOOOOO <end> <start> part 5 worked <end> <start> I think! <end> <start> Yooooooooooo <end> <start> Wowwwww <end> <start> ur actually amazing <end> <start> the loss has been updated to reflect mean loss, not total <end> <start> I just needed to make the learning rate much higher for the classifier <end> <start> high key it's scary when things work <end> <start> better sanity check <end> <start> when i finally figure out how to do it individually i‚Äôll also check ig <end> <start> but it actually putting in so much work it‚Äôs crazy tysm <end> <start> @Jonathan Cheung @Kong Xian Ying would you guys be down to set up the report by tonight? https://www.overleaf.com/6577473217tvkzqrprvbzs#1d8e2e <end> <start> or by tomorrow before 3pm <end> <start> Yeah I‚Äôll do so <end> <start> python main.py --task supcon  --batch-size 256 --contrastive-n-epochs 10 --n-epochs 10 --classifier-input-dim 768 --contrastive-learning-rate 1e-4 --learning-rate 1e-2  --contrastive-drop-rate 0.4 --temperature 0.07 --contrast-type SupCon <end> <start> ^ to get 88.23% test accuracy <end> <start> Imma run the models overnight to get these plots <end> <start> I haven't tested SimCLR yet <end> <start> python main.py --task supcon  --batch-size 256 --contrastive-n-epochs 10 --n-epochs 10 --classifier-input-dim 768 --contrastive-learning-rate 1e-4 --learning-rate 1e-2  --contrastive-drop-rate 0.4 --temperature 0.07 --contrast-type SimCLR <end> <start> ^ if anyone would like to test it <end> <start> Thank you! <end> <start> i'll run it, i need a break anyways <end> <start> You can get the code from ester1! Lmk if it works ü´£ <end> <start> Made fixes to the plots and corresponding stuff <end> <start> @Jeremy Tow how‚Äôs SimCLR? <end> <start> would anyone like to run this alternative SimCLR command using ester1 code before 3pm?  python main.py --task supcon  --batch-size 256 --contrastive-n-epochs 10 --n-epochs 15 --classifier-input-dim 768 --contrastive-learning-rate 1e-4 --learning-rate 5e-3  --contrastive-drop-rate 0.4 --temperature 0.07 --contrast-type SimCLR <end> <start> I‚Äôll run it <end> <start> accuracy is pretty low for SimCLR, might need some tuning <end> <start> Could you try different parameters? <end> <start> Yeah <end> <start> Has anyone read the SimCLR slides? <end> <start> i‚Äôve read the paper <end> <start> i‚Äôm in class rn but i‚Äôll update after i fj ish <end> <start> Btw If you run out of storage, you can comment out the part of the code in supcon_train in main.py that saves the model weight! <end> <start> It gives you the option to only train the classifier using a pre-trained contrastive model with this parameter   --contrastive-model-path SupCon_ep10_dropout0.4_lr0.0001_temp0.07_baseTemp0.07.pth <end> <start> Replacing the .pth portion with whatever the model weight name is in the model/ folder <end> <start> Reminder we are calling at 3! <end> <start> https://ucsd.zoom.us/j/97890026804 <end> <start> I‚Äôll join in one minute, finding a quiet space <end> <start> heres the run that I did last night <end> <start> https://docs.google.com/document/d/1nJQMPYDRe2u6rF9DQKSnqh1L9nu2qlguvrrr8XS9Ofc/edit <end> <start> https://huggingface.co/tasks/zero-shot-classification <end> <start> ^ i was looking at the inference section <end> <start> Counter({50: 131, 13: 126, 45: 123, 12: 105, 32: 102, 49: 90, 22: 82, 44: 73, 0: 64, 33: 63, 26: 55, 47: 50, 59: 50, 30: 47,36: 46, 9: 41, 53: 37, 42: 36, 20: 35, 58: 34, 10: 32, 48: 31, 19: 31, 57: 30, 54: 27, 6: 26, 21: 25, 2: 25, 3: 24, 4: 24, 1: 22, 51: 22, 11: 22, 16: 20, 34: 19, 23: 19, 27: 18, 40: 17, 31: 17, 43: 16, 17: 16, 46: 15, 25: 15, 52: 14, 56: 14, 39: 13,14: 12, 18: 12, 55: 12, 38: 9, 28: 8, 35: 8, 24: 7, 41: 5, 8: 5, 15: 5, 5: 2, 7: 2, 37: 2}) <end> <start> in fact 50 is the most common label <end> <start> üëÄ <end> <start> Question is, how does BERT know which label is the most popular : 0 <end> <start> Train: 11514 Counter({50: 810, 45: 639, 13: 573, 32: 566, 12: 555, 49: 544, 22: 503, 44: 418, 33: 354, 0: 350, 30: 312, 47: 283, 36: 283,26: 267, 42: 227, 9: 207, 59: 198, 58: 193, 6: 190, 48: 182, 21: 177, 19: 173, 53: 164, 57: 154, 40: 153, 4: 152, 20: 150, 10: 142, 16: 135, 23: 130, 2: 127, 17: 127, 1: 125, 56: 124, 3: 122, 11: 117, 43: 113, 51: 112, 14: 110, 46: 110, 27: 108, 54:100, 34: 93, 52: 78, 39: 78, 31: 76, 18: 76, 25: 72, 55: 70, 15: 54, 8: 52, 35: 52, 38: 52, 28: 51, 24: 48, 5: 25, 41: 22, 29: 18, 7: 14, 37: 4}) train acc: 0.0672 | dataset split train size: 11514  Validation: 2033  Counter({50: 131, 13: 126, 45: 123, 12: 105, 32: 102, 49: 90, 22: 82, 44: 73, 0: 64, 33: 63, 26: 55, 47: 50, 59: 50, 30: 47,36: 46, 9: 41, 53: 37, 42: 36, 20: 35, 58: 34, 10: 32, 48: 31, 19: 31, 57: 30, 54: 27, 6: 26, 21: 25, 2: 25, 3: 24, 4: 24, 1: 22, 51: 22, 11: 22, 16: 20, 34: 19, 23: 19, 27: 18, 40: 17, 31: 17, 43: 16, 17: 16, 46: 15, 25: 15, 52: 14, 56: 14, 39: 13,14: 12, 18: 12, 55: 12, 38: 9, 28: 8, 35: 8, 24: 7, 41: 5, 8: 5, 15: 5, 5: 2, 7: 2, 37: 2}) validation acc: 0.059 | dataset split validation size: 2033   Test: 2974 Counter({50: 209, 45: 176, 12: 169, 13: 156, 49: 141, 32: 126, 22: 124, 44: 119, 33: 114, 0: 88, 47: 81, 9: 72, 36: 72, 30: 67, 58: 63, 26: 57, 53: 52, 42: 51, 59: 51, 40: 43, 6: 43, 48: 41, 20: 41, 21: 39, 10: 39, 1: 36, 43: 36, 56: 36, 3: 35, 57: 35, 2: 35, 51: 35, 23: 34, 46: 32, 19: 31, 18: 27, 34: 26, 4: 26, 17: 26, 27: 25, 39: 25, 54: 23, 16: 22, 52: 21, 31: 21, 55:20, 25: 19, 8: 18, 38: 15, 11: 15, 14: 13, 15: 12, 35: 11, 24: 10, 29: 6, 28: 6, 7: 4, 41: 3, 5: 1}) test acc: 0.0689 | dataset split test size: 2974 <end> <start> Yeah looks like answering this = answer to that question <end> <start> Or maybe how does the classifier know which label is the most popular ? <end> <start> oh right i forgot to mention this but im prob going to submit the regrade request later today <end> <start> label 50 is calendar set <end> <start> no idea how it knows tho <end> <start> are you that you aren‚Äôt training it at all <end> <start> I just added train loss to plot names! Please git pull ester1 if you plan to use that code <end> <start> just made another push to use the best model (best val accuracy) instead of final model <end> <start> Confirmed with TA, kinda what we went over just now. So I guess we just have to figure out why test accuracy is roughly the percentage of the dominant class or is it just coincidence  ‚ÄúBefore fine-tuning means: we use BERT + linear classifier to get our target predictions without any training.‚Äù <end> <start> So essentially it goes down to what the linear classifier is predicting? <end> <start> All the plots for report are on GitHub in the plots/good/ folder <end> <start> Except for SimCLR <end> <start> lowering temperature and increasing input-classifier-dim seems to increase test accuracy for SimCLR! <end> <start> best I've gotten is test acc 63.99% using       python main.py --task supcon  --batch-size 512 --contrastive-n-epochs 20 --n-epochs 40 --classifier-input-dim 3000 --contrastive-learning-rate 1e-4 --learning-rate 1e-2  --contrastive-drop-rate 0.1 --temperature 0.01 --base-temperature 1 --contrast-type SimCLR <end> <start> How did you get rid of the cuda memory error, I‚Äôm been getting ~50% by playing with batch sizes <end> <start> You‚Äôll need to set the batch size to something less than 1000 <end> <start> 512 still works <end> <start> Bigger batch size will cause cuda out of memory error <end> <start> If you use node 30 on DSMLP you get a GPU with more GB of memory <end> <start> oo highest I got so far is 67.48% <end> <start> python main.py --task supcon  --batch-size 512 --contrastive-n-epochs 20 --n-epochs 40 --classifier-input-dim 3000 --contrastive-learning-rate 1e-4 --learning-rate 1e-2  --contrastive-drop-rate 0.2 --temperature 0.01 --base-temperature 1 --contrast-type SimCLR <end> <start> looking at SimCLR now <end> <start> good call on using large batch size @Ester Tsai!! I read Lilian Weng's blog that SimCLR does require large batch size to perform well. <end> <start> checked the implementation of SimCLR, it looks all good to me (thanks Ester!), I'm reading papers on the effect of some hyperparameters on it. <end> <start> if we haven't already tried, we could try lower learning rate for classifier (1e-4) <end> <start> SimCSE is using dropout rate of 0.1, and from the doc I see that Ester has tried that <end> <start> side note! rn we use the cosine annealing scheduler for the classifier so the learning rate will decrease over the epochs <end> <start> we should still try different starting learning rates tho! <end> <start> or even different schedulers <end> <start> okie! <end> <start> @Jeremy Tow also.. I kinda suspect that there's actually only 2 published research papers that use Amazon Massive Intent - maybe that's why the instruction explicitly states that we get full credits for related work as long as we have 2 papers that use this dataset lol  I could be wrong though, just couldn't find any other papers. <end> <start> paper 1: https://arxiv.org/pdf/2204.08582.pdf  paper 2: https://arxiv.org/pdf/2310.16609.pdf (their model on HF https://huggingface.co/cartesinus/xlm-r-base-amazon-massive-intent) <end> <start> I'm trying SGD optimizer for the contrastive training loop <end> <start> @Ester Tsai just confirming, we are going with custom 1&3 right? <end> <start> yeah let's do that if you guys are down! <end> <start> yeah sounds good to me <end> <start> @Jeremy Tow@Jonathan Cheung@Samuel Chu How's progress? <end> <start> @Ester Tsai do you also get very low contrastive loss? like 0.0 <end> <start> yes <end> <start> I'm thinking if it is the classifier that's not learning <end> <start> i think i figured out SimCLR <end> <start> will get back to yall in 20 mins <end> <start> Removing the scheduler yields 61% without learning rate modifications <end> <start> The batch size doesn‚Äôt seem to have significant difference compared to 256, though I‚Äôm not on gpu 30 and other params are TBD <end> <start> for SimCLR, it's our model architecture. there are two things we might have missed, i am still checking:  1. we need to drop the projection head when passing to the classifier. we still train supconmodel with the projection head, it is only when training the classifier, we want the hidden representations instead of the projection head output  2. we need to apply dropout two times for unsupervised SimCLR. I am still looking at the paper for details of implementation <end> <start> https://arxiv.org/pdf/2104.08821.pdf <end> <start> my post on piazza: https://piazza.com/class/lr49jt48q953s2/post/634 <end> <start> ok so we took care of item2. I will work on item1 <end> <start> I see! That makes so much sense! <end> <start> I believe I‚Äôve successfully done this and got 0.692 <end> <start> Don‚Äôt know if Params matter as much though training acc becomes much higher at 0.78 <end> <start> By making forward pass return l2 and the final thing, and taking the l2 into the classifier on the eval step <end> <start> I have been experimenting on flipping the normalization and linear head in final_output but don‚Äôt know if it‚Äôs a fluke <end> <start> Yup yup this sounds right <end> <start> I‚Äôm running rn after moving the scheduler update thing into the epoch for loop rather than batch <end> <start> Yup this too <end> <start> Also try removing the cross entropy loss reduction = sum <end> <start> Just use the default <end> <start> Why does the loss sometimes become nan <end> <start> After many epochs of 0 it becomes nan and it breaks <end> <start> classifier or supcon? <end> <start> I mean the mean train loss <end> <start> if you are referring to the mean train loss of supcon (the first training loop), it always reaches 0.0 within 10 epochs for me <end> <start> so i just set it at 10 epochs for contrastive-n-epochs <end> <start> also, might want to add a new argument to this <end> <start> It becomes man for em sometimes too, especially when temperature is lower than 0.01 <end> <start> because if it is taking classifier input dim, it'll always be 768, and the linear is doing 768 -> 768, which is not really meaningful <end> <start> and classifier input dim has to be 768, since it is taking the encoder's embeddings when we are doing SimCLR <end> <start> Yep that‚Äôs right <end> <start> was able to get 71% val acc, and 88% train acc by epoch 7 of training the classifier, just fixing some minor issues on my end right now. I will run a few more experiments and let yall know how it goes. <end> <start> Messed up a line and my test acc became 0.02 üíÄ <end> <start> hahaha high five, been there, done that, i was debugging for like 2 hours <end> <start> that's great! <end> <start> Were u able to move the scheduler step out of the batch loop? <end> <start> Seems to be breaking my code <end> <start> yup <end> <start> lemme push my code to winfrey and maybe you can cross check from there <end> <start> ok my branch is fully updated <end> <start> i'm mostly testing 2 hyperparameters now 1. supcon-linear-head-dim 2. hidden-dim (of the classifier, because we have been using the default 10, so its going 768 -> 10 -> 60) <end> <start> OHH REALLY <end> <start> I thought hidden dim isn‚Äôt used for some reason <end> <start> It‚Äôs a lot of args going on üòÇ <end> <start> O DANG you‚Äôre right <end> <start> It‚Äôs being used by Classifier class <end> <start> OOPS <end> <start> Yeah no worries, I only realized that very much later, the 71% just now is with hidden dim 128 (for Classifier), and sup con linear head dim 128 (for SupConModel) <end> <start> the linear head dim can go pretty high, like 3000 <end> <start> True true! <end> <start> btw I forgot to share earlier! but if you want the DSMLP GPU to have a high time limit (24 hours instead of 6 hours), you can run these commands in the terminal before your GPU launch scrupt <end> <start> export K8S_PRIORITY_CLASS_NAME=normal export K8S_TIMEOUT_SECONDS=86400  export K8S_BYPASS_TIMEOUT_LIMIT=yes <end> <start> Oh my goodness this is so very much needed <end> <start> Thank you <end> <start> added a new table just for SimCLR in the results google doc <end> <start> WAIT <end> <start> WAIT <end> <start> Yes sir <end> <start> VAL ACC 0.83 on 3 epochs <end> <start> Niceeeee <end> <start> sheesh????????? <end> <start> Lesgoooo <end> <start> The train acc is getting to 99% <end> <start> Goodnight everyone <end> <start> HAHAHAHAH <end> <start> What‚Äôs the magic parameter <end> <start> Let me finish the training <end> <start> yuh please send the command line <end> <start> Okieeeee <end> <start> It‚Äôs in the process but should take less than the usual 40 <end> <start> wanna send it rn haha I'll run it along with my other string of commands <end> <start> Oh and push ur code we can just use urs <end> <start> python main.py --task supcon  --batch-size 256 --contrastive-n-epochs 7 --n-epochs 40 --classifier-input-dim 3500 --contrastive-learning-rate 1e-4 --learning-rate 1e-2  --contrastive-drop-rate 0.2 --temperature 0.01 --base-temperature 1 --contrast-type SimCLR --hidden-dim 128 <end> <start> if epcosh is greater than 7 the loss becomes nan <end> <start> Is ur classifier-input-dim actually the classifier‚Äôs input dim? <end> <start> oh yeah ill make a branch bc ive been workignon a copy of esters <end> <start> Yeaaah okkk <end> <start> Thanks Jonathan!! <end> <start> i modifed this line"classifier = Classifier(args, args.embed_dim, target_size).to(device)" <end> <start> shoudl be  line ~298 in main.py <end> <start> bc i was gettign errors from reshaping <end> <start> though idk if its sketch <end> <start> what did you modify it to? <end> <start> args.embed_dim will have to be 768 <end> <start> it was modifed to that, it was args.classifier_input_dim <end> <start> caused an error bc 256x3500 can do matmul wiht 768x10 <end> <start> or 128 in this case <end> <start> Niceeee we can take a look at your code and see if it‚Äôs just naming differences which we can always adjust later <end> <start> btw for CrossEntropyLoss(reduction='sum'), it makes the cross entropy loss return the sum instead of the mean for each batch, so we can divide by total number of data points and get the mean over the whole set <end> <start> if we removed the reduction='sum', it'll be hard to calculate loss consistently <end> <start> Also, very good call on this Ester, val acc is already above 80% at epoch 3 with 1200 linear head dim <end> <start> OHHHH Okok we should put it back then, Soz didn‚Äôt think that far <end> <start> haha I tried 10000 once and might've gotten CUDA out of memeory <end> <start> Pushing the limits we like <end> <start> OOOO I got good results similar to jcheung's <end> <start> python main.py --task supcon  --batch-size 512 --contrastive-n-epochs 20 --n-epochs 40 --hidden-dim 128 --linear-head-dim 3000 --contrastive-learning-rate 1e-4 --learning-rate 1e-2  --contrastive-drop-rate 0.2 --temperature 0.01 --base-temperature 1 --contrast-type SimCLR <end> <start> Yooooooooooo <end> <start> the epochs can be lower and probably still perform the same <end> <start> 82.92% <end> <start> Niceeeeee wow I‚Äôm so glad we worked this out together üòçüòçüòç <end> <start> i made my branch but it looks like ester branch from a week ago, im a brnach noob does anyone know how to update it <end> <start> plot looks reasonable <end> <start> Create a pull request from Ester to ur branch <end> <start> YUH thanks to your important fixes XD <end> <start> I find it easier to do this on GitHub than command line <end> <start> 1. Pull request from Ester to ur branch 2. Then only push ur code to ur branch <end> <start> oh wait im so dumb <end> <start> im wokring on datahub lol <end> <start> not the actual repo on my computer <end> <start> Well, there‚Äôs also brute force, download the py files and upload it, almost guaranteed to work always ü§£ when there‚Äôs too many merge conflicts I just brute force hahaha <end> <start> o wait ester 1 is not updated <end> <start> let me do that rn <end> <start> ok i pshed to my branch <end> <start> the comments and stuff r still there so its super messy but u hsould be able to see the changes <end> <start> mainly on the forward pass and the respective changes from 2 outputs in the eval and clr training loops <end> <start> I had merge conflict so I couldn't properly push my code lolol I ended up copy pasting my changes into ester1 <end> <start> I'm rerunning all the other models after the code fix, but I'm down to just use what we already have on the report to save time changing everything out <end> <start> This plot is from python main.py --task supcon  --batch-size 512 --contrastive-n-epochs 20 --n-epochs 40 --hidden-dim 128 --linear-head-dim 3000 --contrastive-learning-rate 1e-4 --learning-rate 1e-2  --contrastive-drop-rate 0.2 --temperature 0.01 --base-temperature 1 --contrast-type SimCLR <end> <start> I put it in the figures folder for Overleaf but feel free to replace it if something better comes up <end> <start> 82.92% test acc <end> <start> New result for supcon loss: 88.9% at epoch 3 <end> <start> Aaaaaa <end> <start> Noiceeee <end> <start> 83.15% SimCLR <end> <start> Are we confirmed using that for the report? <end> <start> I can fill in the values in the chart and in the abstract then <end> <start> I have a better plot possibly <end> <start> Let me update it on overleaf rn <end> <start> After fixing our code, the baseline is mega good <end> <start> 89% <end> <start> No other model gets better than baseline <end> <start> Should we keep our old set of plots to save time or hyperparameter tune all the models until they are optimized? <end> <start> I‚Äôm fine with keeping out old set <end> <start> The Discussion section still has many blanks. Please help fill it out! I might go to OH at 5pm to ask questions <end> <start> I've at least updated the SupCon and SimCLR plots, but not yet the other ones. I'm also down to keep the other ones the same since we've already written a lot about them. But I also have the new plots after the code fix available if we want to swap the old ones out. <end> <start> yup, i will fill out my parts today. <end> <start> yeah, i'm down with either. <end> <start> is the acc for the fine tuned models very different from the table we have right now? <end> <start> all are around 88% <end> <start> not that different <end> <start> i see okay <end> <start> well.. at least for my part of the discussion, it is easier to discuss that adding fine-tuning strategies improves the model <end> <start> but i can do more research <end> <start> if we want, we can embrace the research mindset by reporting the latest results <end> <start> TRUE <end> <start> yeah, i'm kind of leaning towards reporting our final and most consistent results <end> <start> do you need help with running any more or you have everything we need already? <end> <start> I still need help with hyperparameter tuning <end> <start> for custom 3 and custom1and3 <end> <start> just saw that you took SimCLR on the report! thank you thank you <end> <start> done with baseline and custom1 <end> <start> okkk i will do that tonight, will build off what you have in the results doc <end> <start> These are the new plots that we can use, minus custom3 and custom1and3 <end> <start> I'll put them on Overleaf <end> <start> oooh so custom1 itself is slightly tiny bit better than baseline <end> <start> yeye just got the newest best result from hyperparameter tuning <end> <start> ^these are the current best custom1 and custom1and3 <end> <start> to summarize: I've updated the corresponding report parts for baseline, custom1, and SimCLR. Still waiting on some more SupCon to run for better consistency. Need help on hyperparameter tuning and updating the report for custom3 and custom1and3 <end> <start> Do we hv numbers for technique 2 and the combined ones or is it being rerun <end> <start> These are the current best stats but it is being rerun <end> <start> Also did using the losses rather than the 2 techniques trading faster or was there no significant difference? <end> <start> Wdym by using the losses? <end> <start> Clr and supcon <end> <start> Like do those model train faster in any significant way? <end> <start> I didn‚Äôt keep track of that hmm <end> <start> Feel free to track it <end> <start> Ester1 should be updated <end> <start> For this block in the result section: "For the second technique, we initialize the weights of the last layer in BERT before fine-tuning it.95 Both training and validation accuracies increase very rapidly for the first 4 epochs. The training96 accuracy continues to improve, but the validation accuracy plateaus at around 86%, showing signs of97 overfitting. The final test accuracy is 86.28%. The training stops early at epoch 17 after the validation98 accuracy decreases for 3 epochs in a row. The best model based on the highest validation accuracy99 occurs at epoch 14.100 NEEEEEEEEEEEEED UPDATE!!!!!!!!!!!!!!!!!!!! " do we still need this or is it jsut misplaced section?? <end> <start> like do we need to commnet on each graph or should this be put in the technique 2 section <end> <start> We need to comment on each plot right <end> <start> hmmmm.. the exact wording from the doc is summarize the results, but we can always play safe and mention it. <end> <start> we are meeting at 7pm this Wednesday for final project, right? <end> <start> Yes <end> <start> @Ester Tsai if ester1 branch is the most updated one, custom_train in main.py we gotta move the model.scheduler.step() outside of the batch loop <end> <start> typo from the starter code that got carried forward oopss <end> <start> I moved the scheduler step out to the epoch loop, and the best val acc is above this here at epoch 5 <end> <start> Ohhhhhh <end> <start> I fixed it once but it didn‚Äôt save, so that might be why I missed it when I fixed a second time <end> <start> I got good supcon plot <end> <start> Ooohh I seeee okkk thanks! <end> <start> task=custom1and3 testAcc=0.8907 bestValAcc=0.8942 bestTrainAcc=0.9931 mean_loss=0.0194 ep=20 batch_s=16 drop=0.2 LR=5e-06 hid=128 <end> <start> custom1and3 just slightly worse than baseline <end> <start> Is the scheduler step in the epoch loop when u ran them? Just curious if u have changed it locally <end> <start> oh right! <end> <start> need to rerun <end> <start> üò¨üò¨ I can help run a chunk when I get home in like 45 minutes <end> <start> Thanks for running so many of them <end> <start> üò≠üôåüèº <end> <start> the 5pm TA  is very confused <end> <start> just quit his OH lol <end> <start> I went to his before! and I remember the session I went wasn‚Äôt very helpful too <end> <start> oh shoot sorry i've been afk for most of today. i jst added a bit to the related works and i can help do runs as well <end> <start> which exact ones need reruns? <end> <start> all the custom ones <end> <start> I updated ester1 to fix the scheduler.step <end> <start> these two? <end> <start> yes, and the combined <end> <start> okok i will run those <end> <start> custom1, custom3, and custom1and3 <end> <start> Ester has a chunk of command lines written in the results doc, you can get some from there and modify as you would like to <end> <start> task=custom1and3 testAcc=0.8944 bestValAcc=0.8952 bestTrainAcc=0.9997 mean_loss=0.0036 ep=20 batch_s=16 drop=0.2 LR=5e-06 hid=1000 <end> <start> wow lol that little bug fix solved many things <end> <start> Niceeeee <end> <start> i see your log of code that you will run at 520pm, is there any block that is not in the queue now? <end> <start> SupCon loss is currently worse than the custom ones <end> <start> I also haven't gotten to run any custom1 and custom3 commands <end> <start> i'll run the custom3 commands <end> <start> ok ill do custom1 <end> <start> yuh yuh I'm keeping hidden-dim=1000 for all of them <end> <start> mostly play around with learning rate and drop rate, but 0.2 seems to be the best? <end> <start> 0.2 drop rate* <end> <start> okk! <end> <start> task=custom1 testAcc=0.8914 bestValAcc=0.8957 bestTrainAcc=0.9997 mean_loss=0.0022 ep=19 batch_s=16 drop=0.2 LR=7e-06 hid=1000 <end> <start> ^ just got this! lmk if you get somehting higher : D <end> <start> I got above 89% for all custom models <end> <start> lmk if anything better comes up! Or I can just put these plots on Overleaf <end> <start> Nice nice mine is still running and ntg is higher so far <end> <start> ^ yea same <end> <start> custom1 is on its last one, i am good with going with what you have there. do we still need to run supcon? <end> <start> ^ I'm assuming you meant SimCLR <end> <start> The best is 89.14%, while custom1and3 is 89.51% <end> <start> I‚Äôm ok with calling it good enough haja <end> <start> yes agreed <end> <start> Actually I meant SupCon! <end> <start> oooo <end> <start> SimCLR is 83.15 <end> <start> And it‚Äôs alright since it‚Äôs above 80% <end> <start> yeah <end> <start> All the plots had been updated! <end> <start> ok i will do supcon now <end> <start> Do you mean running more of supcon or writing about it in report? <end> <start> oh wait actually <end> <start> none of the supcon uses the custom techniques, could we say that if the discussion asks us to discuss the results? <end> <start> I‚Äôm updating the CLR chart and doing the description of the plot - do we have updated params and values for that? <end> <start> Also abstract and intro r up to date <end> <start> The table and plot are updated! <end> <start> Oh wait I misread the graph lol <end> <start> Does anyone know what SimCLR stands for lol <end> <start> A Simple framework for learning Contrastive Learning of Visual Representations <end> <start> literally HAHAH <end> <start> For explaining the plots, remember to \ref the figure! <end> <start> i'm pretty much done with my parts on the report. lmk if any of you need me on any parts, if not I'll wait a bit and see where else I can add to. <end> <start> updated github main readme <end> <start> the best i got was all 0.88 so yea <end> <start> ok I edited the introduction + expanded on the discussion section and finished the related works section <end> <start> Wow our Related Works section is stacked <end> <start> love that <end> <start> would you like to merge your branch to main @Ester Tsai? <end> <start> ok! <end> <start> I submitted code, but it'd be helpful if someone git clones the main branch to check if everything works smoothly <end> <start> haha I dropped in a place for most Piazza posts <end> <start> dang both y‚Äôall are on the leaderboard üòÇ <end> <start> 70 contributions is crazy <end> <start> 8 per week <end> <start> I asked this question on Piazza: <end> <start> @Samuel Chu if the TA answers it, would you like to add whatever we are missing to the report? <end> <start> I also asked: <end> <start> I have update Q10 with a more comprehensive answer with respect to simclr and supcon and added a citation in introduction - after I finish if there‚Äôs nothing else I can submit <end> <start> I don‚Äôt get it lol, our un-tuned model is not predicting randomly <end> <start> mb our model is plainly just better <end> <start> built different <end> <start> can you all check on your end what your test accuracy is before training? <end> <start> @Kong Xian Ying @Jeremy Tow @Jonathan Cheung @Samuel Chu ? <end> <start> send in the chat asap haha so we can see if my higher number is just due to random chance <end> <start> maybe try a different seed too <end> <start> What are the most common predictions? Wanna check with a print statement in run_eval? <end> <start> seed of 2 <end> <start> oh okok <end> <start> Jeremy what‚Äôs the line you‚Äôre running <end> <start> Doesn‚Äôt matter, just run baseline ü§ù <end> <start> ok wait i got a 1 percent lol <end> <start> What matters is that run_eval comes before baseline_train <end> <start> it might have been random chance <end> <start> we can prob do some statistical analysis to see how probable that is? <end> <start> Wow is mine just too cracked of a seed, somehow the two most popular labels happen to be guessed over and over <end> <start> I think my code broke or something my baseline has gets acc of 0.8662 <end> <start> How about Before you run training? <end> <start> Might have switched up a paean to use a diff model lol <end> <start> Oh lol ok we can go with Jonathan‚Äôs number <end> <start> For the report <end> <start> Could someone update the table and the Discussion Q2? <end> <start> This is so goofy <end> <start> I‚Äôll rewrite Q2 to say they match <end> <start> Wait <end> <start> Not exactly but same order of Magnitude?? I guess <end> <start> 1/60 is 1.67% while out test acc is about 0.98% <end> <start> So close enough?? <end> <start> im running stats let me give u numbers in a sec <end> <start> im going to run it 100 times and get means + stddevs <end> <start> our seed was actually just built different <end> <start> two percent chance <end> <start> üòÇüòÇüòÇüòÇ <end> <start> mean: 0.01862474781439139 std: 0.015202989747957751 <end> <start> @Jonathan Cheung you can put these numbers into the report <end> <start> over 100 different seeds <end> <start> I‚Äôll out that in Q2 <end> <start> The chart I‚Äôll put the mean number in <end> <start> Ok updated <end> <start> Ready for submit? <end> <start> for my parts yea <end> <start> I‚Äôm gonna submit for now, if there‚Äôs any more changes lmk <end> <start> This is super useful info! <end> 