<start> Jonathan: I‚Äôve got something around 0.06 IOU, I‚Äôll do my write up part but if we get something better by tmrw I can update it accordingly <end> <start> Jonathan: I did the abstract too <end> <start> Winfrey: Also, not exactly sure about the metric IoU but now I‚Äôm not sure if it can be expressed in percentage <end> <start> Winfrey: I have zero prior knowledge about IoU though, just purely based on articles that I read they put it in decimal <end> <start> Jeremy: i think it makes more sense in decimal <end> <start> Ester: Sure! <end> <start> Jeremy: ig its like the intersection is x percent of the union? <end> <start> Jeremy: but idk <end> 

<start> Ester: it trains pretty fast too <end> <start> Jonathan: 2 epochs gets training iou of 0.099 <end> <start> Jonathan: I modified kernel size as well <end> <start> Ester: Could you see if it gets higher than 0.12? <end> <start> Jonathan: 0.11 with leaky relu <end> <start> Winfrey: @Ester Tsai do you have a preference for pushing to main or pushing to my branch for you to compile the models? I‚Äôm only pushing train_5_c.py and unet.py <end> <start> Winfrey: For me to push to main or my branch** whichever is easier for you grab <end> <start> Jonathan: hows this <end> 

<start> Samuel: im trying to run the main.py on your branch on gpu but im getting this error: <end> <start> Samuel: is it supposed to be config['dropout']? <end> <start> Ester: O yeah small typo <end> <start> Samuel: okay <end> <start> Ester: It‚Äôs config[‚Äúmodel_type‚Äù] <end> <start> Samuel: okay thaks <end> <start> Samuel: i got the same error that jonathan got and you were getting earlier <end> <start> Samuel: using datahub tho. i could try ssh <end> <start> Ester: Piazza someone said they created a new conda environment and installed pytorch and it worked <end> <start> Samuel: oh okay. ill try that <end>

<start> Jeremy: wait i‚Äôm not ready <end> <start> Ester: what time works for you? <end> <start> Jeremy: like in 5 mknjtes <end>

<start> Samuel: i tried ssh'ing into capstone gpu but that gave me the same error also <end> <start> Ester: To clarify the most updated branch is ester2! Ignore the original Ester <end> <start> Samuel: @Kong Xian Ying do you think you help me fix the gpu error i‚Äôm having with capstone gpu later today? <end> <start> Winfrey: Oh yeah ofc <end> <start> Winfrey: Is 8.30 ok? <end> <start> Samuel: hmm i might not be home til 9-9:15üò¨ <end> <start> Samuel: would 9:15 work? <end> <start> Winfrey: Okiee <end> <start> Samuel: thanks <end>

<start> Samuel: ok <end> <start> Winfrey: They r different in LSTM <end> <start> Winfrey: In RNN, there‚Äôs only hidden state, no cell state <end> <start> Samuel: oh okay <end> <start> Samuel: thanks <end> <start> Winfrey: The last hyperparameter tuning will be ready shortly! 48 epochs left üò™ <end> <start> Jeremy: i have 200 epochs left on the rnn run and after that i‚Äôll be finished i think <end> <start> Ester: I ran the rnn earlier actually! but you can verify the results with mine <end> <start> Jeremy: oh even better <end> <start> Jeremy: ig ill do generation with different temperatures <end> <start> Jeremy: and then fill out that part of the report <end> <start> Jeremy: i‚Äôll just take the model with the best loss? <end> <start> Ester: how are the results on different dropouts? <end> <start> Ester: is the report the most up to date? <end> <start> Jonathan: I need to do abstract and intro rn <end> <start> Ester: which one is this? <end> <start> Jeremy: it should be, but the numbers look kinda weird so i‚Äôm planning to rerun it overnight <end> <start> Jeremy: i might have mistyped a parameter or smth <end> <start> Ester: aight I can run it too <end> <start> Ester: so far baseline is still the best <end> <start> Winfrey: 250 that I‚Äôm running atm is slightly better than baseline <end> 

<start> Jonathan: I haven‚Äôt rlly looked but I can do part 5 <end> <start> Ester: We‚Äôre pretty good on part 4 since we‚Äôve done methods 1-3. Feel free to try methods 4-5 but we should aim to get part 5 done first! <end> <start> Winfrey: Yupp agreed!! <end> <start> Ester: Anyone going to OH tomorrow? <end> <start> Winfrey: Nope I won‚Äôt be <end> <start> Ester: I have a functional part 5 <end> <start> Ester: there are 2 parts to the supcon_train: the first training loop uses SupCon loss to train the model to output features that group similar data points together; the second training loop uses cross entropy loss to train the classifier to output predictions based on "features" input <end> <start> Ester: ester1 branch has the newest updates <end> <start> Jeremy: oh oop im still working on it rn <end> <start> Jeremy: oh btw did we submit the regrade request yet and if not shuld i do it <end> <start> Ester: feel free to try on your own or improve on mine! <end>

<start> Ester: just made another push to use the best model (best val accuracy) instead of final model <end> <start> Winfrey: Confirmed with TA, kinda what we went over just now. So I guess we just have to figure out why test accuracy is roughly the percentage of the dominant class or is it just coincidence  ‚ÄúBefore fine-tuning means: we use BERT + linear classifier to get our target predictions without any training.‚Äù <end> 

<start> Winfrey: Also, very good call on this Ester, val acc is already above 80% at epoch 3 with 1200 linear head dim <end> <start> Winfrey: OHHHH Okok we should put it back then, Soz didn‚Äôt think that far <end> <start> Ester: haha I tried 10000 once and might've gotten CUDA out of memeory <end> <start> Winfrey: Pushing the limits we like <end> <start> Ester: OOOO I got good results similar to jcheung's <end> 

<start> Jonathan: What is this? <end> <start> Ester: Don't do that haha <end> <start> Ester: it removes CUDNN basically <end>

<start> Samuel: dang both y‚Äôall are on the leaderboard üòÇ <end> <start> Ester: 70 contributions is crazy <end> <start> Ester: 8 per week <end> <start> Ester: I asked this question on Piazza: <end> <start> Ester: @Samuel Chu if the TA answers it, would you like to add whatever we are missing to the report? <end> <start> Ester: I also asked: <end> <start> Jonathan: I have update Q10 with a more comprehensive answer with respect to simclr and supcon and added a citation in introduction - after I finish if there‚Äôs nothing else I can submit <end> <start> Ester: I don‚Äôt get it lol, our un-tuned model is not predicting randomly <end> <start> Jeremy: mb our model is plainly just better <end> <start> Jeremy: built different <end> <start> Ester: can you all check on your end what your test accuracy is before training? <end> <start> Ester: @Kong Xian Ying @Jeremy Tow @Jonathan Cheung @Samuel Chu ? <end> <start> Ester: send in the chat asap haha so we can see if my higher number is just due to random chance <end>

<start> Ester: typo here lol <end> <start> Ester: I can fix and resubmit <end> <start> Jonathan: Lol thanks <end> <start> Jeremy: oh thats a good idea. i think the main thing we would have to figure out would be how to make it different tho <end> <start> Winfrey: Wdym by different? <end> <start> Winfrey: Also, just saying that, we are not set set on the idea so feel free to explore other ideas and we can go over all ideas together tomorrow <end> <start> Jeremy: i feel like it'd be pretty easy to find code for this idea, so we'd need to apply it to a new dataset or a different problem. idk if just using a different set of text messages for training would be sufficient to get past the tas lol <end> <start> Winfrey: Yeah honestly cleaning our chat data is going to take up quite a chunk <end> <start> Winfrey: But yeah one thing for sure is that there‚Äôs a lot of resources available out there. Also could take a look at how we can make changes to model architecture <end> 