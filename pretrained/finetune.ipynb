{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Agm8kvVRU2Ue"
   },
   "outputs": [],
   "source": [
    "# Fine tuning gpt2_medium model and use own data like company profile\n",
    "#\n",
    "# See also medium.com blog\n",
    "# \"GPT-2 Fine-Tuning Guide: Building a Chatbot for Your Company Profile\"\n",
    "# https://medium.com/@datatec.studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHzJziLsVCcT"
   },
   "outputs": [],
   "source": [
    "# Install python packages\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "s5rI-cgkxRYj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "T6fuYpH4VE17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Define environment variable, path of data, model name and device\n",
    "# os.environ[\"HF_HOME\"] = \"/content/huggingface\"  # Replace with your desired directory\n",
    "# print(\"Please replace it with your hf access token:\")\n",
    "# os.environ[\"HF_HOME_TOKEN\"] = \"Please_replace_it_with_your_hf_access_token\"\n",
    "\n",
    "result_dir = 'resources/'\n",
    "data_file_name = 'Winfrey'\n",
    "data_file_path = f'../data/prompt_response/{data_file_name}.txt'\n",
    "\n",
    "model_name = \"gpt2\" # gpt2-medium\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_ZgHPQgRuKbb"
   },
   "outputs": [],
   "source": [
    "# Write a python file to google driver\n",
    "# Sample of json datasets\n",
    "# You can also directly upload this code to your google driver\n",
    "# The code write here in this way is for better understanding of whole project\n",
    "# %%writefile chat_data.py\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class ChatData(Dataset):\n",
    "    def __init__(self, path: str, tokenizer):\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            self.data = f.readlines()#json.load(open(path, \"r\"))\n",
    "\n",
    "        self.X = []\n",
    "        for pair in self.data:\n",
    "            pair = eval(pair)          \n",
    "            self.X.append(f\"<start> {pair['prompt']} <response>: {pair['response']} <end>\")\n",
    "        \n",
    "        total_samples = len(self.X)  # Calculate the total number of samples\n",
    "        print(\"total_samples\", total_samples)\n",
    "        # define samples amount\n",
    "#         self.X = self.X[:500]\n",
    "        print(\"Check the preprocessing for self.X[0]:\")\n",
    "        print(self.X[0])\n",
    "\n",
    "        self.X_encoded = tokenizer(self.X, return_tensors=\"pt\", max_length=30, padding=\"max_length\", truncation=True)\n",
    "        self.input_ids = self.X_encoded['input_ids']\n",
    "        self.attention_mask = self.X_encoded['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attention_mask[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_HLw88IBQHml"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in tokenizer before adding our specific tokens: 50257\n",
      "Number of tokens in tokenizer after adding our specific tokens: 52874\n"
     ]
    }
   ],
   "source": [
    "# Download model, save model and tokernize to harddisk\n",
    "## prepare tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f'Number of tokens in tokenizer before adding our specific tokens: {len(tokenizer.get_vocab())}')\n",
    "\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\",\n",
    "                              \"bos_token\": \"<start>\",\n",
    "                              \"eos_token\": \"<end>\"})\n",
    "\n",
    "tokenizer.add_tokens([\"<response>:\"])\n",
    "with open(f'../data/prompt_response/groupchat.txt', encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()#json.load(open(path, \"r\"))\n",
    "\n",
    "for pair in data:\n",
    "    pair = eval(pair)\n",
    "    tokenizer.add_tokens(pair['prompt'].split() + pair['response'].split())\n",
    "    \n",
    "print(f'Number of tokens in tokenizer after adding our specific tokens: {len(tokenizer.get_vocab())}')\n",
    "\n",
    "## prepare model\n",
    "### Specify the desired embedding size (must be a multiple of 8)\n",
    "desired_embedding_size = 50264  # Change this to the desired size\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "### Resize the embedding layer to the desired size\n",
    "model.resize_token_embeddings(len(tokenizer), desired_embedding_size)\n",
    "model = model.to(device)\n",
    "\n",
    "model.config.pad_token_id = tokenizer.eos_token_id + 1\n",
    "\n",
    "\n",
    "## save tokenizer and model to harddisk\n",
    "# tokenizer.save_pretrained(result_dir)\n",
    "# model.save_pretrained(result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "C9CzY7I9Qjh3"
   },
   "outputs": [],
   "source": [
    "# ## load model and tokenizer from harddisk\n",
    "# ### Load the GPT-2 tokenizer\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(result_dir)\n",
    "\n",
    "# ### Load the GPT-2 model from the local folder\n",
    "# model = GPT2LMHeadModel.from_pretrained(result_dir)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "tuKpppvpVlUA"
   },
   "outputs": [],
   "source": [
    "# Define infer and train function\n",
    "def infer(inp_raw):\n",
    "    inp_appended = \"<start> \" + inp_raw + \" <response>: \"\n",
    "    inp = tokenizer(inp_appended, return_tensors=\"pt\")\n",
    "    X = inp[\"input_ids\"].to(device)  # Use .to(device) method to move the tensor to the specified device\n",
    "    a = inp[\"attention_mask\"].to(device)  # Use .to(device) method here as well\n",
    "\n",
    "    output = model.generate(X, attention_mask=a, max_length=100, num_return_sequences=1)\n",
    "    output = tokenizer.decode(output[0]).replace('<pad>', '')\n",
    "    output = output[output.find('<response>: ')+len('<response>: '):].replace('<response>:', '')\n",
    "    if '<end>' in output:\n",
    "        output = output[:output.find('<end>')]\n",
    "    output = ' '.join(output.split())\n",
    "    \n",
    "    prompt_response = {\n",
    "        'prompt': inp_raw,\n",
    "        'response': output\n",
    "    }\n",
    "\n",
    "    return prompt_response\n",
    "\n",
    "\n",
    "def train(chatData, model, optim):\n",
    "    \n",
    "    batches = len(chatData)\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (X, a) in tqdm(enumerate(chatData), total=len(chatData), desc=\"Training\"):\n",
    "        X = X.to(device)\n",
    "        a = a.to(device)\n",
    "        optim.zero_grad()\n",
    "        loss = model(input_ids=X, attention_mask=a, labels=X).loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    print(infer(\"Any progress on the PA?\"))\n",
    "    print('========================================================')\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "xQrezTeDcsz1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_samples 570\n",
      "Check the preprocessing for self.X[0]:\n",
      "<start> You named the group CSE 151B PA2. <response>: What should our group name be ðŸ˜Ž Ester and I used transformers for PA1 <end>\n"
     ]
    }
   ],
   "source": [
    "# from chat_data import ChatData\n",
    "\n",
    "#Load ChatData, train model and optimizer\n",
    "chatData = ChatData(data_file_path, tokenizer)\n",
    "chatData = DataLoader(chatData, batch_size=1) # batch_size=64\n",
    "\n",
    "model.train()\n",
    "\n",
    "optim = Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7VWe95ug3Bum",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:22<00:00, 25.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Any progress on the PA?', 'response': \"Iâ€™ll keep testing it until test IoU is done, Then will be a composite for the txt files to be over the entire Once this is done, if we haven't we donâ€™t we can embrace the txt files will be large so\"}\n",
      "========================================================\n",
      "Epoch 0 finished in 0.38 minutes | total_loss = 188.19168958067894\n",
      "Epoch 1 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:22<00:00, 25.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Any progress on the PA?', 'response': 'Iâ€™ll be able to build on the top of ester1; (5c) at least first val Iâ€™ll will get more epochs, val acc, in epoch I can build on top of ester1; at 5 epochs Iâ€™ll will get results, the val'}\n",
      "========================================================\n",
      "Epoch 1 finished in 0.39 minutes | total_loss = 187.73083892464638\n",
      "Epoch 2 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 570/570 [00:21<00:00, 26.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Any progress on the PA?', 'response': 'Or any progress on the programming Or whatâ€™s the iou from the doc is summarize interesting'}\n",
      "========================================================\n",
      "Epoch 2 finished in 0.38 minutes | total_loss = 187.6108034849167\n",
      "Epoch 3 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 228/570 [00:08<00:13, 25.62it/s]"
     ]
    }
   ],
   "source": [
    "epochs = 100  # You can adjust the number of epochs as needed\n",
    "training_loss = []\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    print(f\"Epoch {epoch} started\")\n",
    "    total_loss = train(chatData, model, optim)\n",
    "    training_loss += [total_loss]\n",
    "    end = time.time()\n",
    "    print(f\"Epoch {epoch} finished in {np.round((end - start) / 60, 2)} minutes | total_loss = {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "570"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chatData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33749242090342335"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loss[-1] / len(chatData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "toCb6YovWMu5"
   },
   "outputs": [],
   "source": [
    "# inp = \"\"\n",
    "while True:\n",
    "    inp = input(\"Enter your input (press Enter when done): \" + \" \" * 20)\n",
    "    print(infer(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary after training is complete\n",
    "torch.save(model.state_dict(), f\"models/{data_file_name}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate a Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ester = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "ester.resize_token_embeddings(len(tokenizer), desired_embedding_size)\n",
    "ester.load_state_dict(torch.load(\"models/Ester.pt\"))\n",
    "ester.config.pad_token_id = tokenizer.eos_token_id + 1\n",
    "\n",
    "winfrey = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "winfrey.resize_token_embeddings(len(tokenizer), desired_embedding_size)\n",
    "winfrey.load_state_dict(torch.load(\"models/Winfrey.pt\"))\n",
    "winfrey.config.pad_token_id = tokenizer.eos_token_id + 1\n",
    "\n",
    "jeremy = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "jeremy.resize_token_embeddings(len(tokenizer), desired_embedding_size)\n",
    "jeremy.load_state_dict(torch.load(\"models/Jeremy.pt\"))\n",
    "jeremy.config.pad_token_id = tokenizer.eos_token_id + 1\n",
    "\n",
    "jonathan = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "jonathan.resize_token_embeddings(len(tokenizer), desired_embedding_size)\n",
    "jonathan.load_state_dict(torch.load(\"models/Jonathan.pt\"))\n",
    "jonathan.config.pad_token_id = tokenizer.eos_token_id + 1\n",
    "\n",
    "samuel = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "samuel.resize_token_embeddings(len(tokenizer), desired_embedding_size)\n",
    "samuel.load_state_dict(torch.load(\"models/Samuel.pt\"))\n",
    "samuel.config.pad_token_id = tokenizer.eos_token_id + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_infer(inp_raw, model):\n",
    "    inp_appended = \"<start> \" + inp_raw + \" <response>: \"\n",
    "    inp = tokenizer(inp_appended, return_tensors=\"pt\")\n",
    "    X = inp[\"input_ids\"].to(device)  # Use .to(device) method to move the tensor to the specified device\n",
    "    a = inp[\"attention_mask\"].to(device)  # Use .to(device) method here as well\n",
    "\n",
    "    output = model.generate(X, attention_mask=a, max_length=100, num_return_sequences=1)\n",
    "    output = tokenizer.decode(output[0])\n",
    "#     print(output)\n",
    "    output = output[output.find('<response>: ')+len('<response>: '):].replace('<pad>', '')#.replace('<response>:', '')#.replace('<pad>', '')#.replace('<end>', '')\n",
    "    if '<response>' in output:\n",
    "        output = output[:output.find('<response>')]\n",
    "    if '<end>' in output:\n",
    "        output = output[:output.find('<end>')]\n",
    "    output = ' '.join(output.split())\n",
    "    \n",
    "    prompt_response = {\n",
    "        'prompt': inp_raw,\n",
    "        'response': output\n",
    "    }\n",
    "\n",
    "    return prompt_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker = 'Ester'\n",
    "output = group_infer(\"could you present with me?\", eval(speaker.lower())) # How's it going?\n",
    "with_name = f'{speaker}: ' + output['prompt']\n",
    "convo = [with_name]\n",
    "print(with_name)\n",
    "\n",
    "people = ['Winfrey', 'Ester', 'Jeremy', 'Samuel', 'Jonathan']\n",
    "for i in range(15):\n",
    "    response = output['response']\n",
    "    remaining_people = [person for person in people if person != speaker]\n",
    "    speaker = np.random.choice(remaining_people)\n",
    "#     print(speaker)\n",
    "    output = group_infer(response, eval(speaker.lower())) # assuming models are named ester, winfrey, ...\n",
    "    with_name = f'{speaker}: ' + output['response']\n",
    "    convo += [with_name]\n",
    "    print(with_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
