{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Agm8kvVRU2Ue"
   },
   "outputs": [],
   "source": [
    "# Fine tuning gpt2_medium model and use own data like company profile\n",
    "#\n",
    "# See also medium.com blog\n",
    "# \"GPT-2 Fine-Tuning Guide: Building a Chatbot for Your Company Profile\"\n",
    "# https://medium.com/@datatec.studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHzJziLsVCcT"
   },
   "outputs": [],
   "source": [
    "# Install python packages\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "s5rI-cgkxRYj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "T6fuYpH4VE17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Define environment variable, path of data, model name and device\n",
    "# os.environ[\"HF_HOME\"] = \"/content/huggingface\"  # Replace with your desired directory\n",
    "# print(\"Please replace it with your hf access token:\")\n",
    "# os.environ[\"HF_HOME_TOKEN\"] = \"Please_replace_it_with_your_hf_access_token\"\n",
    "\n",
    "result_dir = 'resources/'\n",
    "data_file_name = 'Winfrey'\n",
    "data_file_path = f'../data/prompt_response/{data_file_name}.txt'\n",
    "\n",
    "model_name = \"gpt2\" # gpt2-medium\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_ZgHPQgRuKbb"
   },
   "outputs": [],
   "source": [
    "# Write a python file to google driver\n",
    "# Sample of json datasets\n",
    "# You can also directly upload this code to your google driver\n",
    "# The code write here in this way is for better understanding of whole project\n",
    "# %%writefile chat_data.py\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class ChatData(Dataset):\n",
    "    def __init__(self, path: str, tokenizer):\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            self.data = f.readlines()#json.load(open(path, \"r\"))\n",
    "\n",
    "        self.X = []\n",
    "        for pair in self.data:\n",
    "            pair = eval(pair)          \n",
    "            self.X.append(f\"<start> {pair['prompt']} <response>: {pair['response']} <end>\")\n",
    "        \n",
    "        total_samples = len(self.X)  # Calculate the total number of samples\n",
    "        print(\"total_samples\", total_samples)\n",
    "        # define samples amount\n",
    "#         self.X = self.X[:500]\n",
    "        print(\"Check the preprocessing for self.X[0]:\")\n",
    "        print(self.X[0])\n",
    "\n",
    "        self.X_encoded = tokenizer(self.X, return_tensors=\"pt\", max_length=30, padding=\"max_length\", truncation=True)\n",
    "        self.input_ids = self.X_encoded['input_ids']\n",
    "        self.attention_mask = self.X_encoded['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attention_mask[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_HLw88IBQHml"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in tokenizer before adding our specific tokens: 50257\n",
      "Number of tokens in tokenizer after adding our specific tokens: 51706\n"
     ]
    }
   ],
   "source": [
    "# Download model, save model and tokernize to harddisk\n",
    "## prepare tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f'Number of tokens in tokenizer before adding our specific tokens: {len(tokenizer.get_vocab())}')\n",
    "\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\",\n",
    "                              \"bos_token\": \"<start>\",\n",
    "                              \"eos_token\": \"<end>\"})\n",
    "\n",
    "tokenizer.add_tokens([\"<response>:\"])\n",
    "with open(data_file_path, encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()#json.load(open(path, \"r\"))\n",
    "\n",
    "for pair in data:\n",
    "    pair = eval(pair)\n",
    "    tokenizer.add_tokens(pair['prompt'].split() + pair['response'].split())\n",
    "    \n",
    "print(f'Number of tokens in tokenizer after adding our specific tokens: {len(tokenizer.get_vocab())}')\n",
    "\n",
    "## prepare model\n",
    "### Specify the desired embedding size (must be a multiple of 8)\n",
    "desired_embedding_size = 50264  # Change this to the desired size\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "### Resize the embedding layer to the desired size\n",
    "model.resize_token_embeddings(len(tokenizer), desired_embedding_size)\n",
    "model = model.to(device)\n",
    "\n",
    "## save tokenizer and model to harddisk\n",
    "# tokenizer.save_pretrained(result_dir)\n",
    "# model.save_pretrained(result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "C9CzY7I9Qjh3"
   },
   "outputs": [],
   "source": [
    "# ## load model and tokenizer from harddisk\n",
    "# ### Load the GPT-2 tokenizer\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(result_dir)\n",
    "\n",
    "# ### Load the GPT-2 model from the local folder\n",
    "# model = GPT2LMHeadModel.from_pretrained(result_dir)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "tuKpppvpVlUA"
   },
   "outputs": [],
   "source": [
    "# Define infer and train function\n",
    "def infer(inp_raw):\n",
    "    inp_appended = \"<start> \" + inp_raw + \" <response>: \"\n",
    "    inp = tokenizer(inp_appended, return_tensors=\"pt\")\n",
    "    X = inp[\"input_ids\"].to(device)  # Use .to(device) method to move the tensor to the specified device\n",
    "    a = inp[\"attention_mask\"].to(device)  # Use .to(device) method here as well\n",
    "\n",
    "    output = model.generate(X, attention_mask=a, max_length=100, num_return_sequences=1)\n",
    "\n",
    "    output = tokenizer.decode(output[0])\n",
    "    output = output[output.find('<response>: ')+len('<response>: '):output.find('<end>')].replace('<pad>', '')\n",
    "    output = ' '.join(output.split())\n",
    "    \n",
    "    prompt_response = {\n",
    "        'prompt': inp_raw,\n",
    "        'response': output\n",
    "    }\n",
    "\n",
    "    return prompt_response\n",
    "\n",
    "# def train(chatData, model, optim):\n",
    "    \n",
    "#     batches = len(chatData)\n",
    "\n",
    "#     for i, (X, a) in enumerate(chatData):\n",
    "#         X = X.to(device)\n",
    "#         a = a.to(device)\n",
    "#         optim.zero_grad()\n",
    "#         loss = model(input_ids=X, attention_mask=a, labels=X).loss\n",
    "#         loss.backward()\n",
    "#         optim.step()\n",
    "#         if i % 100 == 0:\n",
    "#             print(f'iter {i} out of {batches}')\n",
    "\n",
    "#     print(infer(\"I created my own branch in our repo\"))\n",
    "#     print('========================================================')\n",
    "\n",
    "def train(chatData, model, optim):\n",
    "    \n",
    "    batches = len(chatData)\n",
    "\n",
    "    for i, (X, a) in tqdm(enumerate(chatData), total=len(chatData), desc=\"Training\"):\n",
    "        X = X.to(device)\n",
    "        a = a.to(device)\n",
    "        optim.zero_grad()\n",
    "        loss = model(input_ids=X, attention_mask=a, labels=X).loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    print(infer(\"Do we need to bring anything?\"))\n",
    "    print('========================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xQrezTeDcsz1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_samples 515\n",
      "Check the preprocessing for self.X[0]:\n",
      "<start> PA2! <response>: Thanks Ester! <end>\n"
     ]
    }
   ],
   "source": [
    "# from chat_data import ChatData\n",
    "\n",
    "#Load ChatData, train model and optimizer\n",
    "chatData = ChatData(data_file_path, tokenizer)\n",
    "chatData = DataLoader(chatData, batch_size=1) # batch_size=64\n",
    "\n",
    "model.train()\n",
    "\n",
    "optim = Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "7VWe95ug3Bum",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:29<00:00, 17.27it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to t\n",
      "========================================================\n",
      "Epoch 0 finished in 0.51 minutes\n",
      "Epoch 1 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.83it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to to t\n",
      "========================================================\n",
      "Epoch 1 finished in 0.49 minutes\n",
      "Epoch 2 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.93it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>:\n",
      "========================================================\n",
      "Epoch 2 finished in 0.49 minutes\n",
      "Epoch 3 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.97it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: to the the the the the the the <pad\n",
      "========================================================\n",
      "Epoch 3 finished in 0.49 minutes\n",
      "Epoch 4 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.04it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "========================================================\n",
      "Epoch 4 finished in 0.49 minutes\n",
      "Epoch 5 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.06it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: the the the the the the the the the the the the the the the the the the <pad\n",
      "========================================================\n",
      "Epoch 5 finished in 0.49 minutes\n",
      "Epoch 6 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.05it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: the the <response>: the the <pad\n",
      "========================================================\n",
      "Epoch 6 finished in 0.49 minutes\n",
      "Epoch 7 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.02it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: <response>: <response>: <response>: <response>: <response>: the the the to the the the the <response>: the <response>: the the the the the the the the the the the the the the the the the the the the the the the\n",
      "========================================================\n",
      "Epoch 7 finished in 0.49 minutes\n",
      "Epoch 8 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.12it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: the <response>: <response>: the the <response>: the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n",
      "========================================================\n",
      "Epoch 8 finished in 0.49 minutes\n",
      "Epoch 9 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.95it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: <response>: <response>: and the <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: and the of the of the of the <response>: <response>: of the <response>: <response>: <response>: <response>: <response>: <response>: the <response>: <response>: to the of the o\n",
      "========================================================\n",
      "Epoch 9 finished in 0.49 minutes\n",
      "Epoch 10 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.00it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: to <response>: is the same of the <response>: <response>: <response>: is the <response>: <response>: <response>: is the same of the <response>: <response>: and the the is the <response>: <response>: is the the of the the of the the <response>: <response>: t\n",
      "========================================================\n",
      "Epoch 10 finished in 0.49 minutes\n",
      "Epoch 11 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.07it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: <response>: <response>: <response>: <response>: <response>: <response>: I can do the work for the <response>: I can do the work for the of the of the of the of the of the of the of the of the of the of th\n",
      "========================================================\n",
      "Epoch 11 finished in 0.49 minutes\n",
      "Epoch 12 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.11it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: <response>: <response>: <response>: <response>: <response>: I can do the the same thing <response>: for the same <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: and <response>\n",
      "========================================================\n",
      "Epoch 12 finished in 0.49 minutes\n",
      "Epoch 13 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.95it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: I think we need to <response>: and we can we do we do the <response>: <response>: and <response>: <response>: and the <response>: for the <response>: is <response>: and the model of the model of the model of the model of th\n",
      "========================================================\n",
      "Epoch 13 finished in 0.49 minutes\n",
      "Epoch 14 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.97it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>: <response>\n",
      "========================================================\n",
      "Epoch 14 finished in 0.49 minutes\n",
      "Epoch 15 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.01it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: <response>: <response>: I can do anything I can get back to my <response>: <response>: I can get the <response>: model model model model model model model model model model model model model model model model model model model model model mode\n",
      "========================================================\n",
      "Epoch 15 finished in 0.49 minutes\n",
      "Epoch 16 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.98it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: <response>: I can do anything to help me with the training <response>: I <response>: I can do anything to help me with the training of\n",
      "========================================================\n",
      "Epoch 16 finished in 0.49 minutes\n",
      "Epoch 17 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.05it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: I can run the best of the best of the\n",
      "========================================================\n",
      "Epoch 17 finished in 0.49 minutes\n",
      "Epoch 18 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.83it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: I think we are going to make a batch batch size of <response>: for each of the <response>: <response>: <response>: <response>: <response>: <response>:\n",
      "========================================================\n",
      "Epoch 18 finished in 0.49 minutes\n",
      "Epoch 19 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.10it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: I can help me get started with the basic <response>: <response>:\n",
      "========================================================\n",
      "Epoch 19 finished in 0.49 minutes\n",
      "Epoch 20 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.97it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: I can help me build a framework for the framework that we can\n",
      "========================================================\n",
      "Epoch 20 finished in 0.49 minutes\n",
      "Epoch 21 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.08it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>:\n",
      "========================================================\n",
      "Epoch 21 finished in 0.49 minutes\n",
      "Epoch 22 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.06it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: I can do for my part of the <response>: I can do a plots for my part of the\n",
      "========================================================\n",
      "Epoch 22 finished in 0.49 minutes\n",
      "Epoch 23 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.03it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: <response>: I am looking at the most common results <response>: <response>:\n",
      "========================================================\n",
      "Epoch 23 finished in 0.49 minutes\n",
      "Epoch 24 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.11it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: <response>:\n",
      "========================================================\n",
      "Epoch 24 finished in 0.49 minutes\n",
      "Epoch 25 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.88it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Or just get back to the original code and check <response>: <response>: I was running at the same time <response>: <response>: <response>: <response>:\n",
      "========================================================\n",
      "Epoch 25 finished in 0.49 minutes\n",
      "Epoch 26 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.90it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>:\n",
      "========================================================\n",
      "Epoch 26 finished in 0.49 minutes\n",
      "Epoch 27 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:29<00:00, 17.75it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: I think we need to set a table for them <response>: <response>:\n",
      "========================================================\n",
      "Epoch 27 finished in 0.5 minutes\n",
      "Epoch 28 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:29<00:00, 17.61it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>:\n",
      "========================================================\n",
      "Epoch 28 finished in 0.5 minutes\n",
      "Epoch 29 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.89it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Or do we need to add another row for the model or model in the row <response>: Or just just add another row for the model in the row\n",
      "========================================================\n",
      "Epoch 29 finished in 0.49 minutes\n",
      "Epoch 30 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.04it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: I will get back on <response>: I will get back on <response>: I will get back on what we had before when we went better with sequence length of length of of epoch than <pad\n",
      "========================================================\n",
      "Epoch 30 finished in 0.49 minutes\n",
      "Epoch 31 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.96it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: I have been using\n",
      "========================================================\n",
      "Epoch 31 finished in 0.49 minutes\n",
      "Epoch 32 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.06it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: I think\n",
      "========================================================\n",
      "Epoch 32 finished in 0.49 minutes\n",
      "Epoch 33 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.10it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: <response>: yes no <response>:\n",
      "========================================================\n",
      "Epoch 33 finished in 0.49 minutes\n",
      "Epoch 34 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.12it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: I am not sure about the plots <response>:\n",
      "========================================================\n",
      "Epoch 34 finished in 0.49 minutes\n",
      "Epoch 35 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.01it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: yes no not sure if classifier\n",
      "========================================================\n",
      "Epoch 35 finished in 0.49 minutes\n",
      "Epoch 36 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.06it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Just realized the poll.\n",
      "========================================================\n",
      "Epoch 36 finished in 0.49 minutes\n",
      "Epoch 37 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.03it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: No I can help run a chunk that I can run with your branch in the poll. <response>:\n",
      "========================================================\n",
      "Epoch 37 finished in 0.49 minutes\n",
      "Epoch 38 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.08it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: yes on my end on my end <response>:\n",
      "========================================================\n",
      "Epoch 38 finished in 0.49 minutes\n",
      "Epoch 39 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.04it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: <response>: I can help run a chunk with some results <response>: I can help run a chunk with some results I find in the poll. section under\n",
      "========================================================\n",
      "Epoch 39 finished in 0.49 minutes\n",
      "Epoch 40 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.10it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: I can help run a chunk on my computer by using a sequence length of 300 and even more research <response>: <response>: <response>: I can help run a batch size of 300 when I can get a val of supcon abov\n",
      "========================================================\n",
      "Epoch 40 finished in 0.49 minutes\n",
      "Epoch 41 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.10it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Or can we just confirm up to 4 epochs now <response>: <response>:\n",
      "========================================================\n",
      "Epoch 41 finished in 0.49 minutes\n",
      "Epoch 42 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.13it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Yes no I think it’s the same for classifier on the poll.\n",
      "========================================================\n",
      "Epoch 42 finished in 0.49 minutes\n",
      "Epoch 43 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.94it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: yes on end <response>: yes\n",
      "========================================================\n",
      "Epoch 43 finished in 0.49 minutes\n",
      "Epoch 44 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.92it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Yeah <response>: I will get back on datahub this\n",
      "========================================================\n",
      "Epoch 44 finished in 0.49 minutes\n",
      "Epoch 45 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.07it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: yes on my end\n",
      "========================================================\n",
      "Epoch 45 finished in 0.49 minutes\n",
      "Epoch 46 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.91it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Or can we just \\ref\n",
      "========================================================\n",
      "Epoch 46 finished in 0.49 minutes\n",
      "Epoch 47 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.96it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: No <response>: No I have never used part IoU the pa pa is slightly mostly taking 4c the architecture with both baseline and epoch of epoch at 10 am for the pa is mostly taking the pa with both both tha\n",
      "========================================================\n",
      "Epoch 47 finished in 0.49 minutes\n",
      "Epoch 48 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.96it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Or can we just realized them in the network <response>:\n",
      "========================================================\n",
      "Epoch 48 finished in 0.49 minutes\n",
      "Epoch 49 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.15it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: yes no I can help cross check the architecture table from <response>: Is there anything we need to do or <response>: other than epoch Just want to check the architecture table from that you can find any number of the poll\n",
      "========================================================\n",
      "Epoch 49 finished in 0.49 minutes\n",
      "Epoch 50 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.01it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Like one hour for 6 epochs\n",
      "========================================================\n",
      "Epoch 50 finished in 0.49 minutes\n",
      "Epoch 51 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:29<00:00, 17.66it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: yes to my end <response>: yes to my end\n",
      "========================================================\n",
      "Epoch 51 finished in 0.5 minutes\n",
      "Epoch 52 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.92it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: No I think <response>: Just curious made another push of epoch the one that I don’t only realized the one with very <pad\n",
      "========================================================\n",
      "Epoch 52 finished in 0.49 minutes\n",
      "Epoch 53 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.90it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Is it about the classifier <response>: Like one hour for 6 <response>:\n",
      "========================================================\n",
      "Epoch 53 finished in 0.49 minutes\n",
      "Epoch 54 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.05it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Or can we just batch size of 1 <response>: Or size of 50? instead of 50? <response>:\n",
      "========================================================\n",
      "Epoch 54 finished in 0.49 minutes\n",
      "Epoch 55 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.87it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Or just use the best model val reduction instead of 50? instead of val train acc 10 <response>: Or 0.1? instead of 10\n",
      "========================================================\n",
      "Epoch 55 finished in 0.49 minutes\n",
      "Epoch 56 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.94it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Or can we just set a new week for epochs <response>: Just cause cause I’m cause for PA1 cause\n",
      "========================================================\n",
      "Epoch 56 finished in 0.49 minutes\n",
      "Epoch 57 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.86it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: yes on my end\n",
      "========================================================\n",
      "Epoch 57 finished in 0.49 minutes\n",
      "Epoch 58 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.94it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Or maybe how to build a UNet on top of ester1; <response>: I’ll with other epochs <response>: = none other than 0.01 this table = 50? = none\n",
      "========================================================\n",
      "Epoch 58 finished in 0.49 minutes\n",
      "Epoch 59 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.88it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: yes on end\n",
      "========================================================\n",
      "Epoch 59 finished in 0.49 minutes\n",
      "Epoch 60 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.97it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Or can we just finetune a table with sequence length of length of 50? Like SEQ_SIZE <response>: Or sequence length of 50? Or sequence length of 50?\n",
      "========================================================\n",
      "Epoch 60 finished in 0.49 minutes\n",
      "Epoch 61 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.89it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Or can we just just put them together <response>: So <response>:\n",
      "========================================================\n",
      "Epoch 61 finished in 0.49 minutes\n",
      "Epoch 62 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.76it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Yes for each of the models in 5, the sequence length is wrong <response>:\n",
      "========================================================\n",
      "Epoch 62 finished in 0.5 minutes\n",
      "Epoch 63 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.97it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Thank you\n",
      "========================================================\n",
      "Epoch 63 finished in 0.49 minutes\n",
      "Epoch 64 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.05it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Okkk is 85.7% tiny bit\n",
      "========================================================\n",
      "Epoch 64 finished in 0.49 minutes\n",
      "Epoch 65 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.00it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: I’ll a high chance that’s <response>:\n",
      "========================================================\n",
      "Epoch 65 finished in 0.49 minutes\n",
      "Epoch 66 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.01it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: @Ester Are we going with our “best\n",
      "========================================================\n",
      "Epoch 66 finished in 0.49 minutes\n",
      "Epoch 67 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.99it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: @Ester I can help run a chunk at 520pm, <response>: I’ll for the human viewing I can run at the server with some of it like 5 epochs <response>: with sequence length of 30 like supcon with sequence length of 50\n",
      "========================================================\n",
      "Epoch 67 finished in 0.49 minutes\n",
      "Epoch 68 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.10it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Just curious the ABC notation 😳 <response>: Niceeeee\n",
      "========================================================\n",
      "Epoch 68 finished in 0.49 minutes\n",
      "Epoch 69 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.16it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Thank you\n",
      "========================================================\n",
      "Epoch 69 finished in 0.49 minutes\n",
      "Epoch 70 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.13it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: yes on my end\n",
      "========================================================\n",
      "Epoch 70 finished in 0.49 minutes\n",
      "Epoch 71 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.18it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: No I can make a post on piazza: <response>: Just curious none of “improving the supcon can be destroyed\n",
      "========================================================\n",
      "Epoch 71 finished in 0.49 minutes\n",
      "Epoch 72 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.07it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: @Ester\n",
      "========================================================\n",
      "Epoch 72 finished in 0.49 minutes\n",
      "Epoch 73 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.11it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: @Ester\n",
      "========================================================\n",
      "Epoch 73 finished in 0.49 minutes\n",
      "Epoch 74 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.93it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Thank you hehe\n",
      "========================================================\n",
      "Epoch 74 finished in 0.49 minutes\n",
      "Epoch 75 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.04it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Sure\n",
      "========================================================\n",
      "Epoch 75 finished in 0.49 minutes\n",
      "Epoch 76 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.94it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Or name it according to the architecture <response>: Quick question,\n",
      "========================================================\n",
      "Epoch 76 finished in 0.49 minutes\n",
      "Epoch 77 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.06it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: @Ester <response>: I like how we do with tiny - tiny tiny\n",
      "========================================================\n",
      "Epoch 77 finished in 0.49 minutes\n",
      "Epoch 78 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.84it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Thank you hehe <response>: Thank you\n",
      "========================================================\n",
      "Epoch 78 finished in 0.49 minutes\n",
      "Epoch 79 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:29<00:00, 17.44it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: I’ll wait a bit <response>: Okk\n",
      "========================================================\n",
      "Epoch 79 finished in 0.51 minutes\n",
      "Epoch 80 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.82it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Yes\n",
      "========================================================\n",
      "Epoch 80 finished in 0.5 minutes\n",
      "Epoch 81 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.76it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Thank you\n",
      "========================================================\n",
      "Epoch 81 finished in 0.5 minutes\n",
      "Epoch 82 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.87it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Anything that has to do with PATH is very painful <response>: I was using a very different architecture and it was different <response>: in my session\n",
      "========================================================\n",
      "Epoch 82 finished in 0.49 minutes\n",
      "Epoch 83 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.88it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: It’s is quite high honestly\n",
      "========================================================\n",
      "Epoch 83 finished in 0.49 minutes\n",
      "Epoch 84 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:29<00:00, 17.71it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Like one hour\n",
      "========================================================\n",
      "Epoch 84 finished in 0.5 minutes\n",
      "Epoch 85 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.97it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Thank you!\n",
      "========================================================\n",
      "Epoch 85 finished in 0.49 minutes\n",
      "Epoch 86 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.14it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Thank you <response>:\n",
      "========================================================\n",
      "Epoch 86 finished in 0.49 minutes\n",
      "Epoch 87 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.12it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Anything that is not set in the Code is not set in the Code <response>: I’ll Code is not set on the epoch\n",
      "========================================================\n",
      "Epoch 87 finished in 0.49 minutes\n",
      "Epoch 88 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.14it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Thank you\n",
      "========================================================\n",
      "Epoch 88 finished in 0.49 minutes\n",
      "Epoch 89 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.93it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Main is the same as Ester 1 right now <response>:\n",
      "========================================================\n",
      "Epoch 89 finished in 0.49 minutes\n",
      "Epoch 90 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.03it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: A Simple framework for learning Contrastive Learning of Contrastive <response>: literally HAHAH\n",
      "========================================================\n",
      "Epoch 90 finished in 0.49 minutes\n",
      "Epoch 91 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.10it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Like one hour\n",
      "========================================================\n",
      "Epoch 91 finished in 0.49 minutes\n",
      "Epoch 92 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.05it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Thank you!\n",
      "========================================================\n",
      "Epoch 92 finished in 0.49 minutes\n",
      "Epoch 93 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.97it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Anything that is not set back to original length of 50? <response>: Or even even lower\n",
      "========================================================\n",
      "Epoch 93 finished in 0.49 minutes\n",
      "Epoch 94 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.04it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Anything that is very painful to main or ester1? is slightly Just very very slightly\n",
      "========================================================\n",
      "Epoch 94 finished in 0.49 minutes\n",
      "Epoch 95 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 18.09it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Anything that is needed for the report <response>: Or just for the report <response>: Or Simple framework for the iou Or iou =\n",
      "========================================================\n",
      "Epoch 95 finished in 0.49 minutes\n",
      "Epoch 96 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.90it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Or add the paper in <response>: Or just use the number\n",
      "========================================================\n",
      "Epoch 96 finished in 0.49 minutes\n",
      "Epoch 97 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.91it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Thank you\n",
      "========================================================\n",
      "Epoch 97 finished in 0.49 minutes\n",
      "Epoch 98 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.87it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: A Simple framework for learning Contrastive <response>: Simple framework for learning Learning\n",
      "========================================================\n",
      "Epoch 98 finished in 0.49 minutes\n",
      "Epoch 99 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 515/515 [00:28<00:00, 17.96it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Do we need to b r in g anything? <response>: Just submitted the scheduler and loop <response>: Just cause for PA1\n",
      "========================================================\n",
      "Epoch 99 finished in 0.49 minutes\n"
     ]
    }
   ],
   "source": [
    "# train 10 times\n",
    "epochs = 100  # You can adjust the number of epochs as needed\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    print(f\"Epoch {epoch} started\")\n",
    "    train(chatData, model, optim)\n",
    "    end = time.time()\n",
    "    print(f\"Epoch {epoch} finished in {np.round((end - start) / 60, 2)} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "toCb6YovWMu5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your input (press Enter when done):                     haha\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'haha', 'response': 'ok I see I seeeee'}\n",
      "Enter your input (press Enter when done):                     yay!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'yay!', 'response': 'also looking at augmentation options in torchvision (part of UNet requires this) <response>: Personal high key'}\n",
      "Enter your input (press Enter when done):                     I'm done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': \"I'm done\", 'response': \"I'm Feel free to build on top of ester1; by line <response>:\"}\n",
      "Enter your input (press Enter when done):                     yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'yes', 'response': 'the loss ones?'}\n",
      "Enter your input (press Enter when done):                     I have a lot of coding experience\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'I have a lot of coding experience', 'response': 'under figures, the visual framework for discussion <response>: Visual Representations'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5860/3056494004.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your input (press Enter when done): \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    979\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             )\n\u001b[0;32m--> 981\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m    982\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "inp = \"\"\n",
    "while True:\n",
    "    inp = input(\"Enter your input (press Enter when done): \" + \" \" * 20)\n",
    "    print(infer(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary after training is complete\n",
    "torch.save(model.state_dict(), f\"models/{data_file_name}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate a Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ester = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "ester.resize_token_embeddings(len(tokenizer), desired_embedding_size)\n",
    "ester.load_state_dict(torch.load(\"models/Ester.pt\"))\n",
    "ester = model.to(device)\n",
    "\n",
    "winfrey = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "winfrey.resize_token_embeddings(len(tokenizer), desired_embedding_size)\n",
    "winfrey.load_state_dict(torch.load(\"models/Winfrey.pt\"))\n",
    "winfrey = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(inp_raw, model):\n",
    "    inp_appended = \"<start> \" + inp_raw + \" <response>: \"\n",
    "    inp = tokenizer(inp_appended, return_tensors=\"pt\")\n",
    "    X = inp[\"input_ids\"].to(device)  # Use .to(device) method to move the tensor to the specified device\n",
    "    a = inp[\"attention_mask\"].to(device)  # Use .to(device) method here as well\n",
    "\n",
    "    output = model.generate(X, attention_mask=a, max_length=100, num_return_sequences=1)\n",
    "\n",
    "    output = tokenizer.decode(output[0])\n",
    "    output = output[output.find('<response>: ')+len('<response>: '):output.find('<end>')].replace('<pad>', '')\n",
    "    output = ' '.join(output.split())\n",
    "    \n",
    "    prompt_response = {\n",
    "        'prompt': inp_raw,\n",
    "        'response': output\n",
    "    }\n",
    "\n",
    "    return prompt_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Ester: How's it going?\",\n",
       " 'Winfrey: Oh no I don’t why not the 4c I think remotely! <response>: I think some of it in the computer folder not very good <response>: ok I think',\n",
       " 'Ester: I think some of it in the computer folder not very good <response>: ok I think <response>: I I’ll might have some errors I have that’s I think',\n",
       " 'Winfrey: ok I think <response>: I I’ll might have some errors I have that’s I think <response>: Maybe in the computer folder that I read through but not the util iou Not sure if I don’t don’t',\n",
       " 'Ester: I I’ll might have some errors I have that’s I think <response>: Maybe in the computer folder that I read through but not the util iou Not sure if I don’t don’t <response>: Also I’ll probably want to set the context of sequence length 50?',\n",
       " 'Winfrey: Maybe in the computer folder that I read through but not the util iou Not sure if I don’t don’t <response>: Also I’ll probably want to set the context of sequence length 50? <response>: Maybe in the',\n",
       " 'Ester: Also I’ll probably want to set the context of sequence length 50? <response>: Maybe in the <response>: Learning of Visual Representations',\n",
       " 'Winfrey: Maybe in the <response>: Learning of Visual Representations <response>: I’ll also might want to set up the context of sequence length of 50? any of 100',\n",
       " 'Ester: Learning of Visual Representations <response>: I’ll also might want to set up the context of sequence length of 50? any of 100 <response>: Maybe this is the thing that you want to finalize',\n",
       " 'Winfrey: I’ll also might want to set up the context of sequence length of 50? any of 100 <response>: Maybe this is the thing that you want to finalize <response>: Or maybe',\n",
       " 'Ester: Maybe this is the thing that you want to finalize <response>: Or maybe <response>: Part of the epoch Kind of like Part of Self']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = infer(\"How's it going?\", ester)\n",
    "with_name = 'Ester: ' + output['prompt']\n",
    "convo = [with_name]\n",
    "for i in range(10):\n",
    "    response = output['response']\n",
    "    if i % 2 == 0:\n",
    "        output = infer(response, winfrey)\n",
    "        with_name = 'Winfrey: ' + output['response']\n",
    "    else:\n",
    "        output = infer(response, ester)\n",
    "        with_name = 'Ester: ' + output['response']\n",
    "    convo += [with_name]\n",
    "    \n",
    "convo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Ester: How's it going?\",\n",
       " 'Winfrey: Oh no I don’t why not the 4c I think remotely! <response>: I think some of it in the computer folder not very good <response>: ok I think',\n",
       " 'Ester: I think some of it in the computer folder not very good <response>: ok I think <response>: I I’ll might have some errors I have that’s I think',\n",
       " 'Winfrey: ok I think <response>: I I’ll might have some errors I have that’s I think <response>: Maybe in the computer folder that I read through but not the util iou Not sure if I don’t don’t',\n",
       " 'Ester: I I’ll might have some errors I have that’s I think <response>: Maybe in the computer folder that I read through but not the util iou Not sure if I don’t don’t <response>: Also I’ll probably want to set the context of sequence length 50?',\n",
       " 'Winfrey: Maybe in the computer folder that I read through but not the util iou Not sure if I don’t don’t <response>: Also I’ll probably want to set the context of sequence length 50? <response>: Maybe in the',\n",
       " 'Ester: Also I’ll probably want to set the context of sequence length 50? <response>: Maybe in the <response>: Learning of Visual Representations',\n",
       " 'Winfrey: Maybe in the <response>: Learning of Visual Representations <response>: I’ll also might want to set up the context of sequence length of 50? any of 100',\n",
       " 'Ester: Learning of Visual Representations <response>: I’ll also might want to set up the context of sequence length of 50? any of 100 <response>: Maybe this is the thing that you want to finalize',\n",
       " 'Winfrey: I’ll also might want to set up the context of sequence length of 50? any of 100 <response>: Maybe this is the thing that you want to finalize <response>: Or maybe',\n",
       " 'Ester: Maybe this is the thing that you want to finalize <response>: Or maybe <response>: Part of the epoch Kind of like Part of Self']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
