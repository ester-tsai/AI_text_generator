{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Agm8kvVRU2Ue"
   },
   "outputs": [],
   "source": [
    "# Fine tuning gpt2_medium model and use own data like company profile\n",
    "#\n",
    "# See also medium.com blog\n",
    "# \"GPT-2 Fine-Tuning Guide: Building a Chatbot for Your Company Profile\"\n",
    "# https://medium.com/@datatec.studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHzJziLsVCcT"
   },
   "outputs": [],
   "source": [
    "# Install python packages\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "s5rI-cgkxRYj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "T6fuYpH4VE17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Define environment variable, path of data, model name and device\n",
    "# os.environ[\"HF_HOME\"] = \"/content/huggingface\"  # Replace with your desired directory\n",
    "# print(\"Please replace it with your hf access token:\")\n",
    "# os.environ[\"HF_HOME_TOKEN\"] = \"Please_replace_it_with_your_hf_access_token\"\n",
    "\n",
    "result_dir = 'resources/'\n",
    "data_file_name = 'Jeremy'\n",
    "data_file_path = f'../data/prompt_response/{data_file_name}.txt'\n",
    "\n",
    "model_name = \"gpt2\" # gpt2-medium\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_ZgHPQgRuKbb"
   },
   "outputs": [],
   "source": [
    "# Write a python file to google driver\n",
    "# Sample of json datasets\n",
    "# You can also directly upload this code to your google driver\n",
    "# The code write here in this way is for better understanding of whole project\n",
    "# %%writefile chat_data.py\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class ChatData(Dataset):\n",
    "    def __init__(self, path: str, tokenizer):\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            self.data = f.readlines()#json.load(open(path, \"r\"))\n",
    "\n",
    "        self.X = []\n",
    "        for pair in self.data:\n",
    "            pair = eval(pair)          \n",
    "            self.X.append(f\"<start> {pair['prompt']} <response>: {pair['response']} <end>\")\n",
    "        \n",
    "        total_samples = len(self.X)  # Calculate the total number of samples\n",
    "        print(\"total_samples\", total_samples)\n",
    "        # define samples amount\n",
    "#         self.X = self.X[:500]\n",
    "        print(\"Check the preprocessing for self.X[0]:\")\n",
    "        print(self.X[0])\n",
    "\n",
    "        self.X_encoded = tokenizer(self.X, return_tensors=\"pt\", max_length=30, padding=\"max_length\", truncation=True)\n",
    "        self.input_ids = self.X_encoded['input_ids']\n",
    "        self.attention_mask = self.X_encoded['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attention_mask[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_HLw88IBQHml"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in tokenizer before adding our specific tokens: 50257\n",
      "Number of tokens in tokenizer after adding our specific tokens: 50816\n"
     ]
    }
   ],
   "source": [
    "# Download model, save model and tokernize to harddisk\n",
    "## prepare tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f'Number of tokens in tokenizer before adding our specific tokens: {len(tokenizer.get_vocab())}')\n",
    "\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\",\n",
    "                              \"bos_token\": \"<start>\",\n",
    "                              \"eos_token\": \"<end>\"})\n",
    "\n",
    "tokenizer.add_tokens([\"<response>:\"])\n",
    "with open(data_file_path, encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()#json.load(open(path, \"r\"))\n",
    "\n",
    "for pair in data:\n",
    "    pair = eval(pair)\n",
    "    tokenizer.add_tokens(pair['prompt'].split() + pair['response'].split())\n",
    "    \n",
    "print(f'Number of tokens in tokenizer after adding our specific tokens: {len(tokenizer.get_vocab())}')\n",
    "\n",
    "## prepare model\n",
    "### Specify the desired embedding size (must be a multiple of 8)\n",
    "desired_embedding_size = 50264  # Change this to the desired size\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "### Resize the embedding layer to the desired size\n",
    "model.resize_token_embeddings(len(tokenizer), desired_embedding_size)\n",
    "model = model.to(device)\n",
    "\n",
    "model.config.pad_token_id = tokenizer.eos_token_id + 1\n",
    "\n",
    "\n",
    "## save tokenizer and model to harddisk\n",
    "# tokenizer.save_pretrained(result_dir)\n",
    "# model.save_pretrained(result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "C9CzY7I9Qjh3"
   },
   "outputs": [],
   "source": [
    "# ## load model and tokenizer from harddisk\n",
    "# ### Load the GPT-2 tokenizer\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(result_dir)\n",
    "\n",
    "# ### Load the GPT-2 model from the local folder\n",
    "# model = GPT2LMHeadModel.from_pretrained(result_dir)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "tuKpppvpVlUA"
   },
   "outputs": [],
   "source": [
    "# Define infer and train function\n",
    "def infer(inp_raw):\n",
    "    inp_appended = \"<start> \" + inp_raw + \" <response>: \"\n",
    "    inp = tokenizer(inp_appended, return_tensors=\"pt\")\n",
    "    X = inp[\"input_ids\"].to(device)  # Use .to(device) method to move the tensor to the specified device\n",
    "    a = inp[\"attention_mask\"].to(device)  # Use .to(device) method here as well\n",
    "\n",
    "    output = model.generate(X, attention_mask=a, max_length=100, num_return_sequences=1)\n",
    "\n",
    "    output = tokenizer.decode(output[0])\n",
    "    output = output[output.find('<response>: ')+len('<response>: '):output.find('<end>')].replace('<pad>', '')\n",
    "    output = ' '.join(output.split())\n",
    "    \n",
    "    prompt_response = {\n",
    "        'prompt': inp_raw,\n",
    "        'response': output\n",
    "    }\n",
    "\n",
    "    return prompt_response\n",
    "\n",
    "\n",
    "def train(chatData, model, optim):\n",
    "    \n",
    "    batches = len(chatData)\n",
    "\n",
    "    for i, (X, a) in tqdm(enumerate(chatData), total=len(chatData), desc=\"Training\"):\n",
    "        X = X.to(device)\n",
    "        a = a.to(device)\n",
    "        optim.zero_grad()\n",
    "        loss = model(input_ids=X, attention_mask=a, labels=X).loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    print(infer(\"Do we need to bring anything?\"))\n",
    "    print('========================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xQrezTeDcsz1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_samples 164\n",
      "Check the preprocessing for self.X[0]:\n",
      "<start> What should our group name be ðŸ˜Ž Ester and I used transformers for PA1 <response>: lol anything works <end>\n"
     ]
    }
   ],
   "source": [
    "# from chat_data import ChatData\n",
    "\n",
    "#Load ChatData, train model and optimizer\n",
    "chatData = ChatData(data_file_path, tokenizer)\n",
    "chatData = DataLoader(chatData, batch_size=1) # batch_size=64\n",
    "\n",
    "model.train()\n",
    "\n",
    "optim = Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7VWe95ug3Bum",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:06<00:00, 23.57it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 1: expected str instance, NoneType found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_136/142576848.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch} started\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchatData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch} finished in {np.round((end - start) / 60, 2)} minutes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_136/446420215.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(chatData, model, optim)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Do we need to bring anything?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'========================================================'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_136/446420215.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(inp_raw)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<response>: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<response>: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<end>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3744\u001b[0m         \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_py_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3746\u001b[0;31m         return self._decode(\n\u001b[0m\u001b[1;32m   3747\u001b[0m             \u001b[0mtoken_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3748\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0mcurrent_sub_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurrent_sub_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0msub_texts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_tokens_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_sub_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspaces_between_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/gpt2/tokenization_gpt2.py\u001b[0m in \u001b[0;36mconvert_tokens_to_string\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert_tokens_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;34m\"\"\"Converts a sequence of tokens (string) in a single string.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbyte_decoder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 1: expected str instance, NoneType found"
     ]
    }
   ],
   "source": [
    "# train 10 times\n",
    "epochs = 50  # You can adjust the number of epochs as needed\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    print(f\"Epoch {epoch} started\")\n",
    "    train(chatData, model, optim)\n",
    "    end = time.time()\n",
    "    print(f\"Epoch {epoch} finished in {np.round((end - start) / 60, 2)} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "toCb6YovWMu5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your input (press Enter when done):                     haha\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'haha', 'response': 'ok I see I seeeee'}\n",
      "Enter your input (press Enter when done):                     yay!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'yay!', 'response': 'also looking at augmentation options in torchvision (part of UNet requires this) <response>: Personal high key'}\n",
      "Enter your input (press Enter when done):                     I'm done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': \"I'm done\", 'response': \"I'm Feel free to build on top of ester1; by line <response>:\"}\n",
      "Enter your input (press Enter when done):                     yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'yes', 'response': 'the loss ones?'}\n",
      "Enter your input (press Enter when done):                     I have a lot of coding experience\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'I have a lot of coding experience', 'response': 'under figures, the visual framework for discussion <response>: Visual Representations'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5860/3056494004.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your input (press Enter when done): \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    979\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             )\n\u001b[0;32m--> 981\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m    982\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "inp = \"\"\n",
    "while True:\n",
    "    inp = input(\"Enter your input (press Enter when done): \" + \" \" * 20)\n",
    "    print(infer(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary after training is complete\n",
    "torch.save(model.state_dict(), f\"models/{data_file_name}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate a Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ester = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "ester.resize_token_embeddings(len(tokenizer), desired_embedding_size)\n",
    "ester.load_state_dict(torch.load(\"models/Ester.pt\"))\n",
    "ester = model.to(device)\n",
    "\n",
    "winfrey = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "winfrey.resize_token_embeddings(len(tokenizer), desired_embedding_size)\n",
    "winfrey.load_state_dict(torch.load(\"models/Winfrey.pt\"))\n",
    "winfrey = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(inp_raw, model):\n",
    "    inp_appended = \"<start> \" + inp_raw + \" <response>: \"\n",
    "    inp = tokenizer(inp_appended, return_tensors=\"pt\")\n",
    "    X = inp[\"input_ids\"].to(device)  # Use .to(device) method to move the tensor to the specified device\n",
    "    a = inp[\"attention_mask\"].to(device)  # Use .to(device) method here as well\n",
    "\n",
    "    output = model.generate(X, attention_mask=a, max_length=100, num_return_sequences=1)\n",
    "\n",
    "    output = tokenizer.decode(output[0])\n",
    "    output = output[output.find('<response>: ')+len('<response>: '):output.find('<end>')].replace('<pad>', '')\n",
    "    output = ' '.join(output.split())\n",
    "    \n",
    "    prompt_response = {\n",
    "        'prompt': inp_raw,\n",
    "        'response': output\n",
    "    }\n",
    "\n",
    "    return prompt_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.eos_token_id + 1\n",
    "output = infer(\"How's it going?\", ester)\n",
    "with_name = 'Ester: ' + output['prompt']\n",
    "convo = [with_name]\n",
    "print(with_name)\n",
    "for i in range(17):\n",
    "    response = output['response']\n",
    "    if i % 2 == 0:\n",
    "        output = infer(response, winfrey)\n",
    "        with_name = 'Winfrey: ' + output['response']\n",
    "    else:\n",
    "        output = infer(response, ester)\n",
    "        with_name = 'Ester: ' + output['response']\n",
    "    convo += [with_name]\n",
    "    print(with_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Ester: How's it going?\",\n",
       " 'Winfrey: Oh no I donâ€™t why not the 4c I think remotely! <response>: I think some of it in the computer folder not very good <response>: ok I think',\n",
       " 'Ester: I think some of it in the computer folder not very good <response>: ok I think <response>: I Iâ€™ll might have some errors I have thatâ€™s I think',\n",
       " 'Winfrey: ok I think <response>: I Iâ€™ll might have some errors I have thatâ€™s I think <response>: Maybe in the computer folder that I read through but not the util iou Not sure if I donâ€™t donâ€™t',\n",
       " 'Ester: I Iâ€™ll might have some errors I have thatâ€™s I think <response>: Maybe in the computer folder that I read through but not the util iou Not sure if I donâ€™t donâ€™t <response>: Also Iâ€™ll probably want to set the context of sequence length 50?',\n",
       " 'Winfrey: Maybe in the computer folder that I read through but not the util iou Not sure if I donâ€™t donâ€™t <response>: Also Iâ€™ll probably want to set the context of sequence length 50? <response>: Maybe in the',\n",
       " 'Ester: Also Iâ€™ll probably want to set the context of sequence length 50? <response>: Maybe in the <response>: Learning of Visual Representations',\n",
       " 'Winfrey: Maybe in the <response>: Learning of Visual Representations <response>: Iâ€™ll also might want to set up the context of sequence length of 50? any of 100',\n",
       " 'Ester: Learning of Visual Representations <response>: Iâ€™ll also might want to set up the context of sequence length of 50? any of 100 <response>: Maybe this is the thing that you want to finalize',\n",
       " 'Winfrey: Iâ€™ll also might want to set up the context of sequence length of 50? any of 100 <response>: Maybe this is the thing that you want to finalize <response>: Or maybe',\n",
       " 'Ester: Maybe this is the thing that you want to finalize <response>: Or maybe <response>: Part of the epoch Kind of like Part of Self']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
