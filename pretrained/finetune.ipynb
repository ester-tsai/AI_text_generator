{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Agm8kvVRU2Ue"
   },
   "outputs": [],
   "source": [
    "# Fine tuning gpt2_medium model and use own data like company profile\n",
    "#\n",
    "# See also medium.com blog\n",
    "# \"GPT-2 Fine-Tuning Guide: Building a Chatbot for Your Company Profile\"\n",
    "# https://medium.com/@datatec.studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHzJziLsVCcT"
   },
   "outputs": [],
   "source": [
    "# Install python packages\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "s5rI-cgkxRYj"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "T6fuYpH4VE17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Define environment variable, path of data, model name and device\n",
    "# os.environ[\"HF_HOME\"] = \"/content/huggingface\"  # Replace with your desired directory\n",
    "# print(\"Please replace it with your hf access token:\")\n",
    "# os.environ[\"HF_HOME_TOKEN\"] = \"Please_replace_it_with_your_hf_access_token\"\n",
    "\n",
    "result_dir = 'resources/'\n",
    "data_file_name = 'Jeremy'\n",
    "data_file_path = f'../data/prompt_response/{data_file_name}.txt'\n",
    "\n",
    "model_name = \"gpt2\" # gpt2-medium\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_ZgHPQgRuKbb"
   },
   "outputs": [],
   "source": [
    "# Write a python file to google driver\n",
    "# Sample of json datasets\n",
    "# You can also directly upload this code to your google driver\n",
    "# The code write here in this way is for better understanding of whole project\n",
    "# %%writefile chat_data.py\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import json\n",
    "\n",
    "class ChatData(Dataset):\n",
    "    def __init__(self, path: str, tokenizer):\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            self.data = f.readlines()#json.load(open(path, \"r\"))\n",
    "\n",
    "        self.X = []\n",
    "        for pair in self.data:\n",
    "            pair = eval(pair)          \n",
    "            self.X.append(f\"<start> {pair['prompt']} <response>: {pair['response']} <end>\")\n",
    "        \n",
    "        total_samples = len(self.X)  # Calculate the total number of samples\n",
    "        print(\"total_samples\", total_samples)\n",
    "        # define samples amount\n",
    "#         self.X = self.X[:500]\n",
    "        print(\"Check the preprocessing for self.X[0]:\")\n",
    "        print(self.X[0])\n",
    "\n",
    "        self.X_encoded = tokenizer(self.X, return_tensors=\"pt\", max_length=30, padding=\"max_length\", truncation=True)\n",
    "        self.input_ids = self.X_encoded['input_ids']\n",
    "        self.attention_mask = self.X_encoded['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.attention_mask[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_HLw88IBQHml"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in tokenizer before adding our specific tokens: 50257\n",
      "Number of tokens in tokenizer after adding our specific tokens: 50816\n"
     ]
    }
   ],
   "source": [
    "# Download model, save model and tokernize to harddisk\n",
    "## prepare tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(f'Number of tokens in tokenizer before adding our specific tokens: {len(tokenizer.get_vocab())}')\n",
    "\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\",\n",
    "                              \"bos_token\": \"<start>\",\n",
    "                              \"eos_token\": \"<end>\"})\n",
    "\n",
    "tokenizer.add_tokens([\"<response>:\"])\n",
    "with open(data_file_path, encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()#json.load(open(path, \"r\"))\n",
    "\n",
    "for pair in data:\n",
    "    pair = eval(pair)\n",
    "    tokenizer.add_tokens(pair['prompt'].split() + pair['response'].split())\n",
    "    \n",
    "print(f'Number of tokens in tokenizer after adding our specific tokens: {len(tokenizer.get_vocab())}')\n",
    "\n",
    "## prepare model\n",
    "### Specify the desired embedding size (must be a multiple of 8)\n",
    "desired_embedding_size = 50264  # Change this to the desired size\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "### Resize the embedding layer to the desired size\n",
    "model.resize_token_embeddings(len(tokenizer), desired_embedding_size)\n",
    "model = model.to(device)\n",
    "\n",
    "model.config.pad_token_id = tokenizer.eos_token_id + 1\n",
    "\n",
    "\n",
    "## save tokenizer and model to harddisk\n",
    "# tokenizer.save_pretrained(result_dir)\n",
    "# model.save_pretrained(result_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "C9CzY7I9Qjh3"
   },
   "outputs": [],
   "source": [
    "# ## load model and tokenizer from harddisk\n",
    "# ### Load the GPT-2 tokenizer\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(result_dir)\n",
    "\n",
    "# ### Load the GPT-2 model from the local folder\n",
    "# model = GPT2LMHeadModel.from_pretrained(result_dir)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "tuKpppvpVlUA"
   },
   "outputs": [],
   "source": [
    "# Define infer and train function\n",
    "def infer(inp_raw):\n",
    "    inp_appended = \"<start> \" + inp_raw + \" <response>: \"\n",
    "    inp = tokenizer(inp_appended, return_tensors=\"pt\")\n",
    "    X = inp[\"input_ids\"].to(device)  # Use .to(device) method to move the tensor to the specified device\n",
    "    a = inp[\"attention_mask\"].to(device)  # Use .to(device) method here as well\n",
    "\n",
    "    output = model.generate(X, attention_mask=a, max_length=100, num_return_sequences=1)\n",
    "    output = tokenizer.decode(output[0]).replace('<pad>', '')\n",
    "    output = output[output.find('<response>: ')+len('<response>: '):].replace('<response>:', '')\n",
    "    if '<end>' in output:\n",
    "        output = output[:output.find('<end>')]\n",
    "    output = ' '.join(output.split())\n",
    "    \n",
    "    prompt_response = {\n",
    "        'prompt': inp_raw,\n",
    "        'response': output\n",
    "    }\n",
    "\n",
    "    return prompt_response\n",
    "\n",
    "\n",
    "def train(chatData, model, optim):\n",
    "    \n",
    "    batches = len(chatData)\n",
    "\n",
    "    for i, (X, a) in tqdm(enumerate(chatData), total=len(chatData), desc=\"Training\"):\n",
    "        X = X.to(device)\n",
    "        a = a.to(device)\n",
    "        optim.zero_grad()\n",
    "        loss = model(input_ids=X, attention_mask=a, labels=X).loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    print(infer(\"Do we need to bring anything?\"))\n",
    "    print('========================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xQrezTeDcsz1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_samples 164\n",
      "Check the preprocessing for self.X[0]:\n",
      "<start> What should our group name be ðŸ˜Ž Ester and I used transformers for PA1 <response>: lol anything works <end>\n"
     ]
    }
   ],
   "source": [
    "# from chat_data import ChatData\n",
    "\n",
    "#Load ChatData, train model and optimizer\n",
    "chatData = ChatData(data_file_path, tokenizer)\n",
    "chatData = DataLoader(chatData, batch_size=1) # batch_size=64\n",
    "\n",
    "model.train()\n",
    "\n",
    "optim = Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "7VWe95ug3Bum",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:07<00:00, 22.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Do we need to bring anything?', 'response': \"our seed is actually just winfrey's and def look like\"}\n",
      "========================================================\n",
      "Epoch 0 finished in 0.14 minutes\n",
      "Epoch 1 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:06<00:00, 24.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Do we need to bring anything?', 'response': 'ok I edited the numbers just pushed'}\n",
      "========================================================\n",
      "Epoch 1 finished in 0.12 minutes\n",
      "Epoch 2 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:06<00:00, 25.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Do we need to bring anything?', 'response': 'our seed was actually just built different i tried only modified the branch i tried the code'}\n",
      "========================================================\n",
      "Epoch 2 finished in 0.12 minutes\n",
      "Epoch 3 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:06<00:00, 24.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Do we need to bring anything?', 'response': 'our seed is still vv scuffed but higher so so i tried modified the basic_fcn and seed so so'}\n",
      "========================================================\n",
      "Epoch 3 finished in 0.12 minutes\n",
      "Epoch 4 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:06<00:00, 24.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Do we need to bring anything?', 'response': 'our seed was actually not really working on the hyperparameters but updated the branch and my new part was really working'}\n",
      "========================================================\n",
      "Epoch 4 finished in 0.12 minutes\n",
      "Epoch 5 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:06<00:00, 25.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Do we need to bring anything?', 'response': 'do we need to submit to 200 in the report ok I remember the one thing we need to know is we still working on it ig in the report ok'}\n",
      "========================================================\n",
      "Epoch 5 finished in 0.12 minutes\n",
      "Epoch 6 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:06<00:00, 24.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Do we need to bring anything?', 'response': 'do we need a lot of And the report to the branch oh ok'}\n",
      "========================================================\n",
      "Epoch 6 finished in 0.12 minutes\n",
      "Epoch 7 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:06<00:00, 24.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Do we need to bring anything?', 'response': '<start> Do we need to br in g anything? <response>: ok I could be wrong <response>: but I remember the session I went to a branch and I remember the session I went wasnâ€™t very different than the results added to the report <response>: added my branch name to my branch and'}\n",
      "========================================================\n",
      "Epoch 7 finished in 0.12 minutes\n",
      "Epoch 8 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:06<00:00, 25.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Do we need to bring anything?', 'response': 'do we need a lot more than the union? give me give me'}\n",
      "========================================================\n",
      "Epoch 8 finished in 0.12 minutes\n",
      "Epoch 9 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:06<00:00, 25.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Do we need to bring anything?', 'response': 'do we need to make a branch over the table and show the report back ok'}\n",
      "========================================================\n",
      "Epoch 9 finished in 0.12 minutes\n",
      "Epoch 10 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:06<00:00, 25.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Do we need to bring anything?', 'response': 'mb try a different server? i tried the one better i tried the one better i tried fixed the thing that numbers fixed the'}\n",
      "========================================================\n",
      "Epoch 10 finished in 0.12 minutes\n",
      "Epoch 11 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:06<00:00, 25.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Do we need to bring anything?', 'response': 'do we need to submit to the report ok'}\n",
      "========================================================\n",
      "Epoch 11 finished in 0.12 minutes\n",
      "Epoch 12 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:06<00:00, 25.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Do we need to bring anything?', 'response': 'ok ok my parts ok my parts'}\n",
      "========================================================\n",
      "Epoch 12 finished in 0.12 minutes\n",
      "Epoch 13 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:06<00:00, 25.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Do we need to bring anything?', 'response': '<start> Do we need to br in g anything? <response>: anything but yea <response>: work on it later <response>: work on it later < model name + the hyperparameters <response>: option <response>: option option option option option option option option option option option option option option option option option option option'}\n",
      "========================================================\n",
      "Epoch 13 finished in 0.12 minutes\n",
      "Epoch 14 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:06<00:00, 25.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Do we need to bring anything?', 'response': 'our names and our names and our'}\n",
      "========================================================\n",
      "Epoch 14 finished in 0.12 minutes\n",
      "Epoch 15 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:06<00:00, 25.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Do we need to bring anything?', 'response': 'our seed is well gone into this'}\n",
      "========================================================\n",
      "Epoch 15 finished in 0.12 minutes\n",
      "Epoch 16 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:06<00:00, 25.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Do we need to bring anything?', 'response': 'our models is checkpoint/best_0219 really different than the cuda is really different just different a a different model from the one name that is mb 5b i tried umm'}\n",
      "========================================================\n",
      "Epoch 16 finished in 0.12 minutes\n",
      "Epoch 17 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:06<00:00, 26.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Do we need to bring anything?', 'response': 'I could never aws i tried doing anything but branch 16 i tried know what were the hyperparameters hyperparameters so expensive doing it as fast as a separate one percent of the union? chance'}\n",
      "========================================================\n",
      "Epoch 17 finished in 0.12 minutes\n",
      "Epoch 18 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:06<00:00, 26.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Do we need to bring anything?', 'response': '<start> Do we need to br in g anything? <response>: just push the fasic_fcn file to my branch and you can numbers for PA1 <response>: <response>: and out the morning ðŸ¤“ <response>: <response>: i just missed it looks and using sequence length of 50 and the training time of 50 and the'}\n",
      "========================================================\n",
      "Epoch 18 finished in 0.12 minutes\n",
      "Epoch 19 started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 164/164 [00:06<00:00, 25.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Do we need to bring anything?', 'response': 'our model is plainly just a better for so we can make a better model of our'}\n",
      "========================================================\n",
      "Epoch 19 finished in 0.12 minutes\n"
     ]
    }
   ],
   "source": [
    "# train 10 times\n",
    "epochs = 20  # You can adjust the number of epochs as needed\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    print(f\"Epoch {epoch} started\")\n",
    "    train(chatData, model, optim)\n",
    "    end = time.time()\n",
    "    print(f\"Epoch {epoch} finished in {np.round((end - start) / 60, 2)} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in tokenizer.get_vocab().items():\n",
    "#     if v == 1169:\n",
    "#         print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "toCb6YovWMu5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your input (press Enter when done):                     Hi! Are you done?\n",
      "{'prompt': 'Hi! Are you done?', 'response': '+ the iou ? ok I just pushed to my branch'}\n",
      "Enter your input (press Enter when done):                     I just pushed\n",
      "{'prompt': 'I just pushed', 'response': 'its under jeremy-unet, the code is still vv but vv scuffed and vv ok my part is supposed'}\n",
      "Enter your input (press Enter when done):                     Hey Jeremy\n",
      "{'prompt': 'Hey Jeremy', 'response': 'I just ran it looks and I do not have this branch so feel like I need to check the code a good numbers i think I tried doing the results with my branch a good on the'}\n",
      "Enter your input (press Enter when done):                     branch\n",
      "{'prompt': 'branch', 'response': 'could you check the code submission? I am not sure if the results are + different i tried different of the same image + the original?'}\n",
      "Enter your input (press Enter when done):                     Ester\n",
      "{'prompt': 'Ester', 'response': 'i just pushed its under jeremy-unet, the code ok will work cuz'}\n",
      "Enter your input (press Enter when done):                     Ester\n",
      "{'prompt': 'Ester', 'response': 'oh i think im done with my part too i tried i thought urs would work cuz'}\n",
      "Enter your input (press Enter when done):                     Sam\n",
      "{'prompt': 'Sam', 'response': 'ok I push the best model to branch ok'}\n",
      "Enter your input (press Enter when done):                     Sam\n",
      "{'prompt': 'Sam', 'response': 'so ok i just pushed its under jeremy-unet, the code ok i just tried its under jeremy-unet, the sequence length is to be significant'}\n",
      "Enter your input (press Enter when done):                     jonathan\n",
      "{'prompt': 'jonathan', 'response': 'the best i got was all 0.88 so ok I edited the numbers in the pictures'}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3726/3056494004.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your input (press Enter when done): \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    979\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m             )\n\u001b[0;32m--> 981\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m    982\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "inp = \"\"\n",
    "while True:\n",
    "    inp = input(\"Enter your input (press Enter when done): \" + \" \" * 20)\n",
    "    print(infer(inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model's state dictionary after training is complete\n",
    "torch.save(model.state_dict(), f\"models/{data_file_name}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate a Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ester = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "ester.resize_token_embeddings(len(tokenizer), desired_embedding_size)\n",
    "ester.load_state_dict(torch.load(\"models/Ester.pt\"))\n",
    "ester = model.to(device)\n",
    "\n",
    "winfrey = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "winfrey.resize_token_embeddings(len(tokenizer), desired_embedding_size)\n",
    "winfrey.load_state_dict(torch.load(\"models/Winfrey.pt\"))\n",
    "winfrey = model.to(device)\n",
    "\n",
    "jeremy = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "jeremy.resize_token_embeddings(len(tokenizer), desired_embedding_size)\n",
    "jeremy.load_state_dict(torch.load(\"models/Jeremy.pt\"))\n",
    "jeremy = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_infer(inp_raw, model):\n",
    "    inp_appended = \"<start> \" + inp_raw + \" <response>: \"\n",
    "    inp = tokenizer(inp_appended, return_tensors=\"pt\")\n",
    "    X = inp[\"input_ids\"].to(device)  # Use .to(device) method to move the tensor to the specified device\n",
    "    a = inp[\"attention_mask\"].to(device)  # Use .to(device) method here as well\n",
    "\n",
    "    output = model.generate(X, attention_mask=a, max_length=100, num_return_sequences=1)\n",
    "    output = tokenizer.decode(output[0])\n",
    "#     print(output)\n",
    "    output = output[output.find('<response>: ')+len('<response>: '):].replace('<response>:', '').replace('<pad>', '').replace('<end>', '')\n",
    "#     if '<end>' in output:\n",
    "#         output = output[:output.find('<end>')]\n",
    "    output = ' '.join(output.split())\n",
    "    \n",
    "    prompt_response = {\n",
    "        'prompt': inp_raw,\n",
    "        'response': output\n",
    "    }\n",
    "\n",
    "    return prompt_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jeremy: Any progress on the PA?\n",
      "Winfrey: but the is open to in a single\n",
      "Jeremy: but the datahub is plots ok i just pushed it ok i put these numbers into the forward section of the branch ok\n",
      "Jeremy: ok i just pushed into the forward section of the section\n",
      "Winfrey: i tried into the forward section of the section i just into the forward section of the section of the section i just into the forward\n",
      "Jeremy: section using the branch id of the section using the same names for of the the\n",
      "Jeremy: i just tried\n",
      "Jeremy: the best i got was all 0.88 so yea ok I edited the numbers file to make a branch file for 5b i tried the cuda file\n",
      "Winfrey: i tried a different of the image I used\n",
      "Winfrey: i will do some numbers training for 5b as a iou i have my own gpu and i want a different image for my\n",
      "Winfrey: like i like like i dont like the intersection id\n",
      "Jeremy: want to branch id want to be a branch model of single image of 16 images? like i tried like i like the like\n",
      "Winfrey: like i tried i like the pixel of the branch like the\n",
      "Ester: as a image is i a cheaper i i a a iou i a 16 images? of a a 4x4 of\n",
      "Jeremy: \n",
      "Ester: i think im done some as a iou option i would say no option to the iou as a based on the datahub\n",
      "Ester: option but ok i think I did scuffed the results with mine original code and using our names\n",
      "Winfrey: so far as it is x iou so slightly\n",
      "Jeremy: or ig as it is x than the datahub but slightly as it is more creative than the one image is\n",
      "Winfrey: as it is more creative (or\n",
      "Winfrey: based on articles that would be a separate based on articles that i\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Jeremy: Any progress on the PA?',\n",
       " 'Winfrey: but the is open to in a single',\n",
       " 'Jeremy: but the datahub is plots ok i just pushed it ok i put these numbers into the forward section of the branch ok',\n",
       " 'Jeremy: ok i just pushed into the forward section of the section',\n",
       " 'Winfrey: i tried into the forward section of the section i just into the forward section of the section of the section i just into the forward',\n",
       " 'Jeremy: section using the branch id of the section using the same names for of the the',\n",
       " 'Jeremy: i just tried',\n",
       " 'Jeremy: the best i got was all 0.88 so yea ok I edited the numbers file to make a branch file for 5b i tried the cuda file',\n",
       " 'Winfrey: i tried a different of the image I used',\n",
       " 'Winfrey: i will do some numbers training for 5b as a iou i have my own gpu and i want a different image for my',\n",
       " 'Winfrey: like i like like i dont like the intersection id',\n",
       " 'Jeremy: want to branch id want to be a branch model of single image of 16 images? like i tried like i like the like',\n",
       " 'Winfrey: like i tried i like the pixel of the branch like the',\n",
       " 'Ester: as a image is i a cheaper i i a a iou i a 16 images? of a a 4x4 of',\n",
       " 'Jeremy: ',\n",
       " 'Ester: i think im done some as a iou option i would say no option to the iou as a based on the datahub',\n",
       " 'Ester: option but ok i think I did scuffed the results with mine original code and using our names',\n",
       " 'Winfrey: so far as it is x iou so slightly',\n",
       " 'Jeremy: or ig as it is x than the datahub but slightly as it is more creative than the one image is',\n",
       " 'Winfrey: as it is more creative (or',\n",
       " 'Winfrey: based on articles that would be a separate based on articles that i']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.pad_token_id = tokenizer.eos_token_id + 1\n",
    "output = group_infer(\"Any progress on the PA?\", jeremy) # How's it going?\n",
    "with_name = 'Jeremy: ' + output['prompt']\n",
    "convo = [with_name]\n",
    "print(with_name)\n",
    "\n",
    "people = ['Winfrey', 'Ester', 'Jeremy']\n",
    "for i in range(20):\n",
    "    response = output['response']\n",
    "    speaker = np.random.choice(people)\n",
    "    output = group_infer(response, eval(speaker.lower())) # assuming models are named ester, winfrey, ...\n",
    "    with_name = f'{speaker}: ' + output['response']\n",
    "    convo += [with_name]\n",
    "    print(with_name)\n",
    "\n",
    "convo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
