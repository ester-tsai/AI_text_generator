{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614c008a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed731665",
   "metadata": {},
   "source": [
    "# Groupchat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4091463c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Can one of you guys download your facebook messenger data and upload the json file for this groupchat to the github repo? I can't download the 3 month one for some reason\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupchat_data = open('data/CSE151B_groupchat_3mo.json')\n",
    "groupchat = json.load(groupchat_data)\n",
    "groupchat['messages'][1]['content'].encode('latin1').decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37f884f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "junk_messages = ['Reacted', 'reacted', 'X:1759 T:F\\\"ur Elise T:Bagatelle', 'Counter({50:']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "752b7d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cleaned.txt', 'w+') as f:\n",
    "    for d in groupchat['messages'][::-1]:\n",
    "        if 'content' in d:\n",
    "            message = d['content']\n",
    "            if np.all([junk not in message for junk in junk_messages]):\n",
    "                cleaned = message.replace('\\n', ' ').encode('latin1').decode('utf-8')\n",
    "                cleaned = '<start> ' + cleaned + ' <end> '\n",
    "                f.write(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d7bc026",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cleaned_with_name.txt', 'w+') as f:\n",
    "    for d in groupchat['messages'][::-1]:\n",
    "        if 'content' in d:\n",
    "            message = d['content']\n",
    "            if np.all([junk not in message for junk in junk_messages]):\n",
    "                sender = d[\"sender_name\"].split(' ')[0].replace('Kong', 'Winfrey')\n",
    "                cleaned = message.replace('\\n', ' ').encode('latin1').decode('utf-8')\n",
    "                cleaned = f'<start> {sender}: ' + cleaned + ' <end> '\n",
    "                f.write(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17ad14b",
   "metadata": {},
   "source": [
    "# Individuals (prompt response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1015163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "junk_messages = ['Reacted', 'reacted', 'X:1759 T:F\\\"ur Elise T:Bagatelle', 'Counter({50:', 'python main.py --task', 'gmail.com',\n",
    "                 '\"because python\"', 'Jeremy:', 'Jonathan:', 'Samuel:', 'Winfrey:', 'Ester:']\n",
    "\n",
    "def clean_message(message):\n",
    "    message = message.replace('\\n', ' ').replace('\\U0001fae8', 'ü´®').replace('\\U0001f979', 'ü•π').encode('latin1').decode('utf-8')\\\n",
    "                     .replace('\\u200d‚ôÄÔ∏è', '').replace('\\u200d‚ôÇÔ∏è', '').replace('\\u200d', '').replace('\\U0001fae3', '')\\\n",
    "                     .replace('\\U0001fae8', '')\n",
    "    if 'https:' in message:\n",
    "        message = message[:message.find('https:')]\n",
    "    return message\n",
    "\n",
    "def create_individual_data(source, name, mode='w'):\n",
    "    with open(f'data/prompt_response/{name}.txt', mode) as f:\n",
    "        message_list = source['messages'][::-1]\n",
    "        for i in range(len(message_list)-1):\n",
    "            d1, d2 = message_list[i], message_list[i+1]\n",
    "            sender1, sender2 = d1[\"sender_name\"].split(' ')[0], d2[\"sender_name\"].split(' ')[0]\n",
    "            if ('content' in d1) & ('content' in d2) & (name == sender2):\n",
    "                message1, message2 = d1['content'], d2['content']\n",
    "                if np.all([junk not in message1 for junk in junk_messages]) & np.all([junk not in message2 for junk in junk_messages]):\n",
    "                    cleaned1 = clean_message(message1)\n",
    "                    cleaned2 = clean_message(message2)\n",
    "                    if (len(cleaned1.strip()) == 0) | (len(cleaned2.strip()) == 0):\n",
    "                        continue\n",
    "                    pair = {\n",
    "                        'prompt': cleaned1,\n",
    "                        'response': cleaned2\n",
    "                    }\n",
    "                    f.write(str(pair) + '\\n')\n",
    "                    \n",
    "    with open(f'data/prompt_response/{name}.txt', 'r') as f:\n",
    "        return f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48669316",
   "metadata": {},
   "source": [
    "# Ester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ac9c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tim_data = open('data/timothy_wu.json')\n",
    "# tim = json.load(tim_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28603dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"{'prompt': 'Thanks Ester!', 'response': 'Ester named the group CSE 151B PA2.'}\\n\",\n",
       " \"{'prompt': 'we could be incredibly original and use our names', 'response': 'Or name it according to the PA content? Like Convolution? \\\\U0001fae0'}\\n\",\n",
       " \"{'prompt': 'Added!', 'response': 'I created my own branch in our repo '}\\n\",\n",
       " \"{'prompt': 'I created my own branch in our repo ', 'response': 'I pushed the updated util.py to master, but I need a minor fix after asking a question on piazza'}\\n\",\n",
       " '{\\'prompt\\': \\'I pushed the updated util.py to master, but I need a minor fix after asking a question on piazza\\', \\'response\\': \"Are you guys able to access a GPU of any sort? Datahub is still down and I can\\'t get Google Colab to work either\"}\\n',\n",
       " \"{'prompt': 'Thinking if we can use our data science capstone‚Äôs ü§î', 'response': 'I guess so!'}\\n\",\n",
       " \"{'prompt': 'Yeahh', 'response': 'Actually it seems like the issue happened because I‚Äôm still occupying a GPU for capstone'}\\n\",\n",
       " \"{'prompt': 'Actually it seems like the issue happened because I‚Äôm still occupying a GPU for capstone', 'response': 'So I can only work on one project at a time'}\\n\",\n",
       " '{\\'prompt\\': \\'Does it say something like CUDA blah blah exceeded or killed when u run two?\\', \\'response\\': \\'It says \"Request exceeds limit of 1 GPUs\" hahaa\\'}\\n',\n",
       " \"{'prompt': 'In case it gets killed and u gotta restart the kernel üòÖ', 'response': 'I can just clear some space in my private folder and git clone the project there'}\\n\",\n",
       " \"{'prompt': '@Ester Tsai are you at part 2 for the programming part? The data loading section', 'response': 'Yes, I have the code for the baseline model but I‚Äôll test it when I finish running something for capstone'}\\n\",\n",
       " \"{'prompt': 'Yes, I have the code for the baseline model but I‚Äôll test it when I finish running something for capstone', 'response': '(I don‚Äôt know if my baseline model works, will likely need to debug!'}\\n\",\n",
       " \"{'prompt': 'Okkk! Sounds good, so have u implemented the test and val data in voc.py?', 'response': 'Feel free to test it by cloning my branch tho!'}\\n\",\n",
       " \"{'prompt': 'Feel free to test it by cloning my branch tho!', 'response': 'Yes'}\\n\",\n",
       " \"{'prompt': 'Ok could you push it to your branch? I don‚Äôt think I see it there just now', 'response': 'Also the first half of visualize.option'}\\n\",\n",
       " \"{'prompt': 'Also the first half of visualize.option', 'response': 'Visualize.ipynb *'}\\n\",\n",
       " \"{'prompt': 'Yup thanks!', 'response': 'Almost done with the baseline model! Gonna keep testing it until test IoU is consistently around 0.05'}\\n\",\n",
       " \"{'prompt': 'I see we are using SGD optimizer currently', 'response': 'ok I can try!'}\\n\",\n",
       " '{\\'prompt\\': \\'Coding is done up to part 3, so feel free to make a branch of the current master branch and start working on part 4\\', \\'response\\': \\'You can test the model by running \"python train.py\" in the terminal\\'}\\n',\n",
       " \"{'prompt': 'Yayyy thank you thank youuu!!', 'response': 'voc.py is where we should implement data augmentation (part 4b), after defining the augmentation'}\\n\",\n",
       " \"{'prompt': 'Just curious if there‚Äôs other places that offer image augmentation methods', 'response': 'TA gave this link as an example: '}\\n\",\n",
       " \"{'prompt': 'So I just downloaded the zip file LOL', 'response': 'Maybe make a new branch haha'}\\n\",\n",
       " \"{'prompt': 'Maybe make a new branch haha', 'response': 'GitHub high key confusing sometimes'}\\n\",\n",
       " \"{'prompt': 'I haven‚Äôt tried running the model yet so', 'response': 'It shouldn‚Äôt be large tho'}\\n\",\n",
       " \"{'prompt': 'Oh yeah did prof say anything about moving the MT date? Since it‚Äôs going to be on lunar new year', 'response': 'I can start working on pa2 again on Monday! And then work on the report Tues and Wednesday. Down to work on both Q4 and 5 if time permits, but if time is tight, I‚Äôll piggy back off of existing Q4 and just work on Q5!'}\\n\",\n",
       " \"{'prompt': 'I can start working on pa2 again on Monday! And then work on the report Tues and Wednesday. Down to work on both Q4 and 5 if time permits, but if time is tight, I‚Äôll piggy back off of existing Q4 and just work on Q5!', 'response': 'There was a piazza post that said we‚Äôre not moving the midterm I think'}\\n\",\n",
       " \"{'prompt': '@Ester Tsai quick question, for the optimizer, did you choose learning rate to be 0.001 or was it given?', 'response': 'I chose it'}\\n\",\n",
       " \"{'prompt': 'I am pretty much done with 4a, just re-running them to check the numbers', 'response': 'Make sense!'}\\n\",\n",
       " \"{'prompt': '4b is done! If everyone is okay, I can push to master', 'response': 'Thanks!!'}\\n\",\n",
       " \"{'prompt': 'I will get back on pa2 tomorrow, is there anything that I can work on?', 'response': 'I can get on that too after capstone tomorrow! What is left that we need to work on?'}\\n\",\n",
       " '{\\'prompt\\': \\'Pretty much done with 4c, just running a bunch of experiments, will let y‚Äôall know before I push\\', \\'response\\': \"I\\'ll start on the report\"}\\n',\n",
       " \"{'prompt': 'Great! I‚Äôll join u on the report tomorrow!', 'response': 'Here is the link to the report: '}\\n\",\n",
       " \"{'prompt': 'Here is the link to the report: ', 'response': 'And the checklist of requriements: '}\\n\",\n",
       " '{\\'prompt\\': \\'And the checklist of requriements: \\', \\'response\\': \"There are a lot to write, so I\\'ll make a poll for how to divide the work if you guys are ok with that!\"}\\n',\n",
       " '{\\'prompt\\': \"There are a lot to write, so I\\'ll make a poll for how to divide the work if you guys are ok with that!\", \\'response\\': \\'Ester created a poll: PA2 Report Responsibility (par...\\'}\\n',\n",
       " \"{'prompt': 'Ester created a poll: PA2 Report Responsibility (par...', 'response': 'Ester created a poll: PA2 Report (part 2).'}\\n\",\n",
       " '{\\'prompt\\': \\'Ester created a poll: PA2 Report (part 2).\\', \\'response\\': \\'Ester voted for \"Methods - Baseline\" and 1 other option in the poll.\\'}\\n',\n",
       " '{\\'prompt\\': \\'Ester voted for \"Methods - Baseline\" and 1 other option in the poll.\\', \\'response\\': \\'Ester voted for \"Discussion - Baseline\" and 1 other option in the poll.\\'}\\n',\n",
       " \"{'prompt': 'oo true it would be good to specify the 3 sections in Methods Experimentation and Discussion Experimentation', 'response': 'Has anyone done 5b yet? If not, I can look into it'}\\n\",\n",
       " '{\\'prompt\\': \\'i‚Äôm going to look into 5b in a bit\\', \\'response\\': \\'Ester voted for \"Abstract\" in the poll.\\'}\\n',\n",
       " \"{'prompt': 'I will push 4c before 12am! Have been fine tuning the hyper parameters, finally got a satisfactory result üòÆüí®üò¥', 'response': 'Yay!'}\\n\",\n",
       " '{\\'prompt\\': \\'Yay!\\', \\'response\\': \\'Ester voted for \"Methods - Improving on baseline\" in the poll.\\'}\\n',\n",
       " '{\\'prompt\\': \\'Ester voted for \"Methods - Improving on baseline\" in the poll.\\', \\'response\\': \\'Ester voted for \"Discussion - Improving on basline\" in the poll.\\'}\\n',\n",
       " '{\\'prompt\\': \\'Ester voted for \"Discussion - Improving on basline\" in the poll.\\', \\'response\\': \\'I filled in the part for Cosine Annealing for both methods and discussion\\'}\\n',\n",
       " \"{'prompt': 'something wrong with the cirterion function', 'response': 'Are you using your own code or Winfrey‚Äôs base code?'}\\n\",\n",
       " \"{'prompt': 'Try changing padding', 'response': '@Kong Xian Ying @Jeremy Tow @Jonathan Cheung Are you all free to call tomorrow at 9am to check in for PA2?'}\\n\",\n",
       " \"{'prompt': 'ye i shuld be free', 'response': 'I‚Äôll implement the training and validation plot for report Results section'}\\n\",\n",
       " \"{'prompt': 'This is great!!!', 'response': 'DANG Winfrey you got up at 5am ü•µ'}\\n\",\n",
       " '{\\'prompt\\': \\'DANG Winfrey you got up at 5am ü•µ\\', \\'response\\': \"I\\'ll create a Zoom link\"}\\n',\n",
       " '{\\'prompt\\': \\'@Jonathan Cheung @Jeremy Tow Zoom?\\', \\'response\\': \\'def set_seed(seed: int = 42) -> None:     np.random.seed(seed)     random.seed(seed)     torch.manual_seed(seed)     torch.cuda.manual_seed(seed)     # When running on the CuDNN backend, two further options must be set     torch.backends.cudnn.deterministic = True     torch.backends.cudnn.benchmark = False     # Set a fixed value for the hash seed     os.environ[\"PYTHONHASHSEED\"] = str(seed)\\'}\\n',\n",
       " '{\\'prompt\\': \\'Ester voted for \"Results (filling in the numbers)\" in the poll.\\', \\'response\\': \\'Ester voted for \"Discussion - Experimentation\" in the poll.\\'}\\n',\n",
       " '{\\'prompt\\': \\'Yesss agreed!\\', \\'response\\': \"@Jeremy Tow could you push your train_5_b code sometime today when you\\'re done?\"}\\n',\n",
       " \"{'prompt': 'i only modified the basic_fcn file', 'response': 'okok could I have your modified basic_fcn file haha'}\\n\",\n",
       " \"{'prompt': 'idk what the easiest way to do this is', 'response': 'sure I can try!'}\\n\",\n",
       " '{\\'prompt\\': \\'ok I just pushed to my branch\\', \\'response\\': \"I\\'ve set up references.bib for  bibtex\"}\\n',\n",
       " '{\\'prompt\\': \"I\\'ve set up references.bib for  bibtex\", \\'response\\': \\'you can cite a paper using \\\\\\\\citet{reference_name} and it will automatically appear in the References section\\'}\\n',\n",
       " \"{'prompt': 'Thank you!', 'response': 'I‚Äôm trying 5a but data hub is not working for me right now'}\\n\",\n",
       " \"{'prompt': 'if u need to i can give u access to my server', 'response': 'it works again!'}\\n\",\n",
       " \"{'prompt': 'Do we want to set a cutoff time for the models? For tomorrow.', 'response': 'Jonathan and I both tried 5A but couldn‚Äôt get it better than 6.6%'}\\n\",\n",
       " \"{'prompt': 'Jonathan and I both tried 5A but couldn‚Äôt get it better than 6.6%', 'response': 'I‚Äôll go to office hours tomorrow'}\\n\",\n",
       " \"{'prompt': 'i think it makes more sense in decimal', 'response': 'Sure!'}\\n\",\n",
       " \"{'prompt': 'Also one thing I didn‚Äôt really try is that, UNet paper mentioned that they use batch size of 1, but I‚Äôm not sure how helpful this will be', 'response': 'How‚Äôs the new test IoU after these improvements?'}\\n\",\n",
       " \"{'prompt': 'this is so mysterious', 'response': 'Hmm set seed is actually not working fully. The results are different after rerunning'}\\n\",\n",
       " \"{'prompt': 'like source of randomness lol', 'response': 'I was pretty sure it worked at one point \\\\U0001fae0\\\\U0001fae0\\\\U0001fae0'}\\n\",\n",
       " '{\\'prompt\\': \\'I was pretty sure it worked at one point \\\\U0001fae0\\\\U0001fae0\\\\U0001fae0\\', \\'response\\': \"I\\'ll try adding this\"}\\n',\n",
       " \"{'prompt': '4c is still random', 'response': 'but train.py is deterministic'}\\n\",\n",
       " '{\\'prompt\\': \\'but train.py is deterministic\\', \\'response\\': \"I checked that data augmentation should be reproducible. Then it\\'s strange why not 4c\"}\\n',\n",
       " \"{'prompt': 'Oohh like the random rotation and random crop?', 'response': 'yeah the angles come out deterministic'}\\n\",\n",
       " '{\\'prompt\\': \\'Kong voted for \"Introduction\" in the poll.\\', \\'response\\': \\'I will try the architecture described in this lecture for 5A \\'}\\n',\n",
       " \"{'prompt': 'I‚Äôm thinking latest by 6pm today, but I‚Äôm flexible', 'response': 'I plan to put up the plots after we finalize our models ye'}\\n\",\n",
       " \"{'prompt': 'I plan to put up the plots after we finalize our models ye', 'response': '6pm sounds good'}\\n\",\n",
       " \"{'prompt': 'Okay and once you run everything, could you push your version of 5b to main', 'response': 'I can also fill in the segmentation result since it requires someone having all 7 models'}\\n\",\n",
       " \"{'prompt': 'Ok I‚Äôll get you the best possible UNet by 6 üòé', 'response': 'Do you guys know how to get rid of the CUDA out of memory issue?'}\\n\",\n",
       " \"{'prompt': 'Do you guys know how to get rid of the CUDA out of memory issue?', 'response': 'Restarting the server used to help, but not this time'}\\n\",\n",
       " \"{'prompt': 'And restarting', 'response': 'for 5A I made the stride 1 and batch_size=4 and got this for epoch 1'}\\n\",\n",
       " '{\\'prompt\\': \\'NAISEEEEEE\\', \\'response\\': \"we\\'ll see if it\\'s gonna be good!\"}\\n',\n",
       " \"{'prompt': 'and yeah this is interesting, I got 0.062 IoU only after changing the kernel size in UNet', 'response': 'changing to what kernel size?'}\\n\",\n",
       " \"{'prompt': 'And final layer from 1*1 to 3*3', 'response': 'epoch 2 IoU: 0.1136'}\\n\",\n",
       " \"{'prompt': 'epoch 2 IoU: 0.1136', 'response': 'but I accidentally interrupted the training lol'}\\n\",\n",
       " \"{'prompt': 'but I accidentally interrupted the training lol', 'response': 'sad, each epoch takes 8 min to train'}\\n\",\n",
       " \"{'prompt': 'so ill report that result', 'response': 'Yes I‚Äôm doing 10 epochs too for 5A'}\\n\",\n",
       " \"{'prompt': 'Yes I‚Äôm doing 10 epochs too for 5A', 'response': 'Good news! TA just said Unet IoU will be graded very leniently'}\\n\",\n",
       " \"{'prompt': 'Good news! TA just said Unet IoU will be graded very leniently', 'response': '0.06 is on'}\\n\",\n",
       " \"{'prompt': '0.06 is on', 'response': 'Ok'}\\n\",\n",
       " \"{'prompt': 'Just curious', 'response': 'Yes, Jeremy said 15 min?'}\\n\",\n",
       " \"{'prompt': 'Yes, Jeremy said 15 min?', 'response': 'Possibly longer'}\\n\",\n",
       " \"{'prompt': 'SHEEEESH', 'response': 'just by changing stride'}\\n\",\n",
       " \"{'prompt': 'just by changing stride', 'response': '5A we gucci'}\\n\",\n",
       " \"{'prompt': '5A we gucci', 'response': '@Jonathan Cheung could you experiment with having other layers too? TA said just changing stride is not big enough architecture change'}\\n\",\n",
       " \"{'prompt': 'Yeah', 'response': '^also remember to add a table for your architecture details like Jeremy and Winfrey did in the report'}\\n\",\n",
       " \"{'prompt': 'thanks for the batch size = 4 idea HAHHA', 'response': 'YOOOOO'}\\n\",\n",
       " \"{'prompt': 'YOOOOO', 'response': 'it trains pretty fast too'}\\n\",\n",
       " \"{'prompt': 'I modified kernel size as well', 'response': 'Could you see if it gets higher than 0.12?'}\\n\",\n",
       " \"{'prompt': 'i can push the 5a code if thats fine with everyone', 'response': 'Your branch is ok! I‚Äôll just copy paste for ease lol'}\\n\",\n",
       " '{\\'prompt\\': \"master\\'s README has been updated for submission later today!\", \\'response\\': \\'THANKS!\\'}\\n',\n",
       " \"{'prompt': 'THANKS!', 'response': 'I‚Äôve finished running 5A and will update the report with the plots for the Results section after my tutor hours'}\\n\",\n",
       " \"{'prompt': 'Ok coools!', 'response': 'Unet now has a lower accuracy'}\\n\",\n",
       " \"{'prompt': 'Sure', 'response': 'I changed train, val, and test batch size to 4'}\\n\",\n",
       " '{\\'prompt\\': \\'I changed train, val, and test batch size to 4\\', \\'response\\': \"I\\'ll double check that the IoU function is not dependent on batch size haha\"}\\n',\n",
       " \"{'prompt': 'Dang UNet was crawling and now it‚Äôs flying', 'response': 'LOL smaller batch size is better I learned'}\\n\",\n",
       " '{\\'prompt\\': \"jk this screenshot doesn\\'t say much\", \\'response\\': \\'The batch size can be understood as a trade-off between accuracy and speed. Large batch sizes can lead to faster training times but may result in lower accuracy and overfitting, while smaller batch sizes can provide better accuracy, but can be computationally expensive and time-consuming.\\'}\\n',\n",
       " \"{'prompt': 'I like this sample!!', 'response': 'O OOP my IoU implementation is not quite right'}\\n\",\n",
       " \"{'prompt': 'O OOP my IoU implementation is not quite right', 'response': 'It‚Äôs supposed to be over the entire validation set'}\\n\",\n",
       " \"{'prompt': 'Or the iou function', 'response': 'Hmm I think I did implement it according to the TA‚Äôs description'}\\n\",\n",
       " \"{'prompt': 'and then u average it in the train file', 'response': 'I suppose it worked nonetheless'}\\n\",\n",
       " \"{'prompt': 'I suppose it worked nonetheless', 'response': 'But I‚Äôm averaging over each batch and then averaging all the batches‚Äô average IoU'}\\n\",\n",
       " \"{'prompt': 'But I‚Äôm averaging over each batch and then averaging all the batches‚Äô average IoU', 'response': 'So the number might be a little odd'}\\n\",\n",
       " \"{'prompt': 'Oh util iou averages over the batch', 'response': 'Util iou is right if the input is only one image, but right now the input is a batch of images'}\\n\",\n",
       " \"{'prompt': 'Util iou is right if the input is only one image, but right now the input is a batch of images', 'response': 'Wait just kidding'}\\n\",\n",
       " \"{'prompt': 'Wait just kidding', 'response': 'Let me think again'}\\n\",\n",
       " \"{'prompt': 'Let me think again', 'response': 'Maybe the input is indeed just an image'}\\n\",\n",
       " \"{'prompt': 'Maybe the input is indeed just an image', 'response': 'LOL sorry for psyching out we‚Äôre probably fine?'}\\n\",\n",
       " \"{'prompt': 'LOL sorry for psyching out we‚Äôre probably fine?', 'response': 'But I discovered that when I make the test batch size smaller the IoU goes up, which it shouldn‚Äôt'}\\n\",\n",
       " \"{'prompt': 'enumerate batch_loader gives a batch. So inputs are 16*3*224*224', 'response': 'O RIGHT'}\\n\",\n",
       " \"{'prompt': 'O RIGHT', 'response': 'hmmm let me go through the code and check'}\\n\",\n",
       " \"{'prompt': 'Or do a double loop in util iou, so   loop through preds, targets:    Loop through all classes:    Np.nanmean for that particular one pred', 'response': 'I‚Äôll try double loop'}\\n\",\n",
       " \"{'prompt': 'I‚Äôll try double loop', 'response': 'Iou becomes 0.2619 for train.py'}\\n\",\n",
       " \"{'prompt': 'Iou becomes 0.2619 for train.py', 'response': 'I have a feeling that should be more accurate but the TA implemented it wrong? üòÆ maybe maybe not '}\\n\",\n",
       " \"{'prompt': 'I have a feeling that should be more accurate but the TA implemented it wrong? üòÆ maybe maybe not ', 'response': '0.05 felt too low'}\\n\",\n",
       " \"{'prompt': '0.05 felt too low', 'response': 'So maybe they meant over the whole validation set instead of each individual image'}\\n\",\n",
       " \"{'prompt': 'So maybe they meant over the whole validation set instead of each individual image', 'response': 'High key I will just revert back to what we had before because it matched the description well'}\\n\",\n",
       " \"{'prompt': 'Agreed', 'response': 'I‚Äôll aim to be done by 9:30pm but'}\\n\",\n",
       " \"{'prompt': 'oh ok i‚Äôll start working on it rn', 'response': 'I‚Äôll update code to do full validation set after and see if there‚Äôs a big difference'}\\n\",\n",
       " \"{'prompt': 'I‚Äôll update code to do full validation set after and see if there‚Äôs a big difference', 'response': 'Samuel chu said he tried and it wasn‚Äôt very different'}\\n\",\n",
       " \"{'prompt': '@Jonathan Cheung just checking, for 5a specifically, did you make any changes to  - weight initialization - data augmentation - optimizer - class weights', 'response': 'just added all the plots'}\\n\",\n",
       " \"{'prompt': 'Oh no worries, I think Ester got u', 'response': 'I got it yeah!'}\\n\",\n",
       " \"{'prompt': 'Ok I see it now!', 'response': 'I‚Äôm waiting to get one more plot updated potentially'}\\n\",\n",
       " \"{'prompt': 'I‚Äôm waiting to get one more plot updated potentially', 'response': 'But I will get the report done rn'}\\n\",\n",
       " \"{'prompt': 'But I will get the report done rn', 'response': 'Please add writing to the results section if you guys can'}\\n\",\n",
       " \"{'prompt': 'Please add writing to the results section if you guys can', 'response': 'I just wanna get a better plot for 5A if possible'}\\n\",\n",
       " \"{'prompt': 'Like it calls FCN and uses AdamW', 'response': 'Ohh let me see'}\\n\",\n",
       " \"{'prompt': 'Ohh let me see', 'response': 'I have the updated version locally but somehow didn‚Äôt update on GitHub'}\\n\",\n",
       " \"{'prompt': 'I have the updated version locally but somehow didn‚Äôt update on GitHub', 'response': 'I‚Äôll fix that rn'}\\n\",\n",
       " \"{'prompt': 'I‚Äôll fix that rn', 'response': 'Resubmitted code'}\\n\",\n",
       " \"{'prompt': 'Resubmitted code', 'response': 'We still need to write some of discussion section'}\\n\",\n",
       " \"{'prompt': 'Currently we are only drawing from table', 'response': 'The 5A test iou is not accurate, it should be lower'}\\n\",\n",
       " \"{'prompt': 'The 5A test iou is not accurate, it should be lower', 'response': 'I‚Äôm rerunning it but it‚Äôs too slow oop'}\\n\",\n",
       " \"{'prompt': 'I‚Äôm rerunning it but it‚Äôs too slow oop', 'response': 'I‚Äôm adding to the discussion right now'}\\n\",\n",
       " \"{'prompt': 'Hmmmm I‚Äôm kind of stuck with UNet‚Äôs discussion on the sample test segmentation, since it had worse results than the other two architecture, the test segmentation shows that UNet captures a lot of them üëÄ', 'response': 'Weird yeah '}\\n\",\n",
       " '{\\'prompt\\': \"yes, could you check if 5a\\'s discussion mentions the loss plot and the segmentation visual\", \\'response\\': \\'Still need to add something about the segmentation visual\\'}\\n',\n",
       " \"{'prompt': 'ok yea looking rn', 'response': 'It might be the old version!'}\\n\",\n",
       " '{\\'prompt\\': \\'rather than \"optima\"\\', \\'response\\': \\'Yes, but optima means the same thing right\\'}\\n',\n",
       " \"{'prompt': 'Rerunning 5A', 'response': 'It‚Äôs getting faster now'}\\n\",\n",
       " \"{'prompt': 'It‚Äôs getting faster now', 'response': '5 min per epoch'}\\n\",\n",
       " \"{'prompt': 'nice nice', 'response': 'It was 24 min or something earlier'}\\n\",\n",
       " \"{'prompt': 'Is there a way to make the discussion section go after the results section‚Äôs figures?', 'response': 'Right now discussion starts in between those figures'}\\n\",\n",
       " \"{'prompt': 'try \\\\\\\\newpage', 'response': 'It‚Äôs already there haha'}\\n\",\n",
       " \"{'prompt': 'forcing new page on new section', 'response': 'No idea why item doesn‚Äôt help'}\\n\",\n",
       " \"{'prompt': 'use \\\\\\\\clearpage instead', 'response': 'Is it intentional we made the images smaller?'}\\n\",\n",
       " \"{'prompt': 'Is it intentional we made the images smaller?', 'response': '2 per page seems to fit pretty well'}\\n\",\n",
       " \"{'prompt': 'yes on my end', 'response': 'Sure!'}\\n\",\n",
       " \"{'prompt': 'i can submit?', 'response': 'The time crunch is real '}\\n\",\n",
       " \"{'prompt': 'Yup!', 'response': 'I‚Äôll need to resubmit it lolol I made some changes'}\\n\",\n",
       " \"{'prompt': 'lmk when ur finsihed changing', 'response': 'My 5A is almost done lol'}\\n\",\n",
       " \"{'prompt': 'My 5A is almost done lol', 'response': 'Let me check the results'}\\n\",\n",
       " \"{'prompt': 'Let me check the results', 'response': 'Dang it‚Äôs cutting real close'}\\n\",\n",
       " \"{'prompt': 'Dang it‚Äôs cutting real close', 'response': 'I‚Äôll resubmit right now just in case'}\\n\",\n",
       " \"{'prompt': 'do u want me to do it?', 'response': 'I‚Äôm updating something rn'}\\n\",\n",
       " \"{'prompt': 'I‚Äôm updating something rn', 'response': 'Ok just submitted, will add people'}\\n\",\n",
       " \"{'prompt': 'Ok just submitted, will add people', 'response': 'Done! Sorry for cutting it so close!'}\\n\",\n",
       " \"{'prompt': 'No no, not at all', 'response': 'Please check if it‚Äôs good'}\\n\",\n",
       " \"{'prompt': 'All good, thanks for the good work everyone!! Get some good rest for now üò¥', 'response': 'YAY'}\\n\",\n",
       " \"{'prompt': 'Group name is recurrent for PA3!', 'response': 'Yayay!'}\\n\",\n",
       " \"{'prompt': 'Yayay!', 'response': 'Joined'}\\n\",\n",
       " \"{'prompt': 'Joined', 'response': 'Ester named the group CSE 151B PA3.'}\\n\",\n",
       " \"{'prompt': 'Thanks for the name update that‚Äôs very thoughtful of you HAHA', 'response': 'Ester Tsai added Samuel Chu to the group.'}\\n\",\n",
       " \"{'prompt': 'Ester Tsai added Samuel Chu to the group.', 'response': '@Samuel Chu if you‚Äôd like to join!'}\\n\",\n",
       " \"{'prompt': 'I will get started on the programming part tonight, has anyone started working on it? Just wanted to see where I should start with, in case some parts have already been completed', 'response': 'I plan to start at 3:30pm!'}\\n\",\n",
       " \"{'prompt': 'i‚Äôm prob going to start working tonight or tmr, if i do start today i‚Äôll lyk', 'response': 'Actually I‚Äôll start maybe tonight! Gonna crank out some tutor stuff that‚Äôs time sensitive'}\\n\",\n",
       " \"{'prompt': 'I should be able to get the baseline out today  But without the generate.py', 'response': 'Thank you Winfrey!'}\\n\",\n",
       " \"{'prompt': 'Thank you Winfrey!', 'response': 'I‚Äôll do the individual part first and then get on the code'}\\n\",\n",
       " \"{'prompt': 'The loss I‚Äôm getting is lower (0.06) than expected (1.9) so I‚Äôm still clarifying with the TA on loss calculation - there‚Äôs a bunch of averaging going on ü•≤', 'response': 'I see!! I‚Äôm done with the individual part so I‚Äôll be able to get to the code tomorrow!'}\\n\",\n",
       " \"{'prompt': 'I see!! I‚Äôm done with the individual part so I‚Äôll be able to get to the code tomorrow!', 'response': 'Or later tonight üòÆ'}\\n\",\n",
       " \"{'prompt': 'Or later tonight üòÆ', 'response': 'Wowow this PA seems fun'}\\n\",\n",
       " \"{'prompt': 'Wowow this PA seems fun', 'response': 'I‚Äôll work on it today'}\\n\",\n",
       " \"{'prompt': 'I‚Äôll work on it today', 'response': 'Just learned the ABC notation üò≥'}\\n\",\n",
       " \"{'prompt': 'The lowest loss I get so far is 2.1679, Im making changes to the optimizer to find the one that gives the closest to 1.9  Other than that, my branch is the most updated of my progress', 'response': '@Jeremy Tow @Jonathan Cheung @Samuel Chu any progress so far? üòÉ'}\\n\",\n",
       " \"{'prompt': 'RuntimeError: cuDNN version incompatibility: PyTorch was compiled  against (8, 7, 0) but found runtime version (8, 6, 0). PyTorch already comes bundled with cuDNN. One option to resolving this error is to ensure PyTorch can find the bundled cuDNN.Looks like your LD_LIBRARY_PATH contains incompatible version of cudnnPlease either remove it from the path or install cudnn (8, 7, 0)', 'response': 'The Piazza post about this issue did not help lol'}\\n\",\n",
       " \"{'prompt': 'The Piazza post about this issue did not help lol', 'response': 'btw here is the Overleaf report link for PA3! '}\\n\",\n",
       " \"{'prompt': 'btw here is the Overleaf report link for PA3! ', 'response': 'Has anyone else figured out how to solve this?'}\\n\",\n",
       " \"{'prompt': 'You got the error when u were trying to install PyTorch?', 'response': 'No, when I run main.py'}\\n\",\n",
       " \"{'prompt': 'No, when I run main.py', 'response': 'This is me lol'}\\n\",\n",
       " '{\\'prompt\\': \\'conda install does not even finish installing and bugs out haha\\', \\'response\\': \"pip install doesn\\'t solve the problem\"}\\n',\n",
       " '{\\'prompt\\': \"pip install doesn\\'t solve the problem\", \\'response\\': \\'O I FIXED IT\\'}\\n',\n",
       " \"{'prompt': 'OH how', 'response': 'unset LD_LIBRARY_PATH'}\\n\",\n",
       " '{\\'prompt\\': \\'Anything that has to do with PATH is very painful\\', \\'response\\': \"Does that mean I\\'m not using CUDA tho?\"}\\n',\n",
       " '{\\'prompt\\': \"Does that mean I\\'m not using CUDA tho?\", \\'response\\': \"it\\'s very slow oop\"}\\n',\n",
       " '{\\'prompt\\': \\'Wifi matters a lot üòÇ\\', \\'response\\': \"I\\'m training on cpu oops\"}\\n',\n",
       " '{\\'prompt\\': \\'Hmmmm\\', \\'response\\': \"but that\\'s what the TA suggested\"}\\n',\n",
       " '{\\'prompt\\': \"but that\\'s what the TA suggested\", \\'response\\': \\'how long does it take for you?\\'}\\n',\n",
       " \"{'prompt': 'For 100 epochs', 'response': 'oh me too then'}\\n\",\n",
       " '{\\'prompt\\': \\'oh me too then\\', \\'response\\': \"jk it\\'s slower for me\"}\\n',\n",
       " \"{'prompt': 'Ahh cause it‚Äôs on cpu?', 'response': 'yeah : //'}\\n\",\n",
       " \"{'prompt': 'yeah : //', 'response': 'O IM OK NOW'}\\n\",\n",
       " '{\\'prompt\\': \\'O IM OK NOW\\', \\'response\\': \"I\\'m using capstone platform\"}\\n',\n",
       " '{\\'prompt\\': \"I\\'m using capstone platform\", \\'response\\': \\'instead of datahub\\'}\\n',\n",
       " \"{'prompt': 'instead of datahub', 'response': 'YAYAYAYAYYAYAYYAYAYAYY'}\\n\",\n",
       " \"{'prompt': 'Oh THATS GREAT', 'response': 'CAPSToNE SUPREMEM'}\\n\",\n",
       " '{\\'prompt\\': \\'Honestly, I have never used datahub once\\', \\'response\\': \"it\\'s really fast, like 3 epochs per minute?\"}\\n',\n",
       " \"{'prompt': 'Wow urs is slightly faster actually', 'response': 'I use n 30'}\\n\",\n",
       " \"{'prompt': 'I use n 30', 'response': 'It‚Äôs a bigger GPU although not sure if that affects speed?'}\\n\",\n",
       " '{\\'prompt\\': \\'i‚Äôm curious what gpu it is lol\\', \\'response\\': \\'it\\\\\\'s called \"a5000\"\\'}\\n',\n",
       " '{\\'prompt\\': \\'it\\\\\\'s called \"a5000\"\\', \\'response\\': \\'Is 2.1679 the validation loss?\\'}\\n',\n",
       " \"{'prompt': 'Is 2.1679 the validation loss?', 'response': 'is it correct there is no test loss?'}\\n\",\n",
       " \"{'prompt': 'And yeah no test loss, not given in the starter code', 'response': 'do you know if we are allowed to change other configs like SEQ_SIZE'}\\n\",\n",
       " \"{'prompt': 'ah that I‚Äôm not sure. I thought we are not supposed to change anything in the config until like 5 mins ago a TA said try more epochs', 'response': 'hmm I suppose instruction said to use SEQ_SIZE=25~30'}\\n\",\n",
       " \"{'prompt': 'Ohhh good catch!', 'response': 'true more epochs might help. the validation loss is still decreasing'}\\n\",\n",
       " '{\\'prompt\\': \\'What is this?\\', \\'response\\': \"Don\\'t do that haha\"}\\n',\n",
       " '{\\'prompt\\': \"Don\\'t do that haha\", \\'response\\': \\'it removes CUDNN basically\\'}\\n',\n",
       " \"{'prompt': 'I‚Äôm getting the same error, you fixed it by switching to capstone stuff?', 'response': 'so it solves the error but you can only use CPU'}\\n\",\n",
       " \"{'prompt': 'so it solves the error but you can only use CPU', 'response': 'yes'}\\n\",\n",
       " \"{'prompt': 'yes', 'response': 'I ssh from terminal, not datahub'}\\n\",\n",
       " \"{'prompt': 'Ok yeah', 'response': 'rn the code for generate.py works but the result is not functional'}\\n\",\n",
       " '{\\'prompt\\': \\'I‚Äôm not sure if those bolded characters are identified as unknown\\', \\'response\\': \"the bold shows bad syntax (system can\\'t recognize what it wants to do)\"}\\n',\n",
       " \"{'prompt': 'So there‚Äôs gonna be some way we need to figure out to control/process them before putting into the converter', 'response': 'For generate.py, Is it correct that the model should only take in the most recent prediction as the input?'}\\n\",\n",
       " \"{'prompt': 'For generate.py, Is it correct that the model should only take in the most recent prediction as the input?', 'response': 'I wonder how it knows what the context is if I do that'}\\n\",\n",
       " \"{'prompt': 'There‚Äôs the hidden state that saves the context', 'response': 'Let me fix something real quick and check again'}\\n\",\n",
       " \"{'prompt': 'Okk', 'response': '@Kong Xian Ying when you finish training, could you send me the model checkpoint if possible? or I can just send you my generate.py'}\\n\",\n",
       " \"{'prompt': '@Ester Tsai r u sshing into capstone and using cpu?', 'response': 'No, GPU'}\\n\",\n",
       " \"{'prompt': 'im getting the same error even on gpu lol', 'response': 'This is the extended fur elise'}\\n\",\n",
       " '{\\'prompt\\': \\'This is the extended fur elise\\', \\'response\\': \\'X:1759 T:F\\\\\\\\\"ur Elise T:Bagatelle No.25 in A, WoO.59 C:Ludwig van Beethoven O:Germany Z:Transcribed by Frank Nordberg - http://www.musicaviva.com F:http://abc.musicaviva.com/tunes/beethoven-ludwig-van/be059/be059-pno2.abc V:1 Program 1 0 Piano V:2 Program 1 0 bass Piano M:3/8 L:1/16 Q:3/8=40 K:Am V:1 e^d|e^deB=dc|A2 z CEA|B2 z E^GB|c2 z Ee^d| V:2 z2|z6|A,,E,A, z z2|E,,E,^G, z z2|A,,E,A, z z2| (DD (FABA A2c|defg abag|(3fee f2 (3gag (3gfe (3dcB BA|1 GAAF Ldeg|bagf gab|ede dcBA|adcB cBAG|FAFA FABd|1 (3BGA ABdB CAFA|d3ef d2fg|(3efe deBd| ecAd edcA|(3BAF GBAA| G2GF A2BG ABcA| AAF AFE|eAAF ABdB|cAA ABA|A GFG A2d:|\\'}\\n',\n",
       " '{\\'prompt\\': \\'X:1759 T:F\\\\\\\\\"ur Elise T:Bagatelle No.25 in A, WoO.59 C:Ludwig van Beethoven O:Germany Z:Transcribed by Frank Nordberg - http://www.musicaviva.com F:http://abc.musicaviva.com/tunes/beethoven-ludwig-van/be059/be059-pno2.abc V:1 Program 1 0 Piano V:2 Program 1 0 bass Piano M:3/8 L:1/16 Q:3/8=40 K:Am V:1 e^d|e^deB=dc|A2 z CEA|B2 z E^GB|c2 z Ee^d| V:2 z2|z6|A,,E,A, z z2|E,,E,^G, z z2|A,,E,A, z z2| (DD (FABA A2c|defg abag|(3fee f2 (3gag (3gfe (3dcB BA|1 GAAF Ldeg|bagf gab|ede dcBA|adcB cBAG|FAFA FABd|1 (3BGA ABdB CAFA|d3ef d2fg|(3efe deBd| ecAd edcA|(3BAF GBAA| G2GF A2BG ABcA| AAF AFE|eAAF ABdB|cAA ABA|A GFG A2d:|\\', \\'response\\': \\'best val loss so far 2.0089\\'}\\n',\n",
       " \"{'prompt': 'Nice niceeeee', 'response': 'my newest progress is on ester2 branch'}\\n\",\n",
       " '{\\'prompt\\': \\'my newest progress is on ester2 branch\\', \\'response\\': \"haven\\'t done hyperparamter tuning yet but I set up the plot name to reflect different hyperparameter choices\"}\\n',\n",
       " '{\\'prompt\\': \"haven\\'t done hyperparamter tuning yet but I set up the plot name to reflect different hyperparameter choices\", \\'response\\': \\'might be helpful to make a branch from my branch\\'}\\n',\n",
       " \"{'prompt': 'ur so amazing', 'response': '@Jeremy Tow @Samuel Chu does the GPU work for you guys when you run Python main.py?'}\\n\",\n",
       " \"{'prompt': '@Jeremy Tow @Samuel Chu does the GPU work for you guys when you run Python main.py?', 'response': 'Also our current val loss is ok! We can move on to hyperparameter tuning,. I;ll start that tonight'}\\n\",\n",
       " '{\\'prompt\\': \"is it supposed to be config[\\'dropout\\']?\", \\'response\\': \\'O yeah small typo\\'}\\n',\n",
       " \"{'prompt': 'okay', 'response': 'It‚Äôs config[‚Äúmodel_type‚Äù]'}\\n\",\n",
       " \"{'prompt': 'using datahub tho. i could try ssh', 'response': 'Piazza someone said they created a new conda environment and installed pytorch and it worked'}\\n\",\n",
       " \"{'prompt': 'for some reason torch.cuda.is_available() is false', 'response': 'Did you request a GPU when you ssh?'}\\n\",\n",
       " \"{'prompt': 'Did you request a GPU when you ssh?', 'response': 'Just updated my branch more for hyper parameter tuning. Gonna run it overnight and get all the results'}\\n\",\n",
       " \"{'prompt': 'Just updated my branch more for hyper parameter tuning. Gonna run it overnight and get all the results', 'response': 'Does anyone understand how to do part 6 (feature evaluation)? Is the ‚Äúactivation of each neuron‚Äù the result of calling model.forward()?'}\\n\",\n",
       " \"{'prompt': 'Does anyone understand how to do part 6 (feature evaluation)? Is the ‚Äúactivation of each neuron‚Äù the result of calling model.forward()?', 'response': 'Currently doing AdamW with lr=1e-3 and no weight decay, but you guys should try different optimizer setup and see if results are different üòÆüòÆ'}\\n\",\n",
       " \"{'prompt': 'Currently doing AdamW with lr=1e-3 and no weight decay, but you guys should try different optimizer setup and see if results are different üòÆüòÆ', 'response': 'run these to test hyper parameters if you clone my latest branch! python main.py python main.py --config config_RNN.json python main.py --config config_200_neurons.json python main.py --config config_250_neurons.json python main.py --config config_dropout_0.2.json python main.py --config config_dropout_0.3.json'}\\n\",\n",
       " \"{'prompt': 'run these to test hyper parameters if you clone my latest branch! python main.py python main.py --config config_RNN.json python main.py --config config_200_neurons.json python main.py --config config_250_neurons.json python main.py --config config_dropout_0.2.json python main.py --config config_dropout_0.3.json', 'response': 'Hold on gonna fix something small real quick'}\\n\",\n",
       " \"{'prompt': 'Not too sure, I only remember seeing a post on piazza about softmax and temperature', 'response': 'lol that‚Äôs my post'}\\n\",\n",
       " \"{'prompt': 'lol that‚Äôs my post', 'response': 'I think I figured the temperature out'}\\n\",\n",
       " \"{'prompt': 'I think I figured the temperature out', 'response': 'But I will ask in OH today to confirm'}\\n\",\n",
       " \"{'prompt': 'But I will ask in OH today to confirm', 'response': 'Finished updating! Now we can know the training loss that correspond to the min Val loss'}\\n\",\n",
       " \"{'prompt': 'Finished updating! Now we can know the training loss that correspond to the min Val loss', 'response': 'Can we call at 3pm today to do a quick check-in for what to get done over weekend?'}\\n\",\n",
       " \"{'prompt': 'Can we call at 3pm today to do a quick check-in for what to get done over weekend?', 'response': '@everyone'}\\n\",\n",
       " \"{'prompt': '@Ester Tsai @Jonathan Cheung logistic is actually a nonlinear technique, good job!! ü§£üéâ', 'response': 'Huhhhhh'}\\n\",\n",
       " \"{'prompt': 'Huhhhhh', 'response': 'I‚Äôm working on the heat map, almost done'}\\n\",\n",
       " \"{'prompt': 'What‚Äôs the lowest val loss so far with hyperparameter tuning?', 'response': 'Baseline is still the best'}\\n\",\n",
       " \"{'prompt': 'Baseline is still the best', 'response': '2.008'}\\n\",\n",
       " \"{'prompt': 'Thanks!!', 'response': 'Reminder we will call at 3pm'}\\n\",\n",
       " \"{'prompt': 'Reminder we will call at 3pm', 'response': 'I‚Äôll go to the 2-3pm OH'}\\n\",\n",
       " \"{'prompt': 'I‚Äôll go to the 2-3pm OH', 'response': 'The heat map has been implemented but it has a small bug'}\\n\",\n",
       " \"{'prompt': 'The heat map has been implemented but it has a small bug', 'response': 'only the first 20 chars show up'}\\n\",\n",
       " \"{'prompt': 'Jk he moved it again so there‚Äôs no OH rn', 'response': 'I‚Äôm ready call anytime if you guys are'}\\n\",\n",
       " \"{'prompt': 'wait i‚Äôm not ready', 'response': 'what time works for you?'}\\n\",\n",
       " \"{'prompt': 'Ah can we keep it at 3? I‚Äôm at work until 3pm and I have a consultation at the moment', 'response': 'ok!'}\\n\",\n",
       " '{\\'prompt\\': \\'I‚Äôll get on soon, in capstone meeting rn\\', \\'response\\': \"let\\'s do 3pm still\"}\\n',\n",
       " '{\\'prompt\\': \"let\\'s do 3pm still\", \\'response\\': \\'both winfreya nd I will need to leave by 3:30\\'}\\n',\n",
       " \"{'prompt': '^ report', 'response': 'ok I pushed latest changes!'}\\n\",\n",
       " '{\\'prompt\\': \"update: Winfrey showed me what she did on the capstone gpu, but it still isn\\'t working on my end :| so I think she will try to train a model with new hyperparameters tmr maybe?\", \\'response\\': \\'Okok! You could still write the report using the plot results on my branch if needed üëåüëå\\'}\\n',\n",
       " \"{'prompt': 'okay is there stuff that you tested that is not on the report already?', 'response': 'I have the basic set of results that the report needs (loss plots, heatmaps, and the training/val loss are in the model checkpoint names), but we can test more models and get better results'}\\n\",\n",
       " \"{'prompt': 'i just timed a single epoch and extrapolated', 'response': 'Should I try running anything when I‚Äôm back or just focus on the report?'}\\n\",\n",
       " \"{'prompt': 'seq length of 50', 'response': 'Woah nice'}\\n\",\n",
       " \"{'prompt': 'Woah nice', 'response': 'Should we just go with the results you guys got?'}\\n\",\n",
       " \"{'prompt': 'Should we just go with the results you guys got?', 'response': 'Lmk if I can help run anything!'}\\n\",\n",
       " \"{'prompt': 'Yes! could you run config 250 neurons with sequence length 50?', 'response': 'AdamW lr=1e-3?'}\\n\",\n",
       " \"{'prompt': 'Yupp everything else the same', 'response': 'how about the existing result on report?'}\\n\",\n",
       " \"{'prompt': 'Ohh I pushed a small fix for train.py last weekend so u might need to check with git pull', 'response': 'okok!'}\\n\",\n",
       " \"{'prompt': 'As in, the current row 1 is accurate', 'response': 'Which branch should I pull?'}\\n\",\n",
       " \"{'prompt': 'ester2', 'response': 'Okok!'}\\n\",\n",
       " \"{'prompt': 'Okok!', 'response': 'Oo what was the bug in train.py?'}\\n\",\n",
       " \"{'prompt': 'So now the songrnn forward returns output, _ instead of just output', 'response': 'Got it!'}\\n\",\n",
       " \"{'prompt': 'Got it!', 'response': 'Now running config_250_neurons and the RNN next if it‚Äôs not done already'}\\n\",\n",
       " \"{'prompt': 'im kind of confused on temperature can affect how deterministic the model is. bc if we run these values: 5, 10, 15 though a softmax no matter what we divide each element by, when we pick the element with the highest softmax value it will always be the 2nd index(15) right?', 'response': 'We are not picking the element with the highest soft max value'}\\n\",\n",
       " \"{'prompt': 'We are not picking the element with the highest soft max value', 'response': 'Soft max value goes into torch.multinomial to serve as a list of probabilities'}\\n\",\n",
       " \"{'prompt': 'Soft max value goes into torch.multinomial to serve as a list of probabilities', 'response': 'Torch.multinomial randomly selects a character to output'}\\n\",\n",
       " '{\\'prompt\\': \\'ok\\', \\'response\\': \"I\\'m updating the related works section with citations\"}\\n',\n",
       " '{\\'prompt\\': \"I\\'m updating the related works section with citations\", \\'response\\': \\'feel free to double check and add more\\'}\\n',\n",
       " '{\\'prompt\\': \\'feel free to double check and add more\\', \\'response\\': \"I\\'m done with my written portions! Lmk if another section needs help\"}\\n',\n",
       " '{\\'prompt\\': \"I\\'m done with my written portions! Lmk if another section needs help\", \\'response\\': \\'I pushed my results for config_250_neurons.json and added the train and val loss to the report\\'}\\n',\n",
       " \"{'prompt': 'Gosh it finished?', 'response': 'LOL YEAH'}\\n\",\n",
       " \"{'prompt': 'result is worse than baseline but better than sequence length 30', 'response': 'overfitting on training'}\\n\",\n",
       " \"{'prompt': 'i have 200 epochs left on the rnn run and after that i‚Äôll be finished i think', 'response': 'I ran the rnn earlier actually! but you can verify the results with mine'}\\n\",\n",
       " \"{'prompt': 'i‚Äôll just take the model with the best loss?', 'response': 'how are the results on different dropouts?'}\\n\",\n",
       " \"{'prompt': 'how are the results on different dropouts?', 'response': 'is the report the most up to date?'}\\n\",\n",
       " \"{'prompt': 'I need to do abstract and intro rn', 'response': 'which one is this?'}\\n\",\n",
       " \"{'prompt': 'i might have mistyped a parameter or smth', 'response': 'aight I can run it too'}\\n\",\n",
       " \"{'prompt': 'aight I can run it too', 'response': 'so far baseline is still the best'}\\n\",\n",
       " \"{'prompt': 'Ok added results and plot for 200 neurons, will generate the heatmaps now', 'response': 'Oops I didn‚Äôt time it. Would it be helpful if I rerun part of it to time it?'}\\n\",\n",
       " \"{'prompt': 'Yes', 'response': 'Sure! We can make the figure a little wider and less tall'}\\n\",\n",
       " \"{'prompt': 'do we need to specify our sequence length for table 1 in results?', 'response': 'We will need to describe it in the methods section'}\\n\",\n",
       " \"{'prompt': 'is it always 30?', 'response': 'It‚Äôs always 50'}\\n\",\n",
       " \"{'prompt': 'Also does red correspond to low or high activation?', 'response': 'High activation'}\\n\",\n",
       " \"{'prompt': 'High activation', 'response': 'You can see the example in the PA3 instruction'}\\n\",\n",
       " \"{'prompt': 'tyty', 'response': '@Jeremy Tow could you add your contributions at the end of the report?'}\\n\",\n",
       " \"{'prompt': 'HAHHAHAHAHAH spot on', 'response': 'Not me laughing out loud'}\\n\",\n",
       " \"{'prompt': 'is that correct?', 'response': 'Seems reasonable'}\\n\",\n",
       " \"{'prompt': 'i think they wanted it uploaded seperately? i vaguely remember reading about this somewhere', 'response': '@Jeremy Tow would you like to add a sentence to the Abstract to describe how the generated music sounds?'}\\n\",\n",
       " \"{'prompt': 'Ok I‚Äôll do that', 'response': 'Do we need any help with the code submission?'}\\n\",\n",
       " '{\\'prompt\\': \"the pa just came out -- i\\'ve created a team called transformative\", \\'response\\': \\'Aight!\\'}\\n',\n",
       " \"{'prompt': 'Aight!', 'response': 'PA4 report can use this copy of PA3 --- '}\\n\",\n",
       " \"{'prompt': 'PA4 report can use this copy of PA3 --- ', 'response': 'still need to update the section headers and remove old content'}\\n\",\n",
       " \"{'prompt': 'still need to update the section headers and remove old content', 'response': '^ if anyone wanna help with setting up PA4 report! If not, I can do it in some days'}\\n\",\n",
       " \"{'prompt': 'Ester named the group CSE 151B PA4.', 'response': 'I started on the code on branch ester1'}\\n\",\n",
       " \"{'prompt': 'I started on the code on branch ester1', 'response': 'Just fixed some formatting issues with the template'}\\n\",\n",
       " \"{'prompt': 'Just fixed some formatting issues with the template', 'response': 'Solved some minor bugs'}\\n\",\n",
       " \"{'prompt': 'Are those on main or ester1? Which one should I pull', 'response': 'Jeremy is also making some progress'}\\n\",\n",
       " \"{'prompt': 'Jeremy is also making some progress', 'response': 'Main is the same as Ester 1 right now'}\\n\",\n",
       " \"{'prompt': 'I‚Äôll work on the programming part this Saturday, I‚Äôll check everyone‚Äôs progress by then before I start', 'response': '@Jeremy Tow would you like to push your code to your branch üòÆ I‚Äôm having a hard time making model.py work oop haha'}\\n\",\n",
       " \"{'prompt': 'ok pushed', 'response': 'update: the baseline training is working but the loss is still high (377) compared to the reference (39)'}\\n\",\n",
       " \"{'prompt': 'the ester1 branch has working code', 'response': 'run the command python main.py --embed-dim 768 --n-epochs 10'}\\n\",\n",
       " \"{'prompt': 'run the command python main.py --embed-dim 768 --n-epochs 10', 'response': 'TA said: Results.txt is irrelevant Reference this result instead: Baseline(10 epochs): Loss = 192.47; Val Accuracy = 84.21; Test Accuracy = 83.49'}\\n\",\n",
       " '{\\'prompt\\': \\'Ooo is our loss 377 still considered too high compared to the reference?\\', \\'response\\': \"yeah :0  it\\'ll need to be under 200\"}\\n',\n",
       " \"{'prompt': 'oooh okok! anywhere that u suspect is not doing the job properly?', 'response': 'we can still test different optimizer and scheduler and other hyperparameter tuning'}\\n\",\n",
       " \"{'prompt': 'Okie', 'response': 'Actually the code on ester1 has some odd issue rn, but Jeremy will push his working code soon'}\\n\",\n",
       " \"{'prompt': 'the code on the ester1 branch works fine for me lol', 'response': 'What‚Äôs wrong with my terminal then üò≠'}\\n\",\n",
       " \"{'prompt': 'What‚Äôs wrong with my terminal then üò≠', 'response': 'What command do you run on the terminal'}\\n\",\n",
       " \"{'prompt': 'i pushed the code btw', 'response': 'BRUH changing the GPU worked üòî'}\\n\",\n",
       " \"{'prompt': 'BRUH changing the GPU worked üòî', 'response': 'Quite possibly the most mysterious bug I‚Äôve seen'}\\n\",\n",
       " \"{'prompt': 'Quite possibly the most mysterious bug I‚Äôve seen', 'response': 'Can someone else run ester1 multiple times? My results behave very differently the second time I run it'}\\n\",\n",
       " \"{'prompt': 'Can someone else run ester1 multiple times? My results behave very differently the second time I run it', 'response': 'cd cse151b251b-wi24-pa4-transformative python main.py --embed-dim 768 --n-epochs 50  ^^ try this'}\\n\",\n",
       " \"{'prompt': '< 1 min per epoch', 'response': 'Feel free to change the number of epochs or add the argument for learning rate and try a higher learning rate'}\\n\",\n",
       " '{\\'prompt\\': \"hmmm since yesterday night i\\'ve been having a problem where the terminal won\\'t run at all lol, though it did run in the afternoon when i tested it. Right now, it\\'s just stuck and interrupting does work as well, has anyone faced this problem before? it\\'s the same for me both on datahub and using ssh\", \\'response\\': \\'Yes that happens sometimes\\'}\\n',\n",
       " \"{'prompt': 'interesting, first time for me, but good to know!', 'response': 'Not sure what‚Äôs the solutionü•∫'}\\n\",\n",
       " \"{'prompt': 'ok I can try your branch as well', 'response': 'TA said I can try adding'}\\n\",\n",
       " \"{'prompt': 'TA said I can try adding', 'response': 'gc.collect() torch.cuda.empty_cache()'}\\n\",\n",
       " '{\\'prompt\\': \\'i just ran it twice and i do not have this issue\\', \\'response\\': \"this doesn\\'t work. I still get validation acc: 0.06443679291687161 for every epocj\"}\\n',\n",
       " \"{'prompt': 'capstone', 'response': 'I am using capstone GPU'}\\n\",\n",
       " \"{'prompt': 'I am using capstone GPU', 'response': 'what command line prompt do you use? @Jeremy Tow'}\\n\",\n",
       " '{\\'prompt\\': \\'i tried the one that you sent\\', \\'response\\': \"Dang I have no idea what\\'s going on, I cloned Jeremy\\'s code and it still doesn\\'t work\"}\\n',\n",
       " '{\\'prompt\\': \"Dang I have no idea what\\'s going on, I cloned Jeremy\\'s code and it still doesn\\'t work\", \\'response\\': \"O wait it works when I don\\'t specify learning rate (default is 1e-4 I believe)\"}\\n',\n",
       " '{\\'prompt\\': \"O wait it works when I don\\'t specify learning rate (default is 1e-4 I believe)\", \\'response\\': \\'sheesh ok\\'}\\n',\n",
       " \"{'prompt': 'I‚Äôm trying drop-rate 0.1 for 10 epochs', 'response': 'so it does work, but I have to keep learning rate below 1.5e-4 (1.5e-4 does not work, but 1.1e-4 works, still testing more)'}\\n\",\n",
       " \"{'prompt': 'I think the culprit is the drop rate..? üòÇ', 'response': '!!!!!!!!!!'}\\n\",\n",
       " \"{'prompt': '!!!!!!!!!!', 'response': 'OK'}\\n\",\n",
       " \"{'prompt': 'Thanks!!', 'response': 'yuh please put your findings here'}\\n\",\n",
       " \"{'prompt': 'yuh please put your findings here', 'response': 'updated ester1 -- updated default arguments to reflect the current best result'}\\n\",\n",
       " \"{'prompt': 'updated ester1 -- updated default arguments to reflect the current best result', 'response': 'I updated the print statement to be more clear too, so would you guys like to make a branch of ester1 and work on that? (ignore ester2)'}\\n\",\n",
       " \"{'prompt': 'Results can be found in this doc here', 'response': 'Are you all free to call at 3pm again this Friday to check-in on PA4?'}\\n\",\n",
       " '{\\'prompt\\': \\'Yes!\\', \\'response\\': \"I\\'ll try method 3 today\"}\\n',\n",
       " \"{'prompt': 'just copied your updated code @Kong Xian Ying', 'response': 'bump! please react if you call on Friday at 3pm, and lmk if we need to find another time'}\\n\",\n",
       " \"{'prompt': 'bump! please react if you call on Friday at 3pm, and lmk if we need to find another time', 'response': '@Jonathan Cheung?'}\\n\",\n",
       " '{\\'prompt\\': \\'@Jonathan Cheung?\\', \\'response\\': \\'Also I don\\\\\\'t wanna see you guys lose points for not coding! I think it\\\\\\'s worth arguing that you guys weren\\\\\\'t able to code for PA3 because of unfixable cuDNN version incompatibility issue, and that you will make sure to code for PA4. This is what our prof wrote for PA3 feedback: \"Hi folks - Based on your description of who did what, it sounds like Jeremy just did hyperparameter tuning, Jonathan only helped with analysis, and Samuel just helped write the report. You know that we expect everyone to code! Please argue with me in a regrade request as to why I should not give Samuel 50% of the points, Jonathan 70%, and Jeremy 80% of the points. In general, I highly recommend using pair or triple programming, trading of who is the driver to avoid this in the future.\"\\'}\\n',\n",
       " '{\\'prompt\\': \\'oh ummmm that‚Äôs kind awkward i kinda did the baseline too I just didn‚Äôt want to write that in my contributions cuz my work for that didn‚Äôt show up in the report\\', \\'response\\': \"you can argue for that! It\\'s ok if it didn\\'t show up in the report\"}\\n',\n",
       " \"{'prompt': 'uhh shuld i directly send an email or should we create a doc or smth and then send everything all at once', 'response': 'We just need to send one reply through Gradescope regrade request. Would you like to start a doc?'}\\n\",\n",
       " '{\\'prompt\\': \"btw we still need help with Part 5! Will anyone be working on that? I\\'m starting on it but can\\'t guarantee I\\'ll figure it out\", \\'response\\': \"It\\'s also helpful if anyone wants to try Stochastic Weight Averaging (SWA) and Frequent Evaluation for Part 4\"}\\n',\n",
       " \"{'prompt': 'Reinitializing the last layer resulted in slight improvement from baseline', 'response': 'my code has been pushed to ester1'}\\n\",\n",
       " '{\\'prompt\\': \\'my code has been pushed to ester1\\', \\'response\\': \"@Jeremy Tow @Jonathan Cheung @Samuel Chu feel free to  build on top of ester1; it has Jeremy\\'s, Winfrey\\'s, and my most updated code\"}\\n',\n",
       " '{\\'prompt\\': \\'okay i‚Äôll add to it\\', \\'response\\': \"I\\'m updating ester1 rn to better test different custom fine tuning\"}\\n',\n",
       " '{\\'prompt\\': \"I\\'m updating ester1 rn to better test different custom fine tuning\", \\'response\\': \\'^ done pushing to ester1\\'}\\n',\n",
       " \"{'prompt': '^ done pushing to ester1', 'response': '@Kong Xian Ying would you like to try dropout 0.15 instead of 0.1? it seemed to perform better for me'}\\n\",\n",
       " '{\\'prompt\\': \\'Sure\\', \\'response\\': \"btw just updated ester1 again! Now it\\'s a lot easier to implement different custom models in models.py and use class inheritance to combine methods\"}\\n',\n",
       " '{\\'prompt\\': \"btw just updated ester1 again! Now it\\'s a lot easier to implement different custom models in models.py and use class inheritance to combine methods\", \\'response\\': \"For now I\\'m gonna test everything with learning rate 1e-5 and dropout 0.15\"}\\n',\n",
       " '{\\'prompt\\': \"For now I\\'m gonna test everything with learning rate 1e-5 and dropout 0.15\", \\'response\\': \\'All results are recorded here btw! \\'}\\n',\n",
       " \"{'prompt': 'Kong pinned a message.', 'response': 'Any progress on part 5?'}\\n\",\n",
       " \"{'prompt': 'I haven‚Äôt rlly looked but I can do part 5', 'response': 'We‚Äôre pretty good on part 4 since we‚Äôve done methods 1-3. Feel free to try methods 4-5 but we should aim to get part 5 done first!'}\\n\",\n",
       " \"{'prompt': 'Yupp agreed!!', 'response': 'Anyone going to OH tomorrow?'}\\n\",\n",
       " \"{'prompt': 'Nope I won‚Äôt be', 'response': 'I have a functional part 5'}\\n\",\n",
       " '{\\'prompt\\': \\'I have a functional part 5\\', \\'response\\': \\'there are 2 parts to the supcon_train: the first training loop uses SupCon loss to train the model to output features that group similar data points together; the second training loop uses cross entropy loss to train the classifier to output predictions based on \"features\" input\\'}\\n',\n",
       " '{\\'prompt\\': \\'there are 2 parts to the supcon_train: the first training loop uses SupCon loss to train the model to output features that group similar data points together; the second training loop uses cross entropy loss to train the classifier to output predictions based on \"features\" input\\', \\'response\\': \\'ester1 branch has the newest updates\\'}\\n',\n",
       " \"{'prompt': 'oh btw did we submit the regrade request yet and if not shuld i do it', 'response': 'feel free to try on your own or improve on mine!'}\\n\",\n",
       " \"{'prompt': 'feel free to try on your own or improve on mine!', 'response': 'I can check'}\\n\",\n",
       " \"{'prompt': 'tyty', 'response': 'not submitted yet'}\\n\",\n",
       " \"{'prompt': 'Can someone run it and see what the test accuracy is?', 'response': 'It‚Äôs only 50% if I set n-epochs to 10'}\\n\",\n",
       " \"{'prompt': 'It‚Äôs only 50% if I set n-epochs to 10', 'response': 'So we can try 20'}\\n\",\n",
       " \"{'prompt': 'just finsihed running with 20 epochs, test acc is 0.5047', 'response': 'lol so more epochs does not help'}\\n\",\n",
       " '{\\'prompt\\': \\'lol so more epochs does not help\\', \\'response\\': \"I\\'ve implemented util.plot\"}\\n',\n",
       " '{\\'prompt\\': \"I\\'ve implemented util.plot\", \\'response\\': \\'YOOOOOOOOOOOOOOOO\\'}\\n',\n",
       " \"{'prompt': 'YOOOOOOOOOOOOOOOO', 'response': 'part 5 worked'}\\n\",\n",
       " \"{'prompt': 'part 5 worked', 'response': 'I think!'}\\n\",\n",
       " \"{'prompt': 'ur actually amazing', 'response': 'the loss has been updated to reflect mean loss, not total'}\\n\",\n",
       " \"{'prompt': 'the loss has been updated to reflect mean loss, not total', 'response': 'I just needed to make the learning rate much higher for the classifier'}\\n\",\n",
       " '{\\'prompt\\': \\'I just needed to make the learning rate much higher for the classifier\\', \\'response\\': \"high key it\\'s scary when things work\"}\\n',\n",
       " '{\\'prompt\\': \"high key it\\'s scary when things work\", \\'response\\': \\'better sanity check\\'}\\n',\n",
       " \"{'prompt': 'but it actually putting in so much work it‚Äôs crazy tysm', 'response': '@Jonathan Cheung @Kong Xian Ying would you guys be down to set up the report by tonight? '}\\n\",\n",
       " \"{'prompt': '@Jonathan Cheung @Kong Xian Ying would you guys be down to set up the report by tonight? ', 'response': 'or by tomorrow before 3pm'}\\n\",\n",
       " \"{'prompt': '^ to get 88.23% test accuracy', 'response': 'Imma run the models overnight to get these plots'}\\n\",\n",
       " '{\\'prompt\\': \"i\\'ll run it, i need a break anyways\", \\'response\\': \\'You can get the code from ester1! Lmk if it works \\'}\\n',\n",
       " \"{'prompt': 'You can get the code from ester1! Lmk if it works ', 'response': 'Made fixes to the plots and corresponding stuff'}\\n\",\n",
       " \"{'prompt': 'Made fixes to the plots and corresponding stuff', 'response': '@Jeremy Tow how‚Äôs SimCLR?'}\\n\",\n",
       " \"{'prompt': 'I‚Äôll run it', 'response': 'accuracy is pretty low for SimCLR, might need some tuning'}\\n\",\n",
       " \"{'prompt': 'Yeah', 'response': 'Has anyone read the SimCLR slides?'}\\n\",\n",
       " \"{'prompt': 'i‚Äôm in class rn but i‚Äôll update after i fj ish', 'response': 'Btw If you run out of storage, you can comment out the part of the code in supcon_train in main.py that saves the model weight!'}\\n\",\n",
       " \"{'prompt': 'Btw If you run out of storage, you can comment out the part of the code in supcon_train in main.py that saves the model weight!', 'response': 'It gives you the option to only train the classifier using a pre-trained contrastive model with this parameter   --contrastive-model-path SupCon_ep10_dropout0.4_lr0.0001_temp0.07_baseTemp0.07.pth'}\\n\",\n",
       " \"{'prompt': 'It gives you the option to only train the classifier using a pre-trained contrastive model with this parameter   --contrastive-model-path SupCon_ep10_dropout0.4_lr0.0001_temp0.07_baseTemp0.07.pth', 'response': 'Replacing the .pth portion with whatever the model weight name is in the model/ folder'}\\n\",\n",
       " \"{'prompt': 'Replacing the .pth portion with whatever the model weight name is in the model/ folder', 'response': 'Reminder we are calling at 3!'}\\n\",\n",
       " \"{'prompt': 'üëÄ', 'response': 'Question is, how does BERT know which label is the most popular : 0'}\\n\",\n",
       " \"{'prompt': 'are you that you aren‚Äôt training it at all', 'response': 'I just added train loss to plot names! Please git pull ester1 if you plan to use that code'}\\n\",\n",
       " \"{'prompt': 'I just added train loss to plot names! Please git pull ester1 if you plan to use that code', 'response': 'just made another push to use the best model (best val accuracy) instead of final model'}\\n\",\n",
       " \"{'prompt': 'So essentially it goes down to what the linear classifier is predicting?', 'response': 'All the plots for report are on GitHub in the plots/good/ folder'}\\n\",\n",
       " \"{'prompt': 'All the plots for report are on GitHub in the plots/good/ folder', 'response': 'Except for SimCLR'}\\n\",\n",
       " \"{'prompt': 'Except for SimCLR', 'response': 'lowering temperature and increasing input-classifier-dim seems to increase test accuracy for SimCLR!'}\\n\",\n",
       " \"{'prompt': 'How did you get rid of the cuda memory error, I‚Äôm been getting ~50% by playing with batch sizes', 'response': 'You‚Äôll need to set the batch size to something less than 1000'}\\n\",\n",
       " \"{'prompt': 'You‚Äôll need to set the batch size to something less than 1000', 'response': '512 still works'}\\n\",\n",
       " \"{'prompt': '512 still works', 'response': 'Bigger batch size will cause cuda out of memory error'}\\n\",\n",
       " \"{'prompt': 'Bigger batch size will cause cuda out of memory error', 'response': 'If you use node 30 on DSMLP you get a GPU with more GB of memory'}\\n\",\n",
       " \"{'prompt': 'If you use node 30 on DSMLP you get a GPU with more GB of memory', 'response': 'oo highest I got so far is 67.48%'}\\n\",\n",
       " \"{'prompt': 'SimCSE is using dropout rate of 0.1, and from the doc I see that Ester has tried that', 'response': 'side note! rn we use the cosine annealing scheduler for the classifier so the learning rate will decrease over the epochs'}\\n\",\n",
       " \"{'prompt': 'side note! rn we use the cosine annealing scheduler for the classifier so the learning rate will decrease over the epochs', 'response': 'we should still try different starting learning rates tho!'}\\n\",\n",
       " \"{'prompt': 'we should still try different starting learning rates tho!', 'response': 'or even different schedulers'}\\n\",\n",
       " '{\\'prompt\\': \\'paper 1: \\', \\'response\\': \"I\\'m trying SGD optimizer for the contrastive training loop\"}\\n',\n",
       " '{\\'prompt\\': \\'@Ester Tsai just confirming, we are going with custom 1&3 right?\\', \\'response\\': \"yeah let\\'s do that if you guys are down!\"}\\n',\n",
       " '{\\'prompt\\': \\'yeah sounds good to me\\', \\'response\\': \"@Jeremy Tow@Jonathan Cheung@Samuel Chu How\\'s progress?\"}\\n',\n",
       " \"{'prompt': '@Ester Tsai do you also get very low contrastive loss? like 0.0', 'response': 'yes'}\\n\",\n",
       " \"{'prompt': 'ok so we took care of item2. I will work on item1', 'response': 'I see! That makes so much sense!'}\\n\",\n",
       " '{\\'prompt\\': \"and classifier input dim has to be 768, since it is taking the encoder\\'s embeddings when we are doing SimCLR\", \\'response\\': \\'Yep that‚Äôs right\\'}\\n',\n",
       " '{\\'prompt\\': \\'hahaha high five, been there, done that, i was debugging for like 2 hours\\', \\'response\\': \"that\\'s great!\"}\\n',\n",
       " '{\\'prompt\\': \"i\\'m mostly testing 2 hyperparameters now 1. supcon-linear-head-dim 2. hidden-dim (of the classifier, because we have been using the default 10, so its going 768 -> 10 -> 60)\", \\'response\\': \\'OHH REALLY\\'}\\n',\n",
       " \"{'prompt': 'OHH REALLY', 'response': 'I thought hidden dim isn‚Äôt used for some reason'}\\n\",\n",
       " \"{'prompt': 'It‚Äôs a lot of args going on üòÇ', 'response': 'O DANG you‚Äôre right'}\\n\",\n",
       " \"{'prompt': 'O DANG you‚Äôre right', 'response': 'It‚Äôs being used by Classifier class'}\\n\",\n",
       " \"{'prompt': 'It‚Äôs being used by Classifier class', 'response': 'OOPS'}\\n\",\n",
       " \"{'prompt': 'Yeah no worries, I only realized that very much later, the 71% just now is with hidden dim 128 (for Classifier), and sup con linear head dim 128 (for SupConModel)', 'response': 'the linear head dim can go pretty high, like 3000'}\\n\",\n",
       " \"{'prompt': 'True true!', 'response': 'btw I forgot to share earlier! but if you want the DSMLP GPU to have a high time limit (24 hours instead of 6 hours), you can run these commands in the terminal before your GPU launch scrupt'}\\n\",\n",
       " \"{'prompt': 'btw I forgot to share earlier! but if you want the DSMLP GPU to have a high time limit (24 hours instead of 6 hours), you can run these commands in the terminal before your GPU launch scrupt', 'response': 'export K8S_PRIORITY_CLASS_NAME=normal export K8S_TIMEOUT_SECONDS=86400  export K8S_BYPASS_TIMEOUT_LIMIT=yes'}\\n\",\n",
       " \"{'prompt': 'Let me finish the training', 'response': 'yuh please send the command line'}\\n\",\n",
       " \"{'prompt': 'though idk if its sketch', 'response': 'what did you modify it to?'}\\n\",\n",
       " \"{'prompt': 'what did you modify it to?', 'response': 'args.embed_dim will have to be 768'}\\n\",\n",
       " '{\\'prompt\\': \\'Niceeee we can take a look at your code and see if it‚Äôs just naming differences which we can always adjust later\\', \\'response\\': \"btw for CrossEntropyLoss(reduction=\\'sum\\'), it makes the cross entropy loss return the sum instead of the mean for each batch, so we can divide by total number of data points and get the mean over the whole set\"}\\n',\n",
       " '{\\'prompt\\': \"btw for CrossEntropyLoss(reduction=\\'sum\\'), it makes the cross entropy loss return the sum instead of the mean for each batch, so we can divide by total number of data points and get the mean over the whole set\", \\'response\\': \"if we removed the reduction=\\'sum\\', it\\'ll be hard to calculate loss consistently\"}\\n',\n",
       " '{\\'prompt\\': \\'OHHHH Okok we should put it back then, Soz didn‚Äôt think that far\\', \\'response\\': \"haha I tried 10000 once and might\\'ve gotten CUDA out of memeory\"}\\n',\n",
       " '{\\'prompt\\': \\'Pushing the limits we like\\', \\'response\\': \"OOOO I got good results similar to jcheung\\'s\"}\\n',\n",
       " \"{'prompt': 'Yooooooooooo', 'response': 'the epochs can be lower and probably still perform the same'}\\n\",\n",
       " \"{'prompt': 'the epochs can be lower and probably still perform the same', 'response': '82.92%'}\\n\",\n",
       " \"{'prompt': 'i made my branch but it looks like ester branch from a week ago, im a brnach noob does anyone know how to update it', 'response': 'plot looks reasonable'}\\n\",\n",
       " \"{'prompt': 'Create a pull request from Ester to ur branch', 'response': 'YUH thanks to your important fixes XD'}\\n\",\n",
       " \"{'prompt': 'Well, there‚Äôs also brute force, download the py files and upload it, almost guaranteed to work always ü§£ when there‚Äôs too many merge conflicts I just brute force hahaha', 'response': 'o wait ester 1 is not updated'}\\n\",\n",
       " \"{'prompt': 'o wait ester 1 is not updated', 'response': 'let me do that rn'}\\n\",\n",
       " '{\\'prompt\\': \"I had merge conflict so I couldn\\'t properly push my code lolol I ended up copy pasting my changes into ester1\", \\'response\\': \"I\\'m rerunning all the other models after the code fix, but I\\'m down to just use what we already have on the report to save time changing everything out\"}\\n',\n",
       " \"{'prompt': 'I put it in the figures folder for Overleaf but feel free to replace it if something better comes up', 'response': '82.92% test acc'}\\n\",\n",
       " \"{'prompt': '82.92% test acc', 'response': 'New result for supcon loss: 88.9% at epoch 3'}\\n\",\n",
       " \"{'prompt': 'Noiceeee', 'response': '83.15% SimCLR'}\\n\",\n",
       " \"{'prompt': 'I can fill in the values in the chart and in the abstract then', 'response': 'I have a better plot possibly'}\\n\",\n",
       " \"{'prompt': 'I have a better plot possibly', 'response': 'Let me update it on overleaf rn'}\\n\",\n",
       " \"{'prompt': 'Let me update it on overleaf rn', 'response': 'After fixing our code, the baseline is mega good'}\\n\",\n",
       " \"{'prompt': 'After fixing our code, the baseline is mega good', 'response': '89%'}\\n\",\n",
       " \"{'prompt': '89%', 'response': 'No other model gets better than baseline'}\\n\",\n",
       " \"{'prompt': 'No other model gets better than baseline', 'response': 'Should we keep our old set of plots to save time or hyperparameter tune all the models until they are optimized?'}\\n\",\n",
       " \"{'prompt': 'I‚Äôm fine with keeping out old set', 'response': 'The Discussion section still has many blanks. Please help fill it out! I might go to OH at 5pm to ask questions'}\\n\",\n",
       " '{\\'prompt\\': \\'The Discussion section still has many blanks. Please help fill it out! I might go to OH at 5pm to ask questions\\', \\'response\\': \"I\\'ve at least updated the SupCon and SimCLR plots, but not yet the other ones. I\\'m also down to keep the other ones the same since we\\'ve already written a lot about them. But I also have the new plots after the code fix available if we want to swap the old ones out.\"}\\n',\n",
       " \"{'prompt': 'is the acc for the fine tuned models very different from the table we have right now?', 'response': 'all are around 88%'}\\n\",\n",
       " \"{'prompt': 'all are around 88%', 'response': 'not that different'}\\n\",\n",
       " \"{'prompt': 'if we want, we can embrace the research mindset by reporting the latest results', 'response': 'TRUE'}\\n\",\n",
       " \"{'prompt': 'do you need help with running any more or you have everything we need already?', 'response': 'I still need help with hyperparameter tuning'}\\n\",\n",
       " \"{'prompt': 'I still need help with hyperparameter tuning', 'response': 'for custom 3 and custom1and3'}\\n\",\n",
       " \"{'prompt': 'just saw that you took SimCLR on the report! thank you thank you', 'response': 'done with baseline and custom1'}\\n\",\n",
       " '{\\'prompt\\': \\'These are the new plots that we can use, minus custom3 and custom1and3\\', \\'response\\': \"I\\'ll put them on Overleaf\"}\\n',\n",
       " \"{'prompt': 'oooh so custom1 itself is slightly tiny bit better than baseline', 'response': 'yeye just got the newest best result from hyperparameter tuning'}\\n\",\n",
       " '{\\'prompt\\': \\'^these are the current best custom1 and custom1and3\\', \\'response\\': \"to summarize: I\\'ve updated the corresponding report parts for baseline, custom1, and SimCLR. Still waiting on some more SupCon to run for better consistency. Need help on hyperparameter tuning and updating the report for custom3 and custom1and3\"}\\n',\n",
       " \"{'prompt': 'Do we hv numbers for technique 2 and the combined ones or is it being rerun', 'response': 'These are the current best stats but it is being rerun'}\\n\",\n",
       " \"{'prompt': 'Also did using the losses rather than the 2 techniques trading faster or was there no significant difference?', 'response': 'Wdym by using the losses?'}\\n\",\n",
       " \"{'prompt': 'Like do those model train faster in any significant way?', 'response': 'I didn‚Äôt keep track of that hmm'}\\n\",\n",
       " \"{'prompt': 'I didn‚Äôt keep track of that hmm', 'response': 'Feel free to track it'}\\n\",\n",
       " \"{'prompt': 'Feel free to track it', 'response': 'Ester1 should be updated'}\\n\",\n",
       " \"{'prompt': 'like do we need to commnet on each graph or should this be put in the technique 2 section', 'response': 'We need to comment on each plot right'}\\n\",\n",
       " \"{'prompt': 'we are meeting at 7pm this Wednesday for final project, right?', 'response': 'Yes'}\\n\",\n",
       " \"{'prompt': 'I moved the scheduler step out to the epoch loop, and the best val acc is above this here at epoch 5', 'response': 'Ohhhhhh'}\\n\",\n",
       " \"{'prompt': 'Ohhhhhh', 'response': 'I fixed it once but it didn‚Äôt save, so that might be why I missed it when I fixed a second time'}\\n\",\n",
       " \"{'prompt': 'I fixed it once but it didn‚Äôt save, so that might be why I missed it when I fixed a second time', 'response': 'I got good supcon plot'}\\n\",\n",
       " \"{'prompt': 'Ooohh I seeee okkk thanks!', 'response': 'task=custom1and3 testAcc=0.8907 bestValAcc=0.8942 bestTrainAcc=0.9931 mean_loss=0.0194 ep=20 batch_s=16 drop=0.2 LR=5e-06 hid=128'}\\n\",\n",
       " \"{'prompt': 'task=custom1and3 testAcc=0.8907 bestValAcc=0.8942 bestTrainAcc=0.9931 mean_loss=0.0194 ep=20 batch_s=16 drop=0.2 LR=5e-06 hid=128', 'response': 'custom1and3 just slightly worse than baseline'}\\n\",\n",
       " \"{'prompt': 'Is the scheduler step in the epoch loop when u ran them? Just curious if u have changed it locally', 'response': 'oh right!'}\\n\",\n",
       " \"{'prompt': 'oh right!', 'response': 'need to rerun'}\\n\",\n",
       " \"{'prompt': 'üò≠üôåüèº', 'response': 'the 5pm TA  is very confused'}\\n\",\n",
       " \"{'prompt': 'the 5pm TA  is very confused', 'response': 'just quit his OH lol'}\\n\",\n",
       " \"{'prompt': 'which exact ones need reruns?', 'response': 'all the custom ones'}\\n\",\n",
       " \"{'prompt': 'all the custom ones', 'response': 'I updated ester1 to fix the scheduler.step'}\\n\",\n",
       " \"{'prompt': 'these two?', 'response': 'yes, and the combined'}\\n\",\n",
       " \"{'prompt': 'okok i will run those', 'response': 'custom1, custom3, and custom1and3'}\\n\",\n",
       " \"{'prompt': 'Ester has a chunk of command lines written in the results doc, you can get some from there and modify as you would like to', 'response': 'task=custom1and3 testAcc=0.8944 bestValAcc=0.8952 bestTrainAcc=0.9997 mean_loss=0.0036 ep=20 batch_s=16 drop=0.2 LR=5e-06 hid=1000'}\\n\",\n",
       " \"{'prompt': 'task=custom1and3 testAcc=0.8944 bestValAcc=0.8952 bestTrainAcc=0.9997 mean_loss=0.0036 ep=20 batch_s=16 drop=0.2 LR=5e-06 hid=1000', 'response': 'wow lol that little bug fix solved many things'}\\n\",\n",
       " \"{'prompt': 'i see your log of code that you will run at 520pm, is there any block that is not in the queue now?', 'response': 'SupCon loss is currently worse than the custom ones'}\\n\",\n",
       " '{\\'prompt\\': \\'SupCon loss is currently worse than the custom ones\\', \\'response\\': \"I also haven\\'t gotten to run any custom1 and custom3 commands\"}\\n',\n",
       " '{\\'prompt\\': \\'ok ill do custom1\\', \\'response\\': \"yuh yuh I\\'m keeping hidden-dim=1000 for all of them\"}\\n',\n",
       " '{\\'prompt\\': \"yuh yuh I\\'m keeping hidden-dim=1000 for all of them\", \\'response\\': \\'mostly play around with learning rate and drop rate, but 0.2 seems to be the best?\\'}\\n',\n",
       " \"{'prompt': 'mostly play around with learning rate and drop rate, but 0.2 seems to be the best?', 'response': '0.2 drop rate*'}\\n\",\n",
       " \"{'prompt': 'okk!', 'response': 'task=custom1 testAcc=0.8914 bestValAcc=0.8957 bestTrainAcc=0.9997 mean_loss=0.0022 ep=19 batch_s=16 drop=0.2 LR=7e-06 hid=1000'}\\n\",\n",
       " \"{'prompt': 'task=custom1 testAcc=0.8914 bestValAcc=0.8957 bestTrainAcc=0.9997 mean_loss=0.0022 ep=19 batch_s=16 drop=0.2 LR=7e-06 hid=1000', 'response': '^ just got this! lmk if you get somehting higher : D'}\\n\",\n",
       " \"{'prompt': '^ just got this! lmk if you get somehting higher : D', 'response': 'I got above 89% for all custom models'}\\n\",\n",
       " '{\\'prompt\\': \"^ I\\'m assuming you meant SimCLR\", \\'response\\': \\'The best is 89.14%, while custom1and3 is 89.51%\\'}\\n',\n",
       " \"{'prompt': 'The best is 89.14%, while custom1and3 is 89.51%', 'response': 'I‚Äôm ok with calling it good enough haja'}\\n\",\n",
       " \"{'prompt': 'yes agreed', 'response': 'Actually I meant SupCon!'}\\n\",\n",
       " \"{'prompt': 'oooo', 'response': 'SimCLR is 83.15'}\\n\",\n",
       " \"{'prompt': 'SimCLR is 83.15', 'response': 'And it‚Äôs alright since it‚Äôs above 80%'}\\n\",\n",
       " \"{'prompt': 'yeah', 'response': 'All the plots had been updated!'}\\n\",\n",
       " \"{'prompt': 'ok i will do supcon now', 'response': 'Do you mean running more of supcon or writing about it in report?'}\\n\",\n",
       " \"{'prompt': 'Also abstract and intro r up to date', 'response': 'The table and plot are updated!'}\\n\",\n",
       " \"{'prompt': 'Oh wait I misread the graph lol', 'response': 'Does anyone know what SimCLR stands for lol'}\\n\",\n",
       " \"{'prompt': 'ok I edited the introduction + expanded on the discussion section and finished the related works section', 'response': 'Wow our Related Works section is stacked'}\\n\",\n",
       " \"{'prompt': 'Wow our Related Works section is stacked', 'response': 'love that'}\\n\",\n",
       " \"{'prompt': 'would you like to merge your branch to main @Ester Tsai?', 'response': 'ok!'}\\n\",\n",
       " '{\\'prompt\\': \\'ok!\\', \\'response\\': \"I submitted code, but it\\'d be helpful if someone git clones the main branch to check if everything works smoothly\"}\\n',\n",
       " '{\\'prompt\\': \"I submitted code, but it\\'d be helpful if someone git clones the main branch to check if everything works smoothly\", \\'response\\': \\'haha I dropped in a place for most Piazza posts\\'}\\n',\n",
       " \"{'prompt': 'dang both y‚Äôall are on the leaderboard üòÇ', 'response': '70 contributions is crazy'}\\n\",\n",
       " \"{'prompt': '70 contributions is crazy', 'response': '8 per week'}\\n\",\n",
       " \"{'prompt': '8 per week', 'response': 'I asked this question on Piazza:'}\\n\",\n",
       " \"{'prompt': '@Samuel Chu if the TA answers it, would you like to add whatever we are missing to the report?', 'response': 'I also asked:'}\\n\",\n",
       " \"{'prompt': 'built different', 'response': 'can you all check on your end what your test accuracy is before training?'}\\n\",\n",
       " \"{'prompt': 'can you all check on your end what your test accuracy is before training?', 'response': '@Kong Xian Ying @Jeremy Tow @Jonathan Cheung @Samuel Chu ?'}\\n\",\n",
       " \"{'prompt': '@Kong Xian Ying @Jeremy Tow @Jonathan Cheung @Samuel Chu ?', 'response': 'send in the chat asap haha so we can see if my higher number is just due to random chance'}\\n\",\n",
       " \"{'prompt': 'send in the chat asap haha so we can see if my higher number is just due to random chance', 'response': 'maybe try a different seed too'}\\n\",\n",
       " \"{'prompt': 'Jeremy what‚Äôs the line you‚Äôre running', 'response': 'Doesn‚Äôt matter, just run baseline ü§ù'}\\n\",\n",
       " \"{'prompt': 'ok wait i got a 1 percent lol', 'response': 'What matters is that run_eval comes before baseline_train'}\\n\",\n",
       " \"{'prompt': 'we can prob do some statistical analysis to see how probable that is?', 'response': 'Wow is mine just too cracked of a seed, somehow the two most popular labels happen to be guessed over and over'}\\n\",\n",
       " \"{'prompt': 'I think my code broke or something my baseline has gets acc of 0.8662', 'response': 'How about Before you run training?'}\\n\",\n",
       " \"{'prompt': 'Oh lol ok we can go with Jonathan‚Äôs number', 'response': 'For the report'}\\n\",\n",
       " \"{'prompt': 'For the report', 'response': 'Could someone update the table and the Discussion Q2?'}\\n\",\n",
       " \"{'prompt': 'Could someone update the table and the Discussion Q2?', 'response': 'This is so goofy'}\\n\",\n",
       " '{\\'prompt\\': \"@Kong Xian Ying@Ester Tsai for the photo focal length idea we talked about earlier in person: 24-30 mm: 3k 30-40 mm: 6k 40-55 mm: 2k 55-70 mm: 6k > 70 mm: 400  This is just the number of photos taken with my current camera so I have a lot more but they\\'re harder to find stats on rn. I think it might be better to pull photos from the internet so there is more variety, and maybe supplement that data with mine, if we do go through with this idea  link to github repo: \", \\'response\\': \"oo Jeremy\\'s idea is super cool! Here is another option that seems feasible: train an LSTM to generate text that resembles how we talk in this Messenger chat\"}\\n',\n",
       " \"{'prompt': 'we could use gpt model too maybe', 'response': 'I can download all of my chat data for a specific time range. Every chat is a different file'}\\n\",\n",
       " \"{'prompt': 'Article that we can refer to for this task: ', 'response': 'typo here lol'}\\n\",\n",
       " \"{'prompt': 'typo here lol', 'response': 'I can fix and resubmit'}\\n\",\n",
       " \"{'prompt': 'Wow this is promising', 'response': '1 epoch and only 10 sequences of 30 tokens haha'}\\n\",\n",
       " \"{'prompt': 'ü§´üßè', 'response': 'keep sending texts and we will have better data üò≥üò≥'}\\n\",\n",
       " '{\\'prompt\\': \\'üòÇ\\', \\'response\\': \"Can one of you guys download your facebook messenger data and upload the json file for this groupchat to the github repo? I can\\'t download the 3 month one for some reason\"}\\n']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'Ester'\n",
    "create_individual_data(groupchat, name, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4e0c9",
   "metadata": {},
   "source": [
    "# Winfrey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f227e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"{'prompt': 'PA2!', 'response': 'Thanks Ester!'}\\n\",\n",
       " \"{'prompt': 'Ester named the group CSE 151B PA2.', 'response': 'What should our group name be üòé Ester and I used transformers for PA1'}\\n\",\n",
       " \"{'prompt': 'Or name it according to the PA content? Like Convolution? \\\\U0001fae0', 'response': 'Okie dokie'}\\n\",\n",
       " \"{'prompt': 'Okie dokie', 'response': 'I‚Äôll make one convolution'}\\n\",\n",
       " \"{'prompt': 'I‚Äôll make one convolution', 'response': 'Created, go join ‚Äúconvolution‚Äù'}\\n\",\n",
       " \"{'prompt': 'Created, go join ‚Äúconvolution‚Äù', 'response': 'Also could you send me your email so I could add y‚Äôall as collaborator?'}\\n\",\n",
       " '{\\'prompt\\': \"Are you guys able to access a GPU of any sort? Datahub is still down and I can\\'t get Google Colab to work either\", \\'response\\': \\'Thinking if we can use our data science capstone‚Äôs ü§î\\'}\\n',\n",
       " \"{'prompt': 'I guess so!', 'response': 'Yeahh'}\\n\",\n",
       " \"{'prompt': 'So I can only work on one project at a time', 'response': 'OHH I see I seeeee'}\\n\",\n",
       " \"{'prompt': 'OHH I see I seeeee', 'response': 'Does it say something like CUDA blah blah exceeded or killed when u run two?'}\\n\",\n",
       " '{\\'prompt\\': \\'It says \"Request exceeds limit of 1 GPUs\" hahaa\\', \\'response\\': \\'OHHH okay I was wondering if u run it on the same session\\'}\\n',\n",
       " \"{'prompt': 'OHHH okay I was wondering if u run it on the same session', 'response': 'But better not risk terminating the project you are running at the moment'}\\n\",\n",
       " \"{'prompt': 'But better not risk terminating the project you are running at the moment', 'response': 'In case it gets killed and u gotta restart the kernel üòÖ'}\\n\",\n",
       " \"{'prompt': 'I can just clear some space in my private folder and git clone the project there', 'response': 'Yeah sure that‚Äôs what I did'}\\n\",\n",
       " \"{'prompt': 'Yeah sure that‚Äôs what I did', 'response': 'But yeah I ran into once when I was running two projects (ie notebooks) and it got too much to handle, and both just terminated - painful!! ü•≤'}\\n\",\n",
       " \"{'prompt': 'But yeah I ran into once when I was running two projects (ie notebooks) and it got too much to handle, and both just terminated - painful!! ü•≤', 'response': '@Ester Tsai are you at part 2 for the programming part? The data loading section'}\\n\",\n",
       " \"{'prompt': '(I don‚Äôt know if my baseline model works, will likely need to debug!', 'response': 'Okkk! Sounds good, so have u implemented the test and val data in voc.py?'}\\n\",\n",
       " \"{'prompt': 'Yes', 'response': 'Ok could you push it to your branch? I don‚Äôt think I see it there just now'}\\n\",\n",
       " \"{'prompt': 'Yeah! wanna focus on the models so we don‚Äôt duplicate efforts on writing basic utils - which will be largely the same anyway', 'response': 'Yup thanks!'}\\n\",\n",
       " \"{'prompt': 'Almost done with the baseline model! Gonna keep testing it until test IoU is consistently around 0.05', 'response': 'Okie!! Thanks!'}\\n\",\n",
       " \"{'prompt': 'Okie!! Thanks!', 'response': 'I got started with building UNet (5c)  ^just a relatively independent task to work on in the mean time. But I‚Äôll build off what you have Ester, when the baseline is ready! Thank you Ester!!'}\\n\",\n",
       " \"{'prompt': 'I got started with building UNet (5c)  ^just a relatively independent task to work on in the mean time. But I‚Äôll build off what you have Ester, when the baseline is ready! Thank you Ester!!', 'response': 'Hmm not sure if this will make a difference and/or you are probably doing it already, might want to try using AdamW optimizer in train.py'}\\n\",\n",
       " \"{'prompt': 'Hmm not sure if this will make a difference and/or you are probably doing it already, might want to try using AdamW optimizer in train.py', 'response': 'I see we are using SGD optimizer currently'}\\n\",\n",
       " '{\\'prompt\\': \\'You can test the model by running \"python train.py\" in the terminal\\', \\'response\\': \\'Yayyy thank you thank youuu!!\\'}\\n',\n",
       " '{\\'prompt\\': \"(this is not pushed to the master branch since I\\'ll just experiment on my own branch first!\", \\'response\\': \\'Yup yupp!\\'}\\n',\n",
       " \"{'prompt': 'Yup yupp!', 'response': 'I‚Äôm also looking at augmentation options in torchvision (part of UNet requires this)'}\\n\",\n",
       " \"{'prompt': 'I‚Äôm also looking at augmentation options in torchvision (part of UNet requires this)', 'response': 'Are you also looking at torchvision?'}\\n\",\n",
       " \"{'prompt': 'Are you also looking at torchvision?', 'response': 'Just curious if there‚Äôs other places that offer image augmentation methods'}\\n\",\n",
       " \"{'prompt': 'TA gave this link as an example: ', 'response': 'I see I see! Okk looks like it‚Äôs the same root library! Thanks for confirming haha'}\\n\",\n",
       " \"{'prompt': 'GitHub high key confusing sometimes', 'response': 'AGREE'}\\n\",\n",
       " \"{'prompt': 'AGREE', 'response': 'What error are you getting tho?'}\\n\",\n",
       " \"{'prompt': 'What error are you getting tho?', 'response': 'Is it related to authentication?'}\\n\",\n",
       " \"{'prompt': 'I‚Äôm guessing it‚Äôs bc the pth model file is large and bc there‚Äôs some byte error', 'response': 'Interesting.. so u get an error when running the model?'}\\n\",\n",
       " \"{'prompt': 'Interesting.. so u get an error when running the model?', 'response': 'I haven‚Äôt tried running the model yet so'}\\n\",\n",
       " \"{'prompt': 'its because my wifi sucks', 'response': 'Wow good to know that wifi actually matters üòÖ'}\\n\",\n",
       " \"{'prompt': 'Wow good to know that wifi actually matters üòÖ', 'response': 'UNet is done, and together with a train_unet.py for that  train_unet is just a duplicate of train.py with minimal changes - if anyone would like to, feel free to combine the two, probably will have to create a TrainLoop object and initialize models given model name == FCN or UNet or more that we will explore. For now, the easiest is to have different train files for each model, which I believe is fine from the description of Part 5 in the assignment pdf  If everyone is okay, I can push to master before noon. Also, it‚Äôd be good to have a few more pairs of eyes to check the architecture'}\\n\",\n",
       " \"{'prompt': 'UNet is done, and together with a train_unet.py for that  train_unet is just a duplicate of train.py with minimal changes - if anyone would like to, feel free to combine the two, probably will have to create a TrainLoop object and initialize models given model name == FCN or UNet or more that we will explore. For now, the easiest is to have different train files for each model, which I believe is fine from the description of Part 5 in the assignment pdf  If everyone is okay, I can push to master before noon. Also, it‚Äôd be good to have a few more pairs of eyes to check the architecture', 'response': 'Yoo before the weekend comes, I am thinking if we want to sync a little on what we are currently doing and/or interested in doing? For the programming part, I believe part 4 and 5'}\\n\",\n",
       " \"{'prompt': 'Yoo before the weekend comes, I am thinking if we want to sync a little on what we are currently doing and/or interested in doing? For the programming part, I believe part 4 and 5', 'response': 'I haven‚Äôt started anything after UNet. But I‚Äôm open to working on 5b / part 4b image augmentations / part 4c imbalanced class'}\\n\",\n",
       " \"{'prompt': 'I‚Äôm down to work on those parts too, but I‚Äôm not available the rest of today and am currently trying to implement the other code myself to make sure I‚Äôm understanding it', 'response': 'Okkk let‚Äôs wait and see what Ester and Jeremy are working on/ plan to work on/ have a preference for'}\\n\",\n",
       " \"{'prompt': 'Okkk let‚Äôs wait and see what Ester and Jeremy are working on/ plan to work on/ have a preference for', 'response': 'Also, no pressure, just wanna make sure we don‚Äôt end up working on the exact same thing üòÇ'}\\n\",\n",
       " \"{'prompt': 'Also, no pressure, just wanna make sure we don‚Äôt end up working on the exact same thing üòÇ', 'response': 'Oh yeah did prof say anything about moving the MT date? Since it‚Äôs going to be on lunar new year'}\\n\",\n",
       " \"{'prompt': 'There was a piazza post that said we‚Äôre not moving the midterm I think', 'response': 'Yupp ok sounds good!! U did parts 1-3 this week and I‚Äôm very thankful for that!!'}\\n\",\n",
       " \"{'prompt': 'Yupp ok sounds good!! U did parts 1-3 this week and I‚Äôm very thankful for that!!', 'response': 'OOOH I seee'}\\n\",\n",
       " \"{'prompt': 'about the same for me i‚Äôm also open to working on any of it', 'response': 'Ok! I‚Äôll get started on part 4a later, I‚Äôll keep y‚Äôall updated on my progress over the weekend'}\\n\",\n",
       " \"{'prompt': 'Ok! I‚Äôll get started on part 4a later, I‚Äôll keep y‚Äôall updated on my progress over the weekend', 'response': 'doc for sharing results  '}\\n\",\n",
       " \"{'prompt': 'doc for sharing results  ', 'response': '@Ester Tsai quick question, for the optimizer, did you choose learning rate to be 0.001 or was it given?'}\\n\",\n",
       " \"{'prompt': 'I chose it', 'response': 'i see.. okay! with learning rate 0.1, the accuracy actually went up to 75% (but the assignment pdf said expected ~65%)  IoU is fine, ~0.05'}\\n\",\n",
       " \"{'prompt': 'I can make a post on piazza', 'response': 'just came across this section in the visualize notebook  I will have to take back my words earlier on combining the train files, it looks like it is recommended to have different train files for all subparts in part 4 & 5'}\\n\",\n",
       " \"{'prompt': 'Make sense!', 'response': '4b is done! If everyone is okay, I can push to master'}\\n\",\n",
       " \"{'prompt': 'Thanks!!', 'response': 'Kong pinned a message.'}\\n\",\n",
       " \"{'prompt': 'Kong pinned a message.', 'response': 'I will get back on pa2 tomorrow, is there anything that I can work on?'}\\n\",\n",
       " \"{'prompt': 'I can get on that too after capstone tomorrow! What is left that we need to work on?', 'response': 'From yesterday, I know we have 4c, 5a and 5b left'}\\n\",\n",
       " \"{'prompt': 'From yesterday, I know we have 4c, 5a and 5b left', 'response': 'I‚Äôll get started on 4c today'}\\n\",\n",
       " \"{'prompt': 'I‚Äôll get started on 4c today', 'response': 'Just did some research for 4c, I should be able to get 4c done today, will keep y‚Äôall posted on my progress'}\\n\",\n",
       " \"{'prompt': 'Thank you!', 'response': 'Pretty much done with 4c, just running a bunch of experiments, will let y‚Äôall know before I push'}\\n\",\n",
       " '{\\'prompt\\': \"I\\'ll start on the report\", \\'response\\': \\'Great! I‚Äôll join u on the report tomorrow!\\'}\\n',\n",
       " '{\\'prompt\\': \\'Ester voted for \"Discussion - Baseline\" and 1 other option in the poll.\\', \\'response\\': \\'Kong voted for \"Discussion - Improving on basline\" and 1 other option in the poll.\\'}\\n',\n",
       " '{\\'prompt\\': \\'Kong voted for \"Discussion - Improving on basline\" and 1 other option in the poll.\\', \\'response\\': \\'Kong voted for \"Methods - Improving on baseline\" and 2 other options in the poll.\\'}\\n',\n",
       " \"{'prompt': 'I can also do experimentation bc I‚Äôm doing 5a', 'response': 'Experimentation (report) I‚Äôll do 5c UNet'}\\n\",\n",
       " '{\\'prompt\\': \\'Ester voted for \"Abstract\" in the poll.\\', \\'response\\': \\'I will push 4c before 12am! Have been fine tuning the hyper parameters, finally got a satisfactory result üòÆüí®üò¥\\'}\\n',\n",
       " \"{'prompt': 'I filled in the part for Cosine Annealing for both methods and discussion', 'response': 'Yayyyy! Thanks!'}\\n\",\n",
       " \"{'prompt': 'Are you using your own code or Winfrey‚Äôs base code?', 'response': 'Did you get this from the line   Loss = criterion(outputs, labels) ?'}\\n\",\n",
       " \"{'prompt': 'i used my own code though it has similar sctructure to the 4 questions', 'response': 'I think it‚Äôs the'}\\n\",\n",
       " \"{'prompt': 'I think it‚Äôs the', 'response': '256 not matching 224'}\\n\",\n",
       " \"{'prompt': '256 not matching 224', 'response': 'Your output H and W should be 224'}\\n\",\n",
       " \"{'prompt': 'Your output H and W should be 224', 'response': 'Try changing padding'}\\n\",\n",
       " \"{'prompt': '@Winfrey Kong @Jeremy Tow @Jonathan Cheung Are you all free to call tomorrow at 9am to check in for PA2?', 'response': 'Yes!'}\\n\",\n",
       " \"{'prompt': 'I‚Äôll implement the training and validation plot for report Results section', 'response': 'Ok sure, thanks!'}\\n\",\n",
       " \"{'prompt': 'Ok sure, thanks!', 'response': 'Working on the rest of ‚Äúimproving the baseline‚Äù now'}\\n\",\n",
       " \"{'prompt': 'Working on the rest of ‚Äúimproving the baseline‚Äù now', 'response': 'Could someone confirm that the number of the original training data is 209?'}\\n\",\n",
       " \"{'prompt': 'Could someone confirm that the number of the original training data is 209?', 'response': 'Oh yeah @Jeremy Tow @Jonathan Cheung when training the models for part 5, id recommend using the current train code implemented up till 4c to check if both IoU and accuracy increase.  Baseline + enhancements up till 4c gives IoU around 0.065 to 0.067 and accuracy 0.72 to 0.74  I saw a piazza post that they expect our models in part 5 to give better iou and accuracy'}\\n\",\n",
       " \"{'prompt': 'or ig today', 'response': 'I think remotely!'}\\n\",\n",
       " \"{'prompt': 'I think remotely!', 'response': 'This is great!!!'}\\n\",\n",
       " '{\\'prompt\\': \"I\\'ll create a Zoom link\", \\'response\\': \\'I normally take a VERY LONG TIME for breakfast üòÇüòÇüòÇ\\'}\\n',\n",
       " '{\\'prompt\\': \\'Ester voted for \"Discussion - Experimentation\" in the poll.\\', \\'response\\': \\'Kong voted for \"Related Work (at least 2 people)\" in the poll.\\'}\\n',\n",
       " '{\\'prompt\\': \\'Kong voted for \"Related Work (at least 2 people)\" in the poll.\\', \\'response\\': \\'I‚Äôll add UNet as one of the related works\\'}\\n',\n",
       " '{\\'prompt\\': \"We\\'ll aim to get the report draft done by tonight!\", \\'response\\': \\'Yesss agreed!\\'}\\n',\n",
       " \"{'prompt': 'okok could I have your modified basic_fcn file haha', 'response': 'Ester do u plan to push ur code to master? The seed and plot that u added to each train file'}\\n\",\n",
       " \"{'prompt': 'you can cite a paper using \\\\\\\\citet{reference_name} and it will automatically appear in the References section', 'response': '@Jeremy Tow if you have extra time, could you double check the architecture for UNet? For some reason it isn‚Äôt doing well, we got IoU 0.0556 and accuracy 0.75'}\\n\",\n",
       " \"{'prompt': 'yea fs i‚Äôll check it in a bit', 'response': 'Thank you!'}\\n\",\n",
       " \"{'prompt': 'I‚Äôm trying 5a but data hub is not working for me right now', 'response': 'Oohhh might be cause there are too many people. I had to wait for quite a bit until I get assign a node (I used ssh)'}\\n\",\n",
       " \"{'prompt': 'it works again!', 'response': 'Do we want to set a cutoff time for the models? For tomorrow.'}\\n\",\n",
       " \"{'prompt': 'I‚Äôll go to office hours tomorrow', 'response': 'Okkk! I‚Äôll probably go too üòÖ'}\\n\",\n",
       " \"{'prompt': 'I did the abstract too', 'response': 'Also, not exactly sure about the metric IoU but now I‚Äôm not sure if it can be expressed in percentage'}\\n\",\n",
       " \"{'prompt': 'Also, not exactly sure about the metric IoU but now I‚Äôm not sure if it can be expressed in percentage', 'response': 'I have zero prior knowledge about IoU though, just purely based on articles that I read they put it in decimal'}\\n\",\n",
       " \"{'prompt': 'but idk', 'response': 'Yeah, sorry Ester,  I said percentage made sense this morning but I‚Äôm not sure anymore ü§îü§ìüòÖ'}\\n\",\n",
       " \"{'prompt': 'Yeah, sorry Ester,  I said percentage made sense this morning but I‚Äôm not sure anymore ü§îü§ìüòÖ', 'response': 'thought I‚Äôd share. So there has been a mystery with 5c where by the accuracy and IoU just stop improving at one point even when validation loss is actually decreasing.  There isn‚Äôt any bug in the architecture, even though I did add some minor changes to the architecture but honestly nothing changed in terms of the results that we are getting'}\\n\",\n",
       " \"{'prompt': 'thought I‚Äôd share. So there has been a mystery with 5c where by the accuracy and IoU just stop improving at one point even when validation loss is actually decreasing.  There isn‚Äôt any bug in the architecture, even though I did add some minor changes to the architecture but honestly nothing changed in terms of the results that we are getting', 'response': 'And I read the original paper multiple times and many forums, and finally broke through the upper bound:  Kind of unexpected but it was because of a combination of the following: 1. Weight initialisation - I changed it to using normal distribution (which was mentioned in the paper)  2. Back to using SGD, cause for some reason the paper mentioned about using momentum'}\\n\",\n",
       " \"{'prompt': 'And I read the original paper multiple times and many forums, and finally broke through the upper bound:  Kind of unexpected but it was because of a combination of the following: 1. Weight initialisation - I changed it to using normal distribution (which was mentioned in the paper)  2. Back to using SGD, cause for some reason the paper mentioned about using momentum', 'response': 'So for 5a, it might be something that‚Äôs beyond the network architecture, not exactly sure but yeah'}\\n\",\n",
       " \"{'prompt': 'So for 5a, it might be something that‚Äôs beyond the network architecture, not exactly sure but yeah', 'response': 'Also one thing I didn‚Äôt really try is that, UNet paper mentioned that they use batch size of 1, but I‚Äôm not sure how helpful this will be'}\\n\",\n",
       " \"{'prompt': 'How‚Äôs the new test IoU after these improvements?', 'response': 'Still running but for val the highest so far is 0.057 (not great but I‚Äôm low-key traumatized by seeing the exact same 0.0556 for the past few hours üòÇüòÇüòÇ'}\\n\",\n",
       " \"{'prompt': '0.0556 is interesting ‚Äî I was using a very different architecture and it also got stuck at that exact number', 'response': 'this is so mysterious'}\\n\",\n",
       " '{\\'prompt\\': \"I checked that data augmentation should be reproducible. Then it\\'s strange why not 4c\", \\'response\\': \\'Oohh like the random rotation and random crop?\\'}\\n',\n",
       " \"{'prompt': 'i tried doing the batch size of 1 thing mentioned in the unet paper and im getting better pixel acc of 0.73 but worse iou of 0.05432', 'response': 'I‚Äôm thinking - when batch size is 1, we will need a lot more epochs. How many epochs did you train on just now?'}\\n\",\n",
       " '{\\'prompt\\': \\'when i did a batch size of 1 i augmented the training data so that each \"img\" is actually a composite of 16 images, so the amount of epochs needed is the same??\\', \\'response\\': \\'In this case, is the backward based on one image or 16 images?\\'}\\n',\n",
       " \"{'prompt': 'well like technically its 1 image that is a composite of 16 imgs', 'response': 'Yeah, like 15 different transformations of the same image + the original?'}\\n\",\n",
       " \"{'prompt': 'umm i think its 16 different images', 'response': 'Oh as in, one image is segmented into 16 pieces?'}\\n\",\n",
       " \"{'prompt': 'i tried sgd as well and its worse', 'response': 'Interestinggggg'}\\n\",\n",
       " \"{'prompt': 'Interestinggggg', 'response': 'Maybe I can run it on your branch? And see if I get the same'}\\n\",\n",
       " \"{'prompt': 'yea give me a bit to make it suitable for human viewing first lol', 'response': 'Yeah take ur time!'}\\n\",\n",
       " \"{'prompt': 'also idk why yet but im consistently getting iou of 0.49 now', 'response': 'For UNet ?'}\\n\",\n",
       " \"{'prompt': 'yea', 'response': 'Ok I‚Äôll look into it in the morning ü§ì thanksss!!'}\\n\",\n",
       " \"{'prompt': '0.49? üòÆüòÆüòÆ', 'response': 'The highest I‚Äôve gotten is 0.0627'}\\n\",\n",
       " \"{'prompt': 'The highest I‚Äôve gotten is 0.0627', 'response': '@Ester Tsai can I confirm that u have already discussed Xavier and batch norm in the baseline method section? If so, I won‚Äôt include those in the introduction'}\\n\",\n",
       " '{\\'prompt\\': \\'@Ester Tsai can I confirm that u have already discussed Xavier and batch norm in the baseline method section? If so, I won‚Äôt include those in the introduction\\', \\'response\\': \\'Kong voted for \"Introduction\" in the poll.\\'}\\n',\n",
       " \"{'prompt': 'I will try the architecture described in this lecture for 5A ', 'response': 'Ester, is there a time that you would like me to finalize UNet? For plots and the final numbers (I‚Äôll take care of the segmentation result for one test data in visualize.ipynb'}\\n\",\n",
       " \"{'prompt': 'Ester, is there a time that you would like me to finalize UNet? For plots and the final numbers (I‚Äôll take care of the segmentation result for one test data in visualize.ipynb', 'response': 'I‚Äôm thinking latest by 6pm today, but I‚Äôm flexible'}\\n\",\n",
       " \"{'prompt': '6pm sounds good', 'response': 'Yeah'}\\n\",\n",
       " \"{'prompt': 'Yeah', 'response': 'Okay and once you run everything, could you push your version of 5b to main'}\\n\",\n",
       " \"{'prompt': 'I can also fill in the segmentation result since it requires someone having all 7 models', 'response': 'Yeah'}\\n\",\n",
       " \"{'prompt': 'Yeah', 'response': 'Sure'}\\n\",\n",
       " \"{'prompt': 'Sure', 'response': 'Ok I‚Äôll get you the best possible UNet by 6 üòé'}\\n\",\n",
       " \"{'prompt': 'Restarting the server used to help, but not this time', 'response': 'Smaller batch size usually works for me'}\\n\",\n",
       " \"{'prompt': 'Smaller batch size usually works for me', 'response': 'And restarting'}\\n\",\n",
       " \"{'prompt': '0.085 IoU', 'response': 'NAISEEEEEE'}\\n\",\n",
       " '{\\'prompt\\': \"we\\'ll see if it\\'s gonna be good!\", \\'response\\': \\'and yeah this is interesting, I got 0.062 IoU only after changing the kernel size in UNet\\'}\\n',\n",
       " \"{'prompt': 'changing to what kernel size?', 'response': 'All those 3*3 to 4*4'}\\n\",\n",
       " \"{'prompt': 'All those 3*3 to 4*4', 'response': 'And final layer from 1*1 to 3*3'}\\n\",\n",
       " \"{'prompt': 'sad, each epoch takes 8 min to train', 'response': 'Ahhhh'}\\n\",\n",
       " \"{'prompt': 'Ahhhh', 'response': 'That‚Äôs quite a while'}\\n\",\n",
       " \"{'prompt': 'That‚Äôs quite a while', 'response': 'Like one hour for 6 epochs'}\\n\",\n",
       " \"{'prompt': 'Like one hour for 6 epochs', 'response': 'Is the network very deep?'}\\n\",\n",
       " \"{'prompt': 'Is the network very deep?', 'response': 'What we could do is have like 6-10epochs, then mention the complexity and efficiency in the discussion section'}\\n\",\n",
       " \"{'prompt': 'Ok', 'response': 'OH THANK YOU!!'}\\n\",\n",
       " \"{'prompt': 'OH THANK YOU!!', 'response': 'Okay then I‚Äôll be able to finalize before 2pm!'}\\n\",\n",
       " \"{'prompt': 'Okay then I‚Äôll be able to finalize before 2pm!', 'response': 'Did it take long to train 5b? The transfer learning model'}\\n\",\n",
       " \"{'prompt': 'Did it take long to train 5b? The transfer learning model', 'response': 'Just curious'}\\n\",\n",
       " \"{'prompt': '^also remember to add a table for your architecture details like Jeremy and Winfrey did in the report', 'response': 'finally something close to 0.07'}\\n\",\n",
       " \"{'prompt': '0.11 with leaky relu', 'response': '@Ester Tsai do you have a preference for pushing to main or pushing to my branch for you to compile the models? I‚Äôm only pushing train_5_c.py and unet.py'}\\n\",\n",
       " \"{'prompt': '@Ester Tsai do you have a preference for pushing to main or pushing to my branch for you to compile the models? I‚Äôm only pushing train_5_c.py and unet.py', 'response': 'For me to push to main or my branch** whichever is easier for you grab'}\\n\",\n",
       " \"{'prompt': 'this is with some kernel, acivation, and layer changes as well comapred to just stride', 'response': 'Personally think that IoU greater than 0.07 IoU good enough and above 0.10 is GrEaT ü§£'}\\n\",\n",
       " \"{'prompt': 'ok then', 'response': 'As long as we can justify we made sufficient changes to the architecture in the report and have some discussion, then we r good'}\\n\",\n",
       " '{\\'prompt\\': \\'pushed\\', \\'response\\': \"ok pushed 5c to my branch as well! there\\'s a high chance you won\\'t get the exact same result, i believe it\\'s the randomness that you mentioned yesterday\"}\\n',\n",
       " '{\\'prompt\\': \"ok pushed 5c to my branch as well! there\\'s a high chance you won\\'t get the exact same result, i believe it\\'s the randomness that you mentioned yesterday\", \\'response\\': \"master\\'s README has been updated for submission later today!\"}\\n',\n",
       " \"{'prompt': 'I‚Äôve finished running 5A and will update the report with the plots for the Results section after my tutor hours', 'response': 'Yeah no problem! Thank you!'}\\n\",\n",
       " \"{'prompt': 'Yeah no problem! Thank you!', 'response': 'I can help cross check the architecture table when you are done @Jeremy Tow @Jonathan Cheung   @Jeremy Tow do you plan to add ResNet under related works? Just checking!'}\\n\",\n",
       " \"{'prompt': 'i‚Äôll add the paper in', 'response': 'Ok coools!'}\\n\",\n",
       " '{\\'prompt\\': \"I\\'ll rerun it maybe?\", \\'response\\': \\'Oh wow test IoU is 0.11?\\'}\\n',\n",
       " \"{'prompt': 'Oh wow test IoU is 0.11?', 'response': 'That‚Äôs interesting'}\\n\",\n",
       " \"{'prompt': 'That‚Äôs interesting', 'response': 'Sure'}\\n\",\n",
       " \"{'prompt': 'WOW', 'response': 'What magic did you do'}\\n\",\n",
       " \"{'prompt': 'What magic did you do', 'response': 'üòÇüòÇüòÇ'}\\n\",\n",
       " \"{'prompt': 'üòÇüòÇüòÇ', 'response': 'Dang UNet was crawling and now it‚Äôs flying'}\\n\",\n",
       " \"{'prompt': 'The batch size can be understood as a trade-off between accuracy and speed. Large batch sizes can lead to faster training times but may result in lower accuracy and overfitting, while smaller batch sizes can provide better accuracy, but can be computationally expensive and time-consuming.', 'response': 'I seeee'}\\n\",\n",
       " \"{'prompt': 'I seeee', 'response': 'Interesting'}\\n\",\n",
       " \"{'prompt': 'Interesting', 'response': 'I like this sample!!'}\\n\",\n",
       " \"{'prompt': 'It‚Äôs supposed to be over the entire validation set', 'response': 'Eh really? I thought I checked'}\\n\",\n",
       " \"{'prompt': 'Eh really? I thought I checked', 'response': 'Oh u mean the average?'}\\n\",\n",
       " \"{'prompt': 'Oh u mean the average?', 'response': 'Or the iou function'}\\n\",\n",
       " \"{'prompt': 'So the number might be a little odd', 'response': 'Ohhh ok we are looking at the train files not the util iou right'}\\n\",\n",
       " \"{'prompt': 'Ohhh ok we are looking at the train files not the util iou right', 'response': 'Oh util iou averages over the batch'}\\n\",\n",
       " \"{'prompt': 'But I discovered that when I make the test batch size smaller the IoU goes up, which it shouldn‚Äôt', 'response': 'enumerate batch_loader gives a batch. So inputs are 16*3*224*224'}\\n\",\n",
       " \"{'prompt': 'hmmm let me go through the code and check', 'response': 'Maybe in util iou np.nanmean can be done on a specific dimension'}\\n\",\n",
       " \"{'prompt': 'Maybe in util iou np.nanmean can be done on a specific dimension', 'response': 'Or do a double loop in util iou, so   loop through preds, targets:    Loop through all classes:    Np.nanmean for that particular one pred'}\\n\",\n",
       " \"{'prompt': 'High key I will just revert back to what we had before because it matched the description well', 'response': 'Actually‚Ä¶ we can keep it as is üòÇ'}\\n\",\n",
       " \"{'prompt': 'Actually‚Ä¶ we can keep it as is üòÇ', 'response': 'Yeah'}\\n\",\n",
       " \"{'prompt': 'Yeah', 'response': 'Agreed'}\\n\",\n",
       " \"{'prompt': 'Samuel chu said he tried and it wasn‚Äôt very different', 'response': 'Okkk double checking the architecture now'}\\n\",\n",
       " \"{'prompt': 'Okkk double checking the architecture now', 'response': 'Oh just joking, I‚Äôll wait a bit'}\\n\",\n",
       " \"{'prompt': 'Oh just joking, I‚Äôll wait a bit', 'response': '@Jonathan Cheung just checking, for 5a specifically, did you make any changes to  - weight initialization - data augmentation - optimizer - class weights'}\\n\",\n",
       " \"{'prompt': 'just added all the plots', 'response': 'yupp they look perfecto'}\\n\",\n",
       " \"{'prompt': 'yupp they look perfecto', 'response': 'thankies!'}\\n\",\n",
       " \"{'prompt': 'But I think that‚Äôs default', 'response': 'Oh then did u use the learning rate scheduler?'}\\n\",\n",
       " \"{'prompt': 'I think so?', 'response': 'Okk'}\\n\",\n",
       " \"{'prompt': 'Okk', 'response': 'Let me add that blurb saying that everything else is kept the same'}\\n\",\n",
       " \"{'prompt': 'Let me add that blurb saying that everything else is kept the same', 'response': 'In the report'}\\n\",\n",
       " \"{'prompt': 'In the report', 'response': 'Quick question, is there supposed to be a separate model Python file for 5a and 5b?'}\\n\",\n",
       " \"{'prompt': 'so it kinda overwrites the 4c one', 'response': 'Oh no worries, I think Ester got u'}\\n\",\n",
       " \"{'prompt': 'I got it yeah!', 'response': 'Ok I see it now!'}\\n\",\n",
       " \"{'prompt': 'I just wanna get a better plot for 5A if possible', 'response': 'Yes on it'}\\n\",\n",
       " \"{'prompt': 'Yes on it', 'response': 'Ah'}\\n\",\n",
       " \"{'prompt': 'Ah', 'response': 'Ester, I am seeing train_5_c has the old version'}\\n\",\n",
       " \"{'prompt': 'Ester, I am seeing train_5_c has the old version', 'response': 'Like it calls FCN and uses AdamW'}\\n\",\n",
       " \"{'prompt': 'We still need to write some of discussion section', 'response': 'Yeah for each of the models in 5, need to draw insights from table and visuals'}\\n\",\n",
       " \"{'prompt': 'Yeah for each of the models in 5, need to draw insights from table and visuals', 'response': 'Currently we are only drawing from table'}\\n\",\n",
       " \"{'prompt': 'I‚Äôm adding to the discussion right now', 'response': 'Hmmmm I‚Äôm kind of stuck with UNet‚Äôs discussion on the sample test segmentation, since it had worse results than the other two architecture, the test segmentation shows that UNet captures a lot of them üëÄ'}\\n\",\n",
       " '{\\'prompt\\': \\'Ok I‚Äôm back home, is there anything I need or finish up\\', \\'response\\': \"yes, could you check if 5a\\'s discussion mentions the loss plot and the segmentation visual\"}\\n',\n",
       " \"{'prompt': 'and i finished the architecture table for resnet', 'response': 'ok added mention on the results plot and visual in part 4 + baseline'}\\n\",\n",
       " \"{'prompt': 'ok added mention on the results plot and visual in part 4 + baseline', 'response': 'I think some of us can start checking the entire paper'}\\n\",\n",
       " \"{'prompt': 'I think some of us can start checking the entire paper', 'response': '@Jeremy Tow there are three layers numbered 31, is that intentional?'}\\n\",\n",
       " \"{'prompt': 'fixed ty', 'response': 'np! thanks'}\\n\",\n",
       " \"{'prompt': 'np! thanks', 'response': 'ok I will stop writing for now, I think some of us might be working on the same section right now, I am starting to see some similar description üòÖ'}\\n\",\n",
       " \"{'prompt': 'ok I will stop writing for now, I think some of us might be working on the same section right now, I am starting to see some similar description üòÖ', 'response': 'also i used present tense for everything and some of us used past tense.   we can stick with one for consistency purpose - whoever is checking the entire paper.'}\\n\",\n",
       " \"{'prompt': 'Actually not understanding the segmentation visual, r the blobs supposed to look like the shape of the ppl?', 'response': '@Jeremy Tow could you check the code submission? I am not sure if the transfer learning file is actually reflecting the architecture in the report'}\\n\",\n",
       " \"{'prompt': 'just checked, it looks good', 'response': 'ok cools! I just realized that none of the encoder is actually called. sorry, got confused for a bit.'}\\n\",\n",
       " \"{'prompt': 'ok cools! I just realized that none of the encoder is actually called. sorry, got confused for a bit.', 'response': 'thanks for confirming!'}\\n\",\n",
       " \"{'prompt': '5 min per epoch', 'response': 'nice nice'}\\n\",\n",
       " \"{'prompt': 'page breaks?', 'response': 'try \\\\\\\\newpage'}\\n\",\n",
       " \"{'prompt': 'It‚Äôs already there haha', 'response': 'forcing new page on new section'}\\n\",\n",
       " \"{'prompt': 'No idea why item doesn‚Äôt help', 'response': 'ohh'}\\n\",\n",
       " \"{'prompt': 'ohh', 'response': 'ok fixed!'}\\n\",\n",
       " \"{'prompt': 'ok fixed!', 'response': 'use \\\\\\\\clearpage instead'}\\n\",\n",
       " \"{'prompt': '2 per page seems to fit pretty well', 'response': 'ya I made it smaller but it was before we organize the report'}\\n\",\n",
       " \"{'prompt': 'ya I made it smaller but it was before we organize the report', 'response': 'so feel free to make new changes haha'}\\n\",\n",
       " \"{'prompt': 'is everyhting else finalized?', 'response': 'yes on my end'}\\n\",\n",
       " \"{'prompt': 'Sure!', 'response': 'good work!'}\\n\",\n",
       " \"{'prompt': 'I‚Äôm looking through it again but feel free to submit', 'response': 'Wait sorry'}\\n\",\n",
       " \"{'prompt': 'Wait sorry', 'response': 'The architecture for transfer learning'}\\n\",\n",
       " \"{'prompt': 'The architecture for transfer learning', 'response': 'I‚Äôll fix it, gimme 1 min'}\\n\",\n",
       " \"{'prompt': 'I‚Äôll fix it, gimme 1 min', 'response': 'Ok done!'}\\n\",\n",
       " \"{'prompt': 'ok ill usbmit now', 'response': 'Thank you Jonathan!'}\\n\",\n",
       " \"{'prompt': 'ok i added u guys, check if u can see the reprot on gradescope', 'response': 'Yup!'}\\n\",\n",
       " \"{'prompt': 'Done! Sorry for cutting it so close!', 'response': 'Thanks hehe'}\\n\",\n",
       " \"{'prompt': 'Thanks hehe', 'response': 'No no, not at all'}\\n\",\n",
       " \"{'prompt': 'nice job', 'response': 'All good, thanks for the good work everyone!! Get some good rest for now üò¥'}\\n\",\n",
       " \"{'prompt': 'tytyty', 'response': 'Group name is recurrent for PA3!'}\\n\",\n",
       " \"{'prompt': 'Ester named the group CSE 151B PA3.', 'response': 'Thanks for the name update that‚Äôs very thoughtful of you HAHA'}\\n\",\n",
       " \"{'prompt': '@Samuel Chu if you‚Äôd like to join!', 'response': 'I will get started on the programming part tonight, has anyone started working on it? Just wanted to see where I should start with, in case some parts have already been completed'}\\n\",\n",
       " \"{'prompt': 'I plan to start at 3:30pm!', 'response': 'Okie I can check with you tonight when I start!'}\\n\",\n",
       " \"{'prompt': 'Actually I‚Äôll start maybe tonight! Gonna crank out some tutor stuff that‚Äôs time sensitive', 'response': 'Yup yuppp no worries!'}\\n\",\n",
       " \"{'prompt': 'Yup yuppp no worries!', 'response': 'Heloo, I made changes to fix the indentation issue mentioned on Piazza and pushed the main branch. None of the body content is changed, just indentation'}\\n\",\n",
       " \"{'prompt': 'Heloo, I made changes to fix the indentation issue mentioned on Piazza and pushed the main branch. None of the body content is changed, just indentation', 'response': 'There has been quite a few typos in the given template from what I have seen in Piazza. I fixed most of them and pushed to main  Another typo that is not on main  In util, get_random_song_slice takes in string data not list  Similarly, get_random_song_sequence_target takes in song in string format not list'}\\n\",\n",
       " \"{'prompt': 'I‚Äôll do the individual part first and then get on the code', 'response': 'I‚Äôve push a working model to my branch called winfrey (should cover up till Q3)'}\\n\",\n",
       " \"{'prompt': 'I‚Äôve push a working model to my branch called winfrey (should cover up till Q3)', 'response': 'The loss I‚Äôm getting is lower (0.06) than expected (1.9) so I‚Äôm still clarifying with the TA on loss calculation - there‚Äôs a bunch of averaging going on ü•≤'}\\n\",\n",
       " \"{'prompt': 'Just learned the ABC notation üò≥', 'response': 'Yaaahhh heheh'}\\n\",\n",
       " \"{'prompt': 'Yaaahhh heheh', 'response': 'The lowest loss I get so far is 2.1679, Im making changes to the optimizer to find the one that gives the closest to 1.9  Other than that, my branch is the most updated of my progress'}\\n\",\n",
       " \"{'prompt': '@Jeremy Tow @Jonathan Cheung @Samuel Chu any progress so far? üòÉ', 'response': 'I‚Äôll be occupied with midterm and capstone until the weekend, so I‚Äôll build off whatever we have by then'}\\n\",\n",
       " \"{'prompt': 'Has anyone else figured out how to solve this?', 'response': 'Looking at it now'}\\n\",\n",
       " \"{'prompt': 'Looking at it now', 'response': 'You got the error when u were trying to install PyTorch?'}\\n\",\n",
       " \"{'prompt': 'O I FIXED IT', 'response': 'OH how'}\\n\",\n",
       " \"{'prompt': 'unset LD_LIBRARY_PATH', 'response': 'OH LOL'}\\n\",\n",
       " \"{'prompt': 'OH LOL', 'response': 'Anything that has to do with PATH is very painful'}\\n\",\n",
       " '{\\'prompt\\': \"it\\'s very slow oop\", \\'response\\': \\'No I don‚Äôt think so, the forum that I read says it‚Äôs just old version stuffs\\'}\\n',\n",
       " \"{'prompt': 'No I don‚Äôt think so, the forum that I read says it‚Äôs just old version stuffs', 'response': 'Wifi matters a lot üòÇ'}\\n\",\n",
       " '{\\'prompt\\': \"I\\'m training on cpu oops\", \\'response\\': \\'It‚Äôs slower when I run at school\\'}\\n',\n",
       " \"{'prompt': 'It‚Äôs slower when I run at school', 'response': 'Hmmmm'}\\n\",\n",
       " \"{'prompt': 'how long does it take for you?', 'response': 'Like one hour'}\\n\",\n",
       " \"{'prompt': 'Like one hour', 'response': 'For 100 epochs'}\\n\",\n",
       " '{\\'prompt\\': \"jk it\\'s slower for me\", \\'response\\': \\'Ahh cause it‚Äôs on cpu?\\'}\\n',\n",
       " \"{'prompt': 'YAYAYAYAYYAYAYYAYAYAYY', 'response': 'Oh THATS GREAT'}\\n\",\n",
       " \"{'prompt': 'CAPSToNE SUPREMEM', 'response': 'Honestly, I have never used datahub once'}\\n\",\n",
       " \"{'prompt': 'Is it about the same?', 'response': 'Wow urs is slightly faster actually'}\\n\",\n",
       " \"{'prompt': 'It‚Äôs a bigger GPU although not sure if that affects speed?', 'response': 'Ooohhh I seeeee'}\\n\",\n",
       " \"{'prompt': 'is it correct there is no test loss?', 'response': 'This is with using Adam, lr=0.001 and weight decay = 0.00001'}\\n\",\n",
       " \"{'prompt': 'This is with using Adam, lr=0.001 and weight decay = 0.00001', 'response': 'Also TA suggested running with more epochs, I‚Äôve only been running 100 epochs'}\\n\",\n",
       " \"{'prompt': 'Also TA suggested running with more epochs, I‚Äôve only been running 100 epochs', 'response': 'And probably seed 10'}\\n\",\n",
       " \"{'prompt': 'And probably seed 10', 'response': 'And yeah no test loss, not given in the starter code'}\\n\",\n",
       " \"{'prompt': 'do you know if we are allowed to change other configs like SEQ_SIZE', 'response': 'ah that I‚Äôm not sure. I thought we are not supposed to change anything in the config until like 5 mins ago a TA said try more epochs'}\\n\",\n",
       " \"{'prompt': 'hmm I suppose instruction said to use SEQ_SIZE=25~30', 'response': 'Ohhh good catch!'}\\n\",\n",
       " '{\\'prompt\\': \"I\\'m gonna work on generate.py for now\", \\'response\\': \\'coooools I‚Äôll keep it running in the background while I do other stuffs üòÇ\\'}\\n',\n",
       " \"{'prompt': 'So.. do we have to remove the unknown characters?', 'response': 'In generate'}\\n\",\n",
       " \"{'prompt': 'In generate', 'response': 'I‚Äôm not sure if those bolded characters are identified as unknown'}\\n\",\n",
       " '{\\'prompt\\': \"the bold shows bad syntax (system can\\'t recognize what it wants to do)\", \\'response\\': \\'I see\\'}\\n',\n",
       " \"{'prompt': 'I see', 'response': 'So there‚Äôs gonna be some way we need to figure out to control/process them before putting into the converter'}\\n\",\n",
       " \"{'prompt': 'I wonder how it knows what the context is if I do that', 'response': 'There‚Äôs the hidden state that saves the context'}\\n\",\n",
       " \"{'prompt': 'Let me fix something real quick and check again', 'response': 'Okk'}\\n\",\n",
       " \"{'prompt': '@Winfrey Kong when you finish training, could you send me the model checkpoint if possible? or I can just send you my generate.py', 'response': 'Yuppp!'}\\n\",\n",
       " \"{'prompt': 'AdamW lr=1e-3', 'response': 'Nice niceeeee'}\\n\",\n",
       " \"{'prompt': 'Also our current val loss is ok! We can move on to hyperparameter tuning,. I;ll start that tonight', 'response': 'Yayyy!! Thank you!!'}\\n\",\n",
       " \"{'prompt': 'Hold on gonna fix something small real quick', 'response': 'Not too sure, I only remember seeing a post on piazza about softmax and temperature'}\\n\",\n",
       " \"{'prompt': '@everyone', 'response': 'Yasssss'}\\n\",\n",
       " \"{'prompt': 'Yasssss', 'response': 'I have midterm at 4 so I‚Äôll can only stay until 3.30pm for the call'}\\n\",\n",
       " \"{'prompt': 'I have midterm at 4 so I‚Äôll can only stay until 3.30pm for the call', 'response': '@Ester Tsai @Jonathan Cheung logistic is actually a nonlinear technique, good job!! ü§£üéâ'}\\n\",\n",
       " \"{'prompt': 'I‚Äôm working on the heat map, almost done', 'response': 'What‚Äôs the lowest val loss so far with hyperparameter tuning?'}\\n\",\n",
       " \"{'prompt': '2.008', 'response': 'Interesting'}\\n\",\n",
       " \"{'prompt': 'Interesting', 'response': 'Thanks!!'}\\n\",\n",
       " \"{'prompt': 'like in 5 mknjtes', 'response': 'Ah can we keep it at 3? I‚Äôm at work until 3pm and I have a consultation at the moment'}\\n\",\n",
       " \"{'prompt': '@Winfrey Kong do you think you help me fix the gpu error i‚Äôm having with capstone gpu later today?', 'response': 'Oh yeah ofc'}\\n\",\n",
       " \"{'prompt': 'Oh yeah ofc', 'response': 'Is 8.30 ok?'}\\n\",\n",
       " \"{'prompt': 'would 9:15 work?', 'response': 'Okiee'}\\n\",\n",
       " '{\\'prompt\\': \"Samuel Chu is inviting you to a scheduled Zoom meeting.  Topic: Samuel Chu\\'s Personal Meeting Room  Join Zoom Meeting \", \\'response\\': \\'Okie coming\\'}\\n',\n",
       " \"{'prompt': 'I have the basic set of results that the report needs (loss plots, heatmaps, and the training/val loss are in the model checkpoint names), but we can test more models and get better results', 'response': 'Training with 150 sequence length right now, it‚Äôs training slower so‚Ä¶  ‚Ä¶ will mostly hear back from me after 2 hours üòÇüòÇ'}\\n\",\n",
       " \"{'prompt': 'Training with 150 sequence length right now, it‚Äôs training slower so‚Ä¶  ‚Ä¶ will mostly hear back from me after 2 hours üòÇüòÇ', 'response': 'Code is still running but I got 1.988 lowest so far with sequence length of 70'}\\n\",\n",
       " \"{'prompt': 'Code is still running but I got 1.988 lowest so far with sequence length of 70', 'response': 'I think we can go ahead with sequence length of 70, and the training speed is acceptable'}\\n\",\n",
       " \"{'prompt': 'i trained with a sequence length of 300 and let it run the whole day, i‚Äôll let you guys know the loss after i get back to my computer', 'response': 'Training with sequence length of 50 is even better. Got 1.97 at epoch 198, also faster for sure'}\\n\",\n",
       " \"{'prompt': 'Training with sequence length of 50 is even better. Got 1.97 at epoch 198, also faster for sure', 'response': 'okie I think I will settle with sequence length of 50 for now'}\\n\",\n",
       " \"{'prompt': 'will also see what Jeremy gets with sequence length of 300!', 'response': 'baseline model plot'}\\n\",\n",
       " \"{'prompt': 'its kinda not good', 'response': 'I‚Äôm thinking if it‚Äôs overfitting'}\\n\",\n",
       " \"{'prompt': 'yea :(', 'response': 'But also getting 2.0 something at epoch 151 is kinda okay ish?'}\\n\",\n",
       " \"{'prompt': 'But also getting 2.0 something at epoch 151 is kinda okay ish?', 'response': 'Oh sorry my bad, I saw epoch instead of iter'}\\n\",\n",
       " \"{'prompt': 'Oh sorry my bad, I saw epoch instead of iter', 'response': 'Ignore this heh'}\\n\",\n",
       " \"{'prompt': 'Ah I see', 'response': 'Just very very slightly higher'}\\n\",\n",
       " \"{'prompt': 'def not worth doing', 'response': 'Would you like to try sequence length of 50? to make sure my GPU is not overly optimistic haha'}\\n\",\n",
       " \"{'prompt': 'ok yea fs let me do a run rn', 'response': 'Okkk'}\\n\",\n",
       " \"{'prompt': 'oh how many neurons are u using', 'response': '150'}\\n\",\n",
       " \"{'prompt': '150', 'response': 'Epochs 260'}\\n\",\n",
       " \"{'prompt': 'skulling', 'response': 'OH LOL'}\\n\",\n",
       " \"{'prompt': 'OH LOL', 'response': 'Hahaha okieee'}\\n\",\n",
       " \"{'prompt': 'Hahaha okieee', 'response': 'And I‚Äôll generate the plots for hyperparamter tuning using sequence length of 50 and all else constant except for the specific hyperparamter , I‚Äôll take the neurons config, and would you like to take the dropout configs?'}\\n\",\n",
       " \"{'prompt': 'do i even try 200 lol the training time is obscene', 'response': 'One round takes like 1.5hours to 2 hours for me (cause sometimes my wifi drops lol   So I def won‚Äôt be able to do all configs in one day'}\\n\",\n",
       " \"{'prompt': 'One round takes like 1.5hours to 2 hours for me (cause sometimes my wifi drops lol   So I def won‚Äôt be able to do all configs in one day', 'response': 'How long did it take üòÇ'}\\n\",\n",
       " \"{'prompt': 'How long did it take üòÇ', 'response': 'Oh I‚Äôm joking'}\\n\",\n",
       " \"{'prompt': 'Oh I‚Äôm joking', 'response': 'I mean.. only if u want to keep it running overnight üòÇ'}\\n\",\n",
       " \"{'prompt': 'like fully 4 minutes per epoch i think', 'response': 'I tried 150 and it got too miserable at one point I forgot how long I waited so I just interrupted it üòÇüòÇüòÇ'}\\n\",\n",
       " \"{'prompt': 'so like 17 hrs', 'response': 'Ouchy, wait how is ur GPU billed?'}\\n\",\n",
       " \"{'prompt': 'i have my own gpu lol', 'response': 'Ohhhhh like u own it?'}\\n\",\n",
       " \"{'prompt': 'yea its sitting in my computer next to me rn lol', 'response': 'when u said u have one, I was thinking that you actively rent/pay on demand from like Amazon'}\\n\",\n",
       " \"{'prompt': 'when u said u have one, I was thinking that you actively rent/pay on demand from like Amazon', 'response': 'üòÇ that‚Äôs very cool'}\\n\",\n",
       " \"{'prompt': 'oh man i could never aws is so expensive', 'response': 'OH REALLY dang I thought they always sell themselves as a cheaper option or like pay as you go, no hidden fee, low cost etc'}\\n\",\n",
       " \"{'prompt': 'training 50 takes me about 1:10 per epoch', 'response': 'Yeah does take a while, I think datahub might be faster but not really too significant? I haven‚Äôt really timed it properly. Anyway, if we train the full 260 epochs each time, we can probably only do like 3-4 rounds per person per day (for the amount of time that we are awake üò¨'}\\n\",\n",
       " '{\\'prompt\\': \"mb i\\'ll do runs on datahub and my personal gpu concurrently\", \\'response\\': \\'I could be wrong cause I just start it and work on other stuffs üòÇüòÇ it could actually be 3-4 hours but yeah I think running on both is a good idea if that‚Äôs manageable\\'}\\n',\n",
       " \"{'prompt': 'Should I try running anything when I‚Äôm back or just focus on the report?', 'response': 'Hmmm I‚Äôd say.. we‚Äôll see? I was thinking that you‚Äôd want to rest and settle down once u get home. I think I will be able to run 2 full rounds by afternoon (covering all of hyperparamter part b)'}\\n\",\n",
       " \"{'prompt': 'Lmk if I can help run anything!', 'response': 'Yup yup'}\\n\",\n",
       " \"{'prompt': 'Yup yup', 'response': 'Yes! could you run config 250 neurons with sequence length 50?'}\\n\",\n",
       " \"{'prompt': 'AdamW lr=1e-3?', 'response': 'Yupp everything else the same'}\\n\",\n",
       " \"{'prompt': 'okok!', 'response': 'I updated row 1'}\\n\",\n",
       " \"{'prompt': 'I updated row 1', 'response': 'Row 2 still running'}\\n\",\n",
       " \"{'prompt': 'Row 2 still running', 'response': 'As in, the current row 1 is accurate'}\\n\",\n",
       " \"{'prompt': 'Which branch should I pull?', 'response': 'ester2'}\\n\",\n",
       " \"{'prompt': 'Oo what was the bug in train.py?', 'response': 'Remember u said u added to SongRNN to return x0 or x1'}\\n\",\n",
       " \"{'prompt': 'Remember u said u added to SongRNN to return x0 or x1', 'response': 'So now the songrnn forward returns output, _ instead of just output'}\\n\",\n",
       " \"{'prompt': 'I pushed my results for config_250_neurons.json and added the train and val loss to the report', 'response': 'Gosh it finished?'}\\n\",\n",
       " \"{'prompt': 'LOL YEAH', 'response': 'So fast!! ü•≤'}\\n\",\n",
       " \"{'prompt': 'does it increase the amount of information that can be stored in the cell state or something?', 'response': 'For this PA, neurons = number of features in hidden state'}\\n\",\n",
       " \"{'prompt': 'hidden state = cell state?', 'response': 'Our LSTM is a single layer one, so it‚Äôs only one LSTM'}\\n\",\n",
       " \"{'prompt': 'ok', 'response': 'They r different in LSTM'}\\n\",\n",
       " \"{'prompt': 'They r different in LSTM', 'response': 'In RNN, there‚Äôs only hidden state, no cell state'}\\n\",\n",
       " \"{'prompt': 'thanks', 'response': 'The last hyperparameter tuning will be ready shortly! 48 epochs left üò™'}\\n\",\n",
       " \"{'prompt': 'so far baseline is still the best', 'response': '250 that I‚Äôm running atm is slightly better than baseline'}\\n\",\n",
       " \"{'prompt': '250 that I‚Äôm running atm is slightly better than baseline', 'response': 'Once this is done, if this is the best we have so far, I‚Äôll generate the heatmaps with this model first. If someone gets a better model, feel free to replace the neuron heatmaps'}\\n\",\n",
       " \"{'prompt': 'Once this is done, if this is the best we have so far, I‚Äôll generate the heatmaps with this model first. If someone gets a better model, feel free to replace the neuron heatmaps', 'response': '@Ester Tsai did you find training RNN faster or slower than LSTM? Or roughly the same'}\\n\",\n",
       " \"{'prompt': '@Ester Tsai did you find training RNN faster or slower than LSTM? Or roughly the same', 'response': 'Ok added results and plot for 200 neurons, will generate the heatmaps now'}\\n\",\n",
       " \"{'prompt': 'Oops I didn‚Äôt time it. Would it be helpful if I rerun part of it to time it?', 'response': '@Jeremy Tow could you add your plots for the dropout experiments? I think they might be helpful for Sam‚Äôs parts'}\\n\",\n",
       " \"{'prompt': '@Jeremy Tow could you add your plots for the dropout experiments? I think they might be helpful for Sam‚Äôs parts', 'response': 'Ohh no worries! No need hehe'}\\n\",\n",
       " \"{'prompt': 'the loss ones?', 'response': 'yes'}\\n\",\n",
       " \"{'prompt': 'yes', 'response': 'the loss plots'}\\n\",\n",
       " \"{'prompt': 'Does the ‚Äú93 unique characters‚Äù refer to all the possible outputs in abc format for each note?', 'response': 'Yes'}\\n\",\n",
       " \"{'prompt': 'Sure! We can make the figure a little wider and less tall', 'response': '@Jonathan Cheung I uploaded all the heatmaps from our best trained model under the figures folder. Are you able to see them?'}\\n\",\n",
       " \"{'prompt': 'Yeah I can see the figures', 'response': 'I‚Äôm thinking of letting you choose 3 heatmaps from there? Those that are more explainable for the discussion section'}\\n\",\n",
       " \"{'prompt': 'I‚Äôm thinking of letting you choose 3 heatmaps from there? Those that are more explainable for the discussion section', 'response': 'Then I‚Äôll put the 3 that u choose to discuss under results'}\\n\",\n",
       " \"{'prompt': 'Ok, I‚Äôll choose 3 between the 0-140 heatmaps', 'response': 'Okie thanks'}\\n\",\n",
       " \"{'prompt': 'i uploaded the dropout plots under figures, the sequence length is wrong so i‚Äôll update them by tmr morning but for now if anyone needs them they‚Äôre there', 'response': '@Jeremy Tow for song generation (results part 6b), this is the best model i have with me. also the best we have so far'}\\n\",\n",
       " '{\\'prompt\\': \\'@Jeremy Tow for song generation (results part 6b), this is the best model i have with me. also the best we have so far\\', \\'response\\': \"ah nvm it\\'s not allowing me send here\"}\\n',\n",
       " '{\\'prompt\\': \"ah nvm it\\'s not allowing me send here\", \\'response\\': \\'ok I push the best model to branch ester2, path is checkpoint/best_0219\\'}\\n',\n",
       " \"{'prompt': 'tysm', 'response': 'the config to use is config_200_neurons.json'}\\n\",\n",
       " \"{'prompt': 'Also are we going with our ‚Äúbest model‚Äù as the one with lowest Val loss?', 'response': 'Yes'}\\n\",\n",
       " \"{'prompt': 'i couldnt find any on github', 'response': 'They are under the figures folder in overleaf'}\\n\",\n",
       " \"{'prompt': 'im gonna add heatmaps in the results section so i can reference them in discussion section, the formatting will suck but we can fix it up later (edited)', 'response': 'Yup yup'}\\n\",\n",
       " \"{'prompt': 'what were the hyperparameters u used when training this model?', 'response': 'u can find it in this config. I remember the more general ones like 200 neurons, Lr 1e-3, dropout 0.1 and LSTM model'}\\n\",\n",
       " \"{'prompt': 'yep right after i finish running the temperature expirements', 'response': 'Also I‚Äôll push ester2 to main at 8pm, lmk before that if there‚Äôs any pending code changes'}\\n\",\n",
       " \"{'prompt': 'Also I‚Äôll push ester2 to main at 8pm, lmk before that if there‚Äôs any pending code changes', 'response': 'ok main repo is updated, the pa doc didnt say anything about what to not submit, so i only kept the configs folder and data folder'}\\n\",\n",
       " '{\\'prompt\\': \\'ok main repo is updated, the pa doc didnt say anything about what to not submit, so i only kept the configs folder and data folder\\', \\'response\\': \"would anyone like to proofread the report before we submit? I think it\\'d be nice to have one or two of us go through and standardize our writeup, e.g. past/present tense etc.   I used present tense for most if not all of my parts but i\\'m good with either past or present.\"}\\n',\n",
       " \"{'prompt': 'ok i can', 'response': 'thank you very much!'}\\n\",\n",
       " \"{'prompt': 'im almost done with my part, just generating a bunch of songs with different temperatures to get statistics', 'response': 'yupp all good!'}\\n\",\n",
       " \"{'prompt': 'yupp all good!', 'response': 'oh yeah the PA doc also said to include their music representation generated from '}\\n\",\n",
       " \"{'prompt': 'i uploaded the txt files and the midi files to the overleaf project', 'response': 'ooo were there any notes that the converter flagged as invalid or unrecognized notes?'}\\n\",\n",
       " \"{'prompt': 'the higher the temperature the more errors', 'response': 'AHAHHAHA'}\\n\",\n",
       " \"{'prompt': 'i tried a bunch of generations and at temp = 2 it was like multiple pages of errors', 'response': 'right, as it is more creative (or hallucinates more) haha'}\\n\",\n",
       " \"{'prompt': 'artists on drugs be like', 'response': 'HAHHAHAHAHAH spot on'}\\n\",\n",
       " \"{'prompt': 'Seems reasonable', 'response': 'Yeah it makes sense to me'}\\n\",\n",
       " \"{'prompt': 'Yeah it makes sense to me', 'response': 'Also that‚Äôs a very good observation üôåüèº'}\\n\",\n",
       " \"{'prompt': 'I can do a final read through if we need', 'response': 'Code is ready'}\\n\",\n",
       " \"{'prompt': 'if theres no other changes I can submit', 'response': 'Can someone double check the requirement here? I might have understood it wrongly but it looks like they asked for the ABC notation (which we have) and the music representation/notation (which we don‚Äôt have at the moment)'}\\n\",\n",
       " \"{'prompt': 'give me like 5 min', 'response': 'Okiee! No worries! There‚Äôs still time'}\\n\",\n",
       " \"{'prompt': 'But I guess the txt and midi files should be in the repo then?', 'response': 'Yeah same here üòÖ'}\\n\",\n",
       " \"{'prompt': 'Yeah same here üòÖ', 'response': 'Oh yeah this is the thing that I‚Äôm not sure'}\\n\",\n",
       " \"{'prompt': 'I guess so', 'response': 'Ok I‚Äôll do that'}\\n\",\n",
       " \"{'prompt': 'Do we need any help with the code submission?', 'response': 'All good! Just submitted'}\\n\",\n",
       " \"{'prompt': 'Solved some minor bugs', 'response': 'Are those on main or ester1? Which one should I pull'}\\n\",\n",
       " \"{'prompt': 'Main is the same as Ester 1 right now', 'response': 'Ooh okkk'}\\n\",\n",
       " \"{'prompt': 'Ooh okkk', 'response': 'Okie! Thanks'}\\n\",\n",
       " \"{'prompt': 'Okie! Thanks', 'response': 'I‚Äôll work on the programming part this Saturday, I‚Äôll check everyone‚Äôs progress by then before I start'}\\n\",\n",
       " \"{'prompt': 'TA said: Results.txt is irrelevant Reference this result instead: Baseline(10 epochs): Loss = 192.47; Val Accuracy = 84.21; Test Accuracy = 83.49', 'response': 'Ooo is our loss 377 still considered too high compared to the reference?'}\\n\",\n",
       " \"{'prompt': 'oh oop', 'response': 'oooh okok! anywhere that u suspect is not doing the job properly?'}\\n\",\n",
       " \"{'prompt': 'we can still test different optimizer and scheduler and other hyperparameter tuning', 'response': 'Okie'}\\n\",\n",
       " \"{'prompt': 'cd cse151b251b-wi24-pa4-transformative python main.py --embed-dim 768 --n-epochs 50  ^^ try this', 'response': 'Ah datahub and dsmlp not working for me at the moment. I‚Äôll have to wait and try again in 30minutes or so.'}\\n\",\n",
       " \"{'prompt': 'Ah datahub and dsmlp not working for me at the moment. I‚Äôll have to wait and try again in 30minutes or so.', 'response': '@Ester Tsai @Jeremy Tow roughly how long does it take to run?'}\\n\",\n",
       " '{\\'prompt\\': \\'Feel free to change the number of epochs or add the argument for learning rate and try a higher learning rate\\', \\'response\\': \"ok thanks! it runs just fine but I have to interrupt it to go to a class now haha, I\\'ll run it again tonight and see what it gives.\"}\\n',\n",
       " '{\\'prompt\\': \"ok thanks! it runs just fine but I have to interrupt it to go to a class now haha, I\\'ll run it again tonight and see what it gives.\", \\'response\\': \\'And is this code implemented up till 3i?\\'}\\n',\n",
       " '{\\'prompt\\': \\'And is this code implemented up till 3i?\\', \\'response\\': \"hmmm since yesterday night i\\'ve been having a problem where the terminal won\\'t run at all lol, though it did run in the afternoon when i tested it. Right now, it\\'s just stuck and interrupting does work as well, has anyone faced this problem before? it\\'s the same for me both on datahub and using ssh\"}\\n',\n",
       " \"{'prompt': 'Yes that happens sometimes', 'response': 'interrupting does not work**'}\\n\",\n",
       " \"{'prompt': 'interrupting does not work**', 'response': 'ohhh okay'}\\n\",\n",
       " \"{'prompt': 'ohhh okay', 'response': 'interesting, first time for me, but good to know!'}\\n\",\n",
       " '{\\'prompt\\': \\'Not sure what‚Äôs the solutionü•∫\\', \\'response\\': \"it\\'s okay!! I\\'ll try another node and wait a bit\"}\\n',\n",
       " '{\\'prompt\\': \"it\\'s okay!! I\\'ll try another node and wait a bit\", \\'response\\': \\'thanks Ester!\\'}\\n',\n",
       " \"{'prompt': 'thanks Ester!', 'response': 'also confirmed this. Yesterday when I ran it for loss was 800+ at some epoch < 10, today I ran the same branch and loss is 2706 after 50 epochs'}\\n\",\n",
       " \"{'prompt': 'cuz i don‚Äôt think i have this issue', 'response': 'ok I can try your branch as well'}\\n\",\n",
       " \"{'prompt': 'gc.collect() torch.cuda.empty_cache()', 'response': 'ooo okok'}\\n\",\n",
       " \"{'prompt': 'sheesh ok', 'response': 'Also dropout 0.9 is quite high honestly'}\\n\",\n",
       " \"{'prompt': 'Also dropout 0.9 is quite high honestly', 'response': 'I‚Äôm trying drop-rate 0.1 for 10 epochs'}\\n\",\n",
       " \"{'prompt': 'so it does work, but I have to keep learning rate below 1.5e-4 (1.5e-4 does not work, but 1.1e-4 works, still testing more)', 'response': 'Ooohh'}\\n\",\n",
       " \"{'prompt': 'Ooohh', 'response': 'Validation accuracy is 85.7% and loss is 183 at epoch 6 for drop rate 0.1, everything else the same'}\\n\",\n",
       " \"{'prompt': 'Validation accuracy is 85.7% and loss is 183 at epoch 6 for drop rate 0.1, everything else the same', 'response': 'I think the culprit is the drop rate..? üòÇ'}\\n\",\n",
       " \"{'prompt': 'OK', 'response': 'Yeah ok test accuracy is 86.58%'}\\n\",\n",
       " '{\\'prompt\\': \\'I updated the print statement to be more clear too, so would you guys like to make a branch of ester1 and work on that? (ignore ester2)\\', \\'response\\': \"yes i\\'ll merge with your branch in a bit, working on the custom model right now. Thank you Ester!!\"}\\n',\n",
       " '{\\'prompt\\': \"yes i\\'ll merge with your branch in a bit, working on the custom model right now. Thank you Ester!!\", \\'response\\': \\'Implemented part 4 strategy 1 & 2 from the list of five strategies - I just went with the order of the list   feel free to check the code in my branch winfrey (merged pull from the latest update in ester1 branch)  also feel free to explore the other strategies - we will only need to report 2 out of the 5 strategies\\'}\\n',\n",
       " \"{'prompt': 'Implemented part 4 strategy 1 & 2 from the list of five strategies - I just went with the order of the list   feel free to check the code in my branch winfrey (merged pull from the latest update in ester1 branch)  also feel free to explore the other strategies - we will only need to report 2 out of the 5 strategies', 'response': 'Results can be found in this doc here'}\\n\",\n",
       " \"{'prompt': 'Are you all free to call at 3pm again this Friday to check-in on PA4?', 'response': 'Yes!'}\\n\",\n",
       " '{\\'prompt\\': \"ok yea i\\'ll make it\", \\'response\\': \\'Oh yeaaaah definitely mention this, cause for PA1 both me and Ester worked on some parts that are the same and we said something like we both coded this part and we ended up taking xx‚Äôs version\\'}\\n',\n",
       " '{\\'prompt\\': \"@Jeremy Tow @Jonathan Cheung @Samuel Chu feel free to  build on top of ester1; it has Jeremy\\'s, Winfrey\\'s, and my most updated code\", \\'response\\': \\'I like how we are stacking on top one another‚Äôs üòÇüòÇ\\'}\\n',\n",
       " \"{'prompt': 'I like how we are stacking on top one another‚Äôs üòÇüòÇ', 'response': 'Hello hello, to plan our tasks and time for the rest of this week, is anyone planning to work on or currently working on part 4 or part 5?'}\\n\",\n",
       " \"{'prompt': '@Winfrey Kong would you like to try dropout 0.15 instead of 0.1? it seemed to perform better for me', 'response': 'Sure'}\\n\",\n",
       " \"{'prompt': 'All results are recorded here btw! ', 'response': 'Kong pinned a message.'}\\n\",\n",
       " \"{'prompt': 'Also would anyone like to set up the report sections according to the PA instruction?     ', 'response': 'I will do that'}\\n\",\n",
       " \"{'prompt': 'I will do that', 'response': 'I‚Äôll also work on what is left for Part 4 this Sunday. @Jonathan Cheung @Samuel Chu lmk if you are already working on any of Part 4 (or Part 5) so I can plan my time accordingly.   Also I‚Äôll need help with getting started on the report, if any of you would like to work together with me, I‚Äôd really appreciate that!!'}\\n\",\n",
       " \"{'prompt': 'We‚Äôre pretty good on part 4 since we‚Äôve done methods 1-3. Feel free to try methods 4-5 but we should aim to get part 5 done first!', 'response': 'Yupp agreed!!'}\\n\",\n",
       " \"{'prompt': 'Anyone going to OH tomorrow?', 'response': 'Nope I won‚Äôt be'}\\n\",\n",
       " \"{'prompt': 'Yooooooooooo', 'response': 'Wowwwww'}\\n\",\n",
       " \"{'prompt': '^ if anyone would like to test it', 'response': 'Thank you!'}\\n\",\n",
       " \"{'prompt': 'in fact 50 is the most common label', 'response': 'üëÄ'}\\n\",\n",
       " \"{'prompt': 'Yeah looks like answering this = answer to that question', 'response': 'Or maybe how does the classifier know which label is the most popular ?'}\\n\",\n",
       " \"{'prompt': 'just made another push to use the best model (best val accuracy) instead of final model', 'response': 'Confirmed with TA, kinda what we went over just now. So I guess we just have to figure out why test accuracy is roughly the percentage of the dominant class or is it just coincidence  ‚ÄúBefore fine-tuning means: we use BERT + linear classifier to get our target predictions without any training.‚Äù'}\\n\",\n",
       " \"{'prompt': 'Confirmed with TA, kinda what we went over just now. So I guess we just have to figure out why test accuracy is roughly the percentage of the dominant class or is it just coincidence  ‚ÄúBefore fine-tuning means: we use BERT + linear classifier to get our target predictions without any training.‚Äù', 'response': 'So essentially it goes down to what the linear classifier is predicting?'}\\n\",\n",
       " '{\\'prompt\\': \"good call on using large batch size @Ester Tsai!! I read Lilian Weng\\'s blog that SimCLR does require large batch size to perform well.\", \\'response\\': \"checked the implementation of SimCLR, it looks all good to me (thanks Ester!), I\\'m reading papers on the effect of some hyperparameters on it.\"}\\n',\n",
       " '{\\'prompt\\': \"checked the implementation of SimCLR, it looks all good to me (thanks Ester!), I\\'m reading papers on the effect of some hyperparameters on it.\", \\'response\\': \"if we haven\\'t already tried, we could try lower learning rate for classifier (1e-4)\"}\\n',\n",
       " '{\\'prompt\\': \"if we haven\\'t already tried, we could try lower learning rate for classifier (1e-4)\", \\'response\\': \\'SimCSE is using dropout rate of 0.1, and from the doc I see that Ester has tried that\\'}\\n',\n",
       " \"{'prompt': 'or even different schedulers', 'response': 'okie!'}\\n\",\n",
       " '{\\'prompt\\': \\'okie!\\', \\'response\\': \"@Jeremy Tow also.. I kinda suspect that there\\'s actually only 2 published research papers that use Amazon Massive Intent - maybe that\\'s why the instruction explicitly states that we get full credits for related work as long as we have 2 papers that use this dataset lol  I could be wrong though, just couldn\\'t find any other papers.\"}\\n',\n",
       " '{\\'prompt\\': \"@Jeremy Tow also.. I kinda suspect that there\\'s actually only 2 published research papers that use Amazon Massive Intent - maybe that\\'s why the instruction explicitly states that we get full credits for related work as long as we have 2 papers that use this dataset lol  I could be wrong though, just couldn\\'t find any other papers.\", \\'response\\': \\'paper 1: \\'}\\n',\n",
       " '{\\'prompt\\': \"I\\'m trying SGD optimizer for the contrastive training loop\", \\'response\\': \\'@Ester Tsai just confirming, we are going with custom 1&3 right?\\'}\\n',\n",
       " '{\\'prompt\\': \"yeah let\\'s do that if you guys are down!\", \\'response\\': \\'yeah sounds good to me\\'}\\n',\n",
       " '{\\'prompt\\': \"@Jeremy Tow@Jonathan Cheung@Samuel Chu How\\'s progress?\", \\'response\\': \\'@Ester Tsai do you also get very low contrastive loss? like 0.0\\'}\\n',\n",
       " '{\\'prompt\\': \\'yes\\', \\'response\\': \"I\\'m thinking if it is the classifier that\\'s not learning\"}\\n',\n",
       " '{\\'prompt\\': \"I\\'m thinking if it is the classifier that\\'s not learning\", \\'response\\': \\'i think i figured out SimCLR\\'}\\n',\n",
       " \"{'prompt': 'i think i figured out SimCLR', 'response': 'will get back to yall in 20 mins'}\\n\",\n",
       " '{\\'prompt\\': \\'The batch size doesn‚Äôt seem to have significant difference compared to 256, though I‚Äôm not on gpu 30 and other params are TBD\\', \\'response\\': \"for SimCLR, it\\'s our model architecture. there are two things we might have missed, i am still checking:  1. we need to drop the projection head when passing to the classifier. we still train supconmodel with the projection head, it is only when training the classifier, we want the hidden representations instead of the projection head output  2. we need to apply dropout two times for unsupervised SimCLR. I am still looking at the paper for details of implementation\"}\\n',\n",
       " \"{'prompt': 'my post on piazza: ', 'response': 'ok so we took care of item2. I will work on item1'}\\n\",\n",
       " \"{'prompt': 'I have been experimenting on flipping the normalization and linear head in final_output but don‚Äôt know if it‚Äôs a fluke', 'response': 'Yup yup this sounds right'}\\n\",\n",
       " \"{'prompt': 'I‚Äôm running rn after moving the scheduler update thing into the epoch for loop rather than batch', 'response': 'Yup this too'}\\n\",\n",
       " \"{'prompt': 'Yup this too', 'response': 'Also try removing the cross entropy loss reduction = sum'}\\n\",\n",
       " \"{'prompt': 'Also try removing the cross entropy loss reduction = sum', 'response': 'Just use the default'}\\n\",\n",
       " \"{'prompt': 'After many epochs of 0 it becomes nan and it breaks', 'response': 'classifier or supcon?'}\\n\",\n",
       " \"{'prompt': 'I mean the mean train loss', 'response': 'if you are referring to the mean train loss of supcon (the first training loop), it always reaches 0.0 within 10 epochs for me'}\\n\",\n",
       " \"{'prompt': 'if you are referring to the mean train loss of supcon (the first training loop), it always reaches 0.0 within 10 epochs for me', 'response': 'so i just set it at 10 epochs for contrastive-n-epochs'}\\n\",\n",
       " \"{'prompt': 'so i just set it at 10 epochs for contrastive-n-epochs', 'response': 'also, might want to add a new argument to this'}\\n\",\n",
       " '{\\'prompt\\': \\'It becomes man for em sometimes too, especially when temperature is lower than 0.01\\', \\'response\\': \"because if it is taking classifier input dim, it\\'ll always be 768, and the linear is doing 768 -> 768, which is not really meaningful\"}\\n',\n",
       " '{\\'prompt\\': \"because if it is taking classifier input dim, it\\'ll always be 768, and the linear is doing 768 -> 768, which is not really meaningful\", \\'response\\': \"and classifier input dim has to be 768, since it is taking the encoder\\'s embeddings when we are doing SimCLR\"}\\n',\n",
       " \"{'prompt': 'Yep that‚Äôs right', 'response': 'was able to get 71% val acc, and 88% train acc by epoch 7 of training the classifier, just fixing some minor issues on my end right now. I will run a few more experiments and let yall know how it goes.'}\\n\",\n",
       " \"{'prompt': 'Messed up a line and my test acc became 0.02 üíÄ', 'response': 'hahaha high five, been there, done that, i was debugging for like 2 hours'}\\n\",\n",
       " \"{'prompt': 'Seems to be breaking my code', 'response': 'yup'}\\n\",\n",
       " \"{'prompt': 'yup', 'response': 'lemme push my code to winfrey and maybe you can cross check from there'}\\n\",\n",
       " \"{'prompt': 'lemme push my code to winfrey and maybe you can cross check from there', 'response': 'ok my branch is fully updated'}\\n\",\n",
       " '{\\'prompt\\': \\'ok my branch is fully updated\\', \\'response\\': \"i\\'m mostly testing 2 hyperparameters now 1. supcon-linear-head-dim 2. hidden-dim (of the classifier, because we have been using the default 10, so its going 768 -> 10 -> 60)\"}\\n',\n",
       " \"{'prompt': 'I thought hidden dim isn‚Äôt used for some reason', 'response': 'It‚Äôs a lot of args going on üòÇ'}\\n\",\n",
       " \"{'prompt': 'OOPS', 'response': 'Yeah no worries, I only realized that very much later, the 71% just now is with hidden dim 128 (for Classifier), and sup con linear head dim 128 (for SupConModel)'}\\n\",\n",
       " \"{'prompt': 'the linear head dim can go pretty high, like 3000', 'response': 'True true!'}\\n\",\n",
       " \"{'prompt': 'Oh my goodness this is so very much needed', 'response': 'Thank you'}\\n\",\n",
       " \"{'prompt': 'Thank you', 'response': 'added a new table just for SimCLR in the results google doc'}\\n\",\n",
       " \"{'prompt': 'WAIT', 'response': 'Yes sir'}\\n\",\n",
       " \"{'prompt': 'VAL ACC 0.83 on 3 epochs', 'response': 'Niceeeee'}\\n\",\n",
       " \"{'prompt': 'sheesh?????????', 'response': 'Lesgoooo'}\\n\",\n",
       " \"{'prompt': 'The train acc is getting to 99%', 'response': 'Goodnight everyone'}\\n\",\n",
       " \"{'prompt': 'Goodnight everyone', 'response': 'HAHAHAHAH'}\\n\",\n",
       " \"{'prompt': 'HAHAHAHAH', 'response': 'What‚Äôs the magic parameter'}\\n\",\n",
       " \"{'prompt': 'yuh please send the command line', 'response': 'Okieeeee'}\\n\",\n",
       " '{\\'prompt\\': \"wanna send it rn haha I\\'ll run it along with my other string of commands\", \\'response\\': \\'Oh and push ur code we can just use urs\\'}\\n',\n",
       " \"{'prompt': 'oh yeah ill make a branch bc ive been workignon a copy of esters', 'response': 'Yeaaah okkk'}\\n\",\n",
       " \"{'prompt': 'Yeaaah okkk', 'response': 'Thanks Jonathan!!'}\\n\",\n",
       " '{\\'prompt\\': \"if we removed the reduction=\\'sum\\', it\\'ll be hard to calculate loss consistently\", \\'response\\': \\'Also, very good call on this Ester, val acc is already above 80% at epoch 3 with 1200 linear head dim\\'}\\n',\n",
       " \"{'prompt': 'Also, very good call on this Ester, val acc is already above 80% at epoch 3 with 1200 linear head dim', 'response': 'OHHHH Okok we should put it back then, Soz didn‚Äôt think that far'}\\n\",\n",
       " '{\\'prompt\\': \"haha I tried 10000 once and might\\'ve gotten CUDA out of memeory\", \\'response\\': \\'Pushing the limits we like\\'}\\n',\n",
       " \"{'prompt': '82.92%', 'response': 'Niceeeeee wow I‚Äôm so glad we worked this out together üòçüòçüòç'}\\n\",\n",
       " \"{'prompt': 'YUH thanks to your important fixes XD', 'response': 'I find it easier to do this on GitHub than command line'}\\n\",\n",
       " \"{'prompt': 'I find it easier to do this on GitHub than command line', 'response': '1. Pull request from Ester to ur branch 2. Then only push ur code to ur branch'}\\n\",\n",
       " \"{'prompt': 'not the actual repo on my computer', 'response': 'Well, there‚Äôs also brute force, download the py files and upload it, almost guaranteed to work always ü§£ when there‚Äôs too many merge conflicts I just brute force hahaha'}\\n\",\n",
       " \"{'prompt': 'Aaaaaa', 'response': 'Noiceeee'}\\n\",\n",
       " '{\\'prompt\\': \"I\\'ve at least updated the SupCon and SimCLR plots, but not yet the other ones. I\\'m also down to keep the other ones the same since we\\'ve already written a lot about them. But I also have the new plots after the code fix available if we want to swap the old ones out.\", \\'response\\': \\'yup, i will fill out my parts today.\\'}\\n',\n",
       " '{\\'prompt\\': \\'yup, i will fill out my parts today.\\', \\'response\\': \"yeah, i\\'m down with either.\"}\\n',\n",
       " '{\\'prompt\\': \"yeah, i\\'m down with either.\", \\'response\\': \\'is the acc for the fine tuned models very different from the table we have right now?\\'}\\n',\n",
       " \"{'prompt': 'not that different', 'response': 'i see okay'}\\n\",\n",
       " \"{'prompt': 'i see okay', 'response': 'well.. at least for my part of the discussion, it is easier to discuss that adding fine-tuning strategies improves the model'}\\n\",\n",
       " \"{'prompt': 'well.. at least for my part of the discussion, it is easier to discuss that adding fine-tuning strategies improves the model', 'response': 'but i can do more research'}\\n\",\n",
       " \"{'prompt': 'but i can do more research', 'response': 'if we want, we can embrace the research mindset by reporting the latest results'}\\n\",\n",
       " '{\\'prompt\\': \\'TRUE\\', \\'response\\': \"yeah, i\\'m kind of leaning towards reporting our final and most consistent results\"}\\n',\n",
       " '{\\'prompt\\': \"yeah, i\\'m kind of leaning towards reporting our final and most consistent results\", \\'response\\': \\'do you need help with running any more or you have everything we need already?\\'}\\n',\n",
       " \"{'prompt': 'for custom 3 and custom1and3', 'response': 'just saw that you took SimCLR on the report! thank you thank you'}\\n\",\n",
       " \"{'prompt': 'done with baseline and custom1', 'response': 'okkk i will do that tonight, will build off what you have in the results doc'}\\n\",\n",
       " '{\\'prompt\\': \"I\\'ll put them on Overleaf\", \\'response\\': \\'oooh so custom1 itself is slightly tiny bit better than baseline\\'}\\n',\n",
       " \"{'prompt': 'We need to comment on each plot right', 'response': 'hmmmm.. the exact wording from the doc is summarize the results, but we can always play safe and mention it.'}\\n\",\n",
       " \"{'prompt': 'hmmmm.. the exact wording from the doc is summarize the results, but we can always play safe and mention it.', 'response': 'we are meeting at 7pm this Wednesday for final project, right?'}\\n\",\n",
       " \"{'prompt': 'Yes', 'response': '@Ester Tsai if ester1 branch is the most updated one, custom_train in main.py we gotta move the model.scheduler.step() outside of the batch loop'}\\n\",\n",
       " \"{'prompt': '@Ester Tsai if ester1 branch is the most updated one, custom_train in main.py we gotta move the model.scheduler.step() outside of the batch loop', 'response': 'typo from the starter code that got carried forward oopss'}\\n\",\n",
       " \"{'prompt': 'typo from the starter code that got carried forward oopss', 'response': 'I moved the scheduler step out to the epoch loop, and the best val acc is above this here at epoch 5'}\\n\",\n",
       " \"{'prompt': 'I got good supcon plot', 'response': 'Ooohh I seeee okkk thanks!'}\\n\",\n",
       " \"{'prompt': 'custom1and3 just slightly worse than baseline', 'response': 'Is the scheduler step in the epoch loop when u ran them? Just curious if u have changed it locally'}\\n\",\n",
       " \"{'prompt': 'need to rerun', 'response': 'üò¨üò¨ I can help run a chunk when I get home in like 45 minutes'}\\n\",\n",
       " \"{'prompt': 'üò¨üò¨ I can help run a chunk when I get home in like 45 minutes', 'response': 'Thanks for running so many of them'}\\n\",\n",
       " \"{'prompt': 'Thanks for running so many of them', 'response': 'üò≠üôåüèº'}\\n\",\n",
       " \"{'prompt': 'just quit his OH lol', 'response': 'I went to his before! and I remember the session I went wasn‚Äôt very helpful too'}\\n\",\n",
       " \"{'prompt': 'custom1, custom3, and custom1and3', 'response': 'Ester has a chunk of command lines written in the results doc, you can get some from there and modify as you would like to'}\\n\",\n",
       " \"{'prompt': 'wow lol that little bug fix solved many things', 'response': 'Niceeeee'}\\n\",\n",
       " \"{'prompt': 'Niceeeee', 'response': 'i see your log of code that you will run at 520pm, is there any block that is not in the queue now?'}\\n\",\n",
       " '{\\'prompt\\': \"i\\'ll run the custom3 commands\", \\'response\\': \\'ok ill do custom1\\'}\\n',\n",
       " \"{'prompt': '0.2 drop rate*', 'response': 'okk!'}\\n\",\n",
       " \"{'prompt': 'lmk if anything better comes up! Or I can just put these plots on Overleaf', 'response': 'Nice nice mine is still running and ntg is higher so far'}\\n\",\n",
       " \"{'prompt': '^ yea same', 'response': 'custom1 is on its last one, i am good with going with what you have there. do we still need to run supcon?'}\\n\",\n",
       " '{\\'prompt\\': \\'custom1 is on its last one, i am good with going with what you have there. do we still need to run supcon?\\', \\'response\\': \"^ I\\'m assuming you meant SimCLR\"}\\n',\n",
       " \"{'prompt': 'I‚Äôm ok with calling it good enough haja', 'response': 'yes agreed'}\\n\",\n",
       " \"{'prompt': 'Actually I meant SupCon!', 'response': 'oooo'}\\n\",\n",
       " \"{'prompt': 'And it‚Äôs alright since it‚Äôs above 80%', 'response': 'yeah'}\\n\",\n",
       " \"{'prompt': 'All the plots had been updated!', 'response': 'ok i will do supcon now'}\\n\",\n",
       " \"{'prompt': 'Do you mean running more of supcon or writing about it in report?', 'response': 'oh wait actually'}\\n\",\n",
       " \"{'prompt': 'oh wait actually', 'response': 'none of the supcon uses the custom techniques, could we say that if the discussion asks us to discuss the results?'}\\n\",\n",
       " \"{'prompt': 'Does anyone know what SimCLR stands for lol', 'response': 'A Simple framework for learning Contrastive Learning of Visual Representations'}\\n\",\n",
       " \"{'prompt': 'A Simple framework for learning Contrastive Learning of Visual Representations', 'response': 'literally HAHAH'}\\n\",\n",
       " '{\\'prompt\\': \\'For explaining the plots, remember to \\\\\\\\ref the figure!\\', \\'response\\': \"i\\'m pretty much done with my parts on the report. lmk if any of you need me on any parts, if not I\\'ll wait a bit and see where else I can add to.\"}\\n',\n",
       " '{\\'prompt\\': \"i\\'m pretty much done with my parts on the report. lmk if any of you need me on any parts, if not I\\'ll wait a bit and see where else I can add to.\", \\'response\\': \\'updated github main readme\\'}\\n',\n",
       " \"{'prompt': 'love that', 'response': 'would you like to merge your branch to main @Ester Tsai?'}\\n\",\n",
       " \"{'prompt': 'two percent chance', 'response': 'üòÇüòÇüòÇüòÇ'}\\n\",\n",
       " \"{'prompt': 'Yeah just fogured that out it‚Äôs really cool', 'response': 'Article that we can refer to for this task: '}\\n\",\n",
       " \"{'prompt': 'oh thats a good idea. i think the main thing we would have to figure out would be how to make it different tho', 'response': 'Wdym by different?'}\\n\",\n",
       " \"{'prompt': 'Wdym by different?', 'response': 'Also, just saying that, we are not set set on the idea so feel free to explore other ideas and we can go over all ideas together tomorrow'}\\n\",\n",
       " \"{'prompt': 'Yeah honestly cleaning our chat data is going to take up quite a chunk', 'response': 'But yeah one thing for sure is that there‚Äôs a lot of resources available out there. Also could take a look at how we can make changes to model architecture'}\\n\",\n",
       " \"{'prompt': 'But yeah one thing for sure is that there‚Äôs a lot of resources available out there. Also could take a look at how we can make changes to model architecture', 'response': 'if we build our model sort of from scratch (but with some reference out there), this will give us a lot more flexibility'}\\n\",\n",
       " \"{'prompt': 'oh do we need to do the training? or can we just finetune', 'response': 'the only thing that I am not sure about just plugging gpt is how much flexibility we have in terms of actually training it. Most of the examples that I‚Äôve seen is just calling the model to generate, as in there‚Äôs no trainer loop etc.'}\\n\",\n",
       " \"{'prompt': 'the only thing that I am not sure about just plugging gpt is how much flexibility we have in terms of actually training it. Most of the examples that I‚Äôve seen is just calling the model to generate, as in there‚Äôs no trainer loop etc.', 'response': 'For plugging a pre trained generative model, fine tuning looks like to me is finding the right sequence length, setting the right temperature, trying different pretrained models, top_p, top_k etc  Which we should probably have it as some sort of comparison for evaluation, but if we are only fine tuning, I‚Äôm not sure if that can count as our final project. I can confirm with TA tmr though'}\\n\",\n",
       " \"{'prompt': 'okay', 'response': 'Ooh actually, we can train GPT2! The one u shared'}\\n\",\n",
       " \"{'prompt': 'Ooh actually, we can train GPT2! The one u shared', 'response': 'Kind of like how we train our baseline bert for this round.'}\\n\",\n",
       " \"{'prompt': 'cool ok', 'response': 'just added final project idea notes to our team doc'}\\n\",\n",
       " \"{'prompt': 'ah here is how to get the emoji', 'response': 'So cute'}\\n\",\n",
       " \"{'prompt': 'with very little training', 'response': 'Wow this is promising'}\\n\",\n",
       " \"{'prompt': '1 epoch and only 10 sequences of 30 tokens haha', 'response': 'I seeeee'}\\n\"]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'Winfrey'\n",
    "create_individual_data(groupchat, name, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9f85f6",
   "metadata": {},
   "source": [
    "# Jonathan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c8eb92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"{'prompt': 'I see I see! Okk looks like it‚Äôs the same root library! Thanks for confirming haha', 'response': 'Idk if u guys r getting the same error but I can update my branch from the master I get an error'}\\n\",\n",
       " \"{'prompt': 'Idk if u guys r getting the same error but I can update my branch from the master I get an error', 'response': 'Cant*'}\\n\",\n",
       " \"{'prompt': 'Cant*', 'response': 'So I just downloaded the zip file LOL'}\\n\",\n",
       " \"{'prompt': 'Is it related to authentication?', 'response': 'I‚Äôm guessing it‚Äôs bc the pth model file is large and bc there‚Äôs some byte error'}\\n\",\n",
       " \"{'prompt': 'It shouldn‚Äôt be large tho', 'response': 'ok its fixed'}\\n\",\n",
       " \"{'prompt': 'ok its fixed', 'response': 'its because my wifi sucks'}\\n\",\n",
       " \"{'prompt': 'I haven‚Äôt started anything after UNet. But I‚Äôm open to working on 5b / part 4b image augmentations / part 4c imbalanced class', 'response': 'I‚Äôm down to work on those parts too, but I‚Äôm not available the rest of today and am currently trying to implement the other code myself to make sure I‚Äôm understanding it'}\\n\",\n",
       " \"{'prompt': 'Just did some research for 4c, I should be able to get 4c done today, will keep y‚Äôall posted on my progress', 'response': 'Attempting 5a - will update on how that goes lol'}\\n\",\n",
       " '{\\'prompt\\': \\'Kong voted for \"Methods - Improving on baseline\" and 2 other options in the poll.\\', \\'response\\': \\'You voted for \"Double Check everything before...\" in the poll.\\'}\\n',\n",
       " '{\\'prompt\\': \\'You voted for \"Double Check everything before...\" in the poll.\\', \\'response\\': \\'I can also do experimentation bc I‚Äôm doing 5a\\'}\\n',\n",
       " '{\\'prompt\\': \\'Yayyyy! Thanks!\\', \\'response\\': \\'anyone getting an error like \"RuntimeError: input and target batch or spatial sizes don\\\\\\'t match: target [16, 224, 224], input [16, 21, 256, 256]\" when messing with kernel size?\\'}\\n',\n",
       " '{\\'prompt\\': \\'anyone getting an error like \"RuntimeError: input and target batch or spatial sizes don\\\\\\'t match: target [16, 224, 224], input [16, 21, 256, 256]\" when messing with kernel size?\\', \\'response\\': \\'how do i adjust the fix it?\\'}\\n',\n",
       " \"{'prompt': 'how do i adjust the fix it?', 'response': 'something wrong with the cirterion function'}\\n\",\n",
       " \"{'prompt': 'Did you get this from the line   Loss = criterion(outputs, labels) ?', 'response': 'i used my own code though it has similar sctructure to the 4 questions'}\\n\",\n",
       " \"{'prompt': 'Yes!', 'response': 'Yeah'}\\n\",\n",
       " \"{'prompt': 'Okkk! I‚Äôll probably go too üòÖ', 'response': 'I‚Äôve got something around 0.06 IOU, I‚Äôll do my write up part but if we get something better by tmrw I can update it accordingly'}\\n\",\n",
       " \"{'prompt': 'I‚Äôve got something around 0.06 IOU, I‚Äôll do my write up part but if we get something better by tmrw I can update it accordingly', 'response': 'I did the abstract too'}\\n\",\n",
       " \"{'prompt': 'What we could do is have like 6-10epochs, then mention the complexity and efficiency in the discussion section', 'response': 'im running for 10 epochs'}\\n\",\n",
       " \"{'prompt': 'im running for 10 epochs', 'response': 'so ill report that result'}\\n\",\n",
       " \"{'prompt': '@Jonathan Cheung could you experiment with having other layers too? TA said just changing stride is not big enough architecture change', 'response': 'Yeah'}\\n\",\n",
       " \"{'prompt': 'it trains pretty fast too', 'response': '2 epochs gets training iou of 0.099'}\\n\",\n",
       " \"{'prompt': '2 epochs gets training iou of 0.099', 'response': 'I modified kernel size as well'}\\n\",\n",
       " \"{'prompt': 'Could you see if it gets higher than 0.12?', 'response': '0.11 with leaky relu'}\\n\",\n",
       " \"{'prompt': 'For me to push to main or my branch** whichever is easier for you grab', 'response': 'hows this'}\\n\",\n",
       " \"{'prompt': 'Personally think that IoU greater than 0.07 IoU good enough and above 0.10 is GrEaT ü§£', 'response': 'ok then'}\\n\",\n",
       " \"{'prompt': 'As long as we can justify we made sufficient changes to the architecture in the report and have some discussion, then we r good', 'response': 'i can push the 5a code if thats fine with everyone'}\\n\",\n",
       " \"{'prompt': 'Your branch is ok! I‚Äôll just copy paste for ease lol', 'response': 'pushed'}\\n\",\n",
       " \"{'prompt': 'tysm tysm', 'response': 'Learning rate I believe is 0.001 and the other thing is 0.0005'}\\n\",\n",
       " \"{'prompt': 'Learning rate I believe is 0.001 and the other thing is 0.0005', 'response': 'But I think that‚Äôs default'}\\n\",\n",
       " \"{'prompt': 'Oh then did u use the learning rate scheduler?', 'response': 'I think so?'}\\n\",\n",
       " \"{'prompt': 'Weird yeah ', 'response': 'Ok I‚Äôm back home, is there anything I need or finish up'}\\n\",\n",
       " \"{'prompt': 'Still need to add something about the segmentation visual', 'response': 'I‚Äôm not even sure what to write abt the segmentation part'}\\n\",\n",
       " \"{'prompt': 'I‚Äôm not even sure what to write abt the segmentation part', 'response': 'Like it looks like it‚Äôs much worse than the IOU and accuracy would say'}\\n\",\n",
       " \"{'prompt': 'also i used present tense for everything and some of us used past tense.   we can stick with one for consistency purpose - whoever is checking the entire paper.', 'response': 'I can chnage all the tenses to present'}\\n\",\n",
       " \"{'prompt': 'ok thanks!', 'response': 'Actually not understanding the segmentation visual, r the blobs supposed to look like the shape of the ppl?'}\\n\",\n",
       " '{\\'prompt\\': \\'It might be the old version!\\', \\'response\\': \\'\"More specifically, a constant learning rate can cause the model to get stuck in local optima and result in a slower convergence or sub-optimal solution.\" is this sentence supposed to be referring to local minima?\\'}\\n',\n",
       " '{\\'prompt\\': \\'\"More specifically, a constant learning rate can cause the model to get stuck in local optima and result in a slower convergence or sub-optimal solution.\" is this sentence supposed to be referring to local minima?\\', \\'response\\': \\'rather than \"optima\"\\'}\\n',\n",
       " \"{'prompt': 'Yes, but optima means the same thing right', 'response': 'oh wait it means the same thing lol'}\\n\",\n",
       " \"{'prompt': 'It was 24 min or something earlier', 'response': 'Everything except maybe a sentence or 2 in the last section are present tense, it should be ok because its a building off of the previous experiments?'}\\n\",\n",
       " \"{'prompt': 'Right now discussion starts in between those figures', 'response': 'page breaks?'}\\n\",\n",
       " \"{'prompt': 'so feel free to make new changes haha', 'response': 'is everyhting else finalized?'}\\n\",\n",
       " \"{'prompt': 'tysm tysm', 'response': 'i can submit?'}\\n\",\n",
       " \"{'prompt': 'Ok done!', 'response': 'ok ill usbmit now'}\\n\",\n",
       " \"{'prompt': 'Thank you Jonathan!', 'response': 'ok i added u guys, check if u can see the reprot on gradescope'}\\n\",\n",
       " \"{'prompt': 'I‚Äôll need to resubmit it lolol I made some changes', 'response': 'lel'}\\n\",\n",
       " \"{'prompt': 'lel', 'response': 'i can resubmit it again then, idk if it messes up if theres diff submissions from diff ppl in the ame team'}\\n\",\n",
       " \"{'prompt': 'i can resubmit it again then, idk if it messes up if theres diff submissions from diff ppl in the ame team', 'response': 'lmk when ur finsihed changing'}\\n\",\n",
       " \"{'prompt': 'I‚Äôll resubmit right now just in case', 'response': 'do u want me to do it?'}\\n\",\n",
       " \"{'prompt': 'Please check if it‚Äôs good', 'response': 'nice job'}\\n\",\n",
       " \"{'prompt': 'I‚Äôll be occupied with midterm and capstone until the weekend, so I‚Äôll build off whatever we have by then', 'response': 'I‚Äôve done literally nothing but I‚Äôll try for part 4'}\\n\",\n",
       " \"{'prompt': 'coooools I‚Äôll keep it running in the background while I do other stuffs üòÇ', 'response': 'What is this?'}\\n\",\n",
       " \"{'prompt': 'it removes CUDNN basically', 'response': 'I‚Äôm getting the same error, you fixed it by switching to capstone stuff?'}\\n\",\n",
       " \"{'prompt': 'I ssh from terminal, not datahub', 'response': 'Like using the capstone jupyterhub or ssh ing?'}\\n\",\n",
       " \"{'prompt': 'Like using the capstone jupyterhub or ssh ing?', 'response': 'Ok yeah'}\\n\",\n",
       " \"{'prompt': 'Yuppp!', 'response': '@Ester Tsai r u sshing into capstone and using cpu?'}\\n\",\n",
       " \"{'prompt': 'No, GPU', 'response': 'im getting the same error even on gpu lol'}\\n\",\n",
       " \"{'prompt': 'ok!', 'response': 'I‚Äôll get on soon, in capstone meeting rn'}\\n\",\n",
       " \"{'prompt': 'is the report the most up to date?', 'response': 'I need to do abstract and intro rn'}\\n\",\n",
       " \"{'prompt': 'ok will do', 'response': 'Does the ‚Äú93 unique characters‚Äù refer to all the possible outputs in abc format for each note?'}\\n\",\n",
       " \"{'prompt': '@Jonathan Cheung I uploaded all the heatmaps from our best trained model under the figures folder. Are you able to see them?', 'response': 'Yeah I can see the figures'}\\n\",\n",
       " \"{'prompt': 'Then I‚Äôll put the 3 that u choose to discuss under results', 'response': 'Ok, I‚Äôll choose 3 between the 0-140 heatmaps'}\\n\",\n",
       " \"{'prompt': 'the config to use is config_200_neurons.json', 'response': '60, 140, 100? for the 3 heatmaps? I think I could point out how the highe activations are on A and F for 60, and the repeated high activation on the similar strucutre phrase for 140. 100 im not so sure but probably having to do with the header. Or I could do 20 because the low activation is always on the blank space'}\\n\",\n",
       " \"{'prompt': 'They are under the figures folder in overleaf', 'response': 'im gonna add heatmaps in the results section so i can reference them in discussion section, the formatting will sucj but we can fix it up later'}\\n\",\n",
       " \"{'prompt': 'im gonna add heatmaps in the results section so i can reference them in discussion section, the formatting will sucj but we can fix it up later', 'response': '@Winfrey Kong'}\\n\",\n",
       " \"{'prompt': '@Winfrey Kong', 'response': 'im gonna add heatmaps in the results section so i can reference them in discussion section, the formatting will suck but we can fix it up later (edited)'}\\n\",\n",
       " \"{'prompt': 'Yup yup', 'response': 'Also does red correspond to low or high activation?'}\\n\",\n",
       " \"{'prompt': 'You can see the example in the PA3 instruction', 'response': 'Ok the formatting is fixed and discussion added for the heatmaps. I added a page break so that the loss plots under the hyper parameter tuning sections so the graphs are correctly placed'}\\n\",\n",
       " \"{'prompt': 'Ok the formatting is fixed and discussion added for the heatmaps. I added a page break so that the loss plots under the hyper parameter tuning sections so the graphs are correctly placed', 'response': 'Take a look to check it over'}\\n\",\n",
       " \"{'prompt': 'Also that‚Äôs a very good observation üôåüèº', 'response': 'Are we read to submit?'}\\n\",\n",
       " \"{'prompt': 'Are we read to submit?', 'response': 'I can do a final read through if we need'}\\n\",\n",
       " \"{'prompt': 'Okiee! No worries! There‚Äôs still time', 'response': 'We don‚Äôt necessarily need the screenshots but we need the txt file output and the midi file'}\\n\",\n",
       " \"{'prompt': 'We don‚Äôt necessarily need the screenshots but we need the txt file output and the midi file', 'response': 'In the submission'}\\n\",\n",
       " \"{'prompt': 'i feel like it kinda reads like we need the music representation tho', 'response': 'Ok thats fine'}\\n\",\n",
       " \"{'prompt': 'i just missed it the first time i read it oop', 'response': 'But I guess the txt and midi files should be in the repo then?'}\\n\",\n",
       " \"{'prompt': 'ok i updated the abstract, also added in the pictures of the music', 'response': 'Report submitted, I added u guys'}\\n\",\n",
       " \"{'prompt': 'So are we going with this?', 'response': 'I guess so'}\\n\",\n",
       " \"{'prompt': '^ if anyone wanna help with setting up PA4 report! If not, I can do it in some days', 'response': 'I‚Äôll do more coding this time around, I‚Äôll have less capstone work'}\\n\",\n",
       " '{\\'prompt\\': \"ok my bit is finished. i don\\'t really know about the issue that you guys faced with cuDNN, so I couldn\\'t really write about it\", \\'response\\': \\'I had the incompatibility issue so I couldn‚Äôt code. I don‚Äôt rlly have any other excuse bc I wrote what I did in the report. If that means I get 70% so be it, I am P/NP anyways\\'}\\n',\n",
       " '{\\'prompt\\': \"It\\'s also helpful if anyone wants to try Stochastic Weight Averaging (SWA) and Frequent Evaluation for Part 4\", \\'response\\': \\'I‚Äôll try\\'}\\n',\n",
       " \"{'prompt': 'I‚Äôll also work on what is left for Part 4 this Sunday. @Jonathan Cheung @Samuel Chu lmk if you are already working on any of Part 4 (or Part 5) so I can plan my time accordingly.   Also I‚Äôll need help with getting started on the report, if any of you would like to work together with me, I‚Äôd really appreciate that!!', 'response': 'I haven‚Äôt rlly looked but I can do part 5'}\\n\",\n",
       " \"{'prompt': 'So we can try 20', 'response': 'ill run it'}\\n\",\n",
       " \"{'prompt': 'rip no nodes avialble rn', 'response': 'nvm its working now'}\\n\",\n",
       " \"{'prompt': 'nvm its working now', 'response': 'just finsihed running with 20 epochs, test acc is 0.5047'}\\n\",\n",
       " \"{'prompt': 'or by tomorrow before 3pm', 'response': 'Yeah I‚Äôll do so'}\\n\",\n",
       " \"{'prompt': 'Could you try different parameters?', 'response': 'Yeah'}\\n\",\n",
       " \"{'prompt': 'will get back to yall in 20 mins', 'response': 'Removing the scheduler yields 61% without learning rate modifications'}\\n\",\n",
       " \"{'prompt': 'Removing the scheduler yields 61% without learning rate modifications', 'response': 'The batch size doesn‚Äôt seem to have significant difference compared to 256, though I‚Äôm not on gpu 30 and other params are TBD'}\\n\",\n",
       " \"{'prompt': 'I see! That makes so much sense!', 'response': 'I believe I‚Äôve successfully done this and got 0.692'}\\n\",\n",
       " \"{'prompt': 'I believe I‚Äôve successfully done this and got 0.692', 'response': 'Don‚Äôt know if Params matter as much though training acc becomes much higher at 0.78'}\\n\",\n",
       " \"{'prompt': 'Don‚Äôt know if Params matter as much though training acc becomes much higher at 0.78', 'response': 'By making forward pass return l2 and the final thing, and taking the l2 into the classifier on the eval step'}\\n\",\n",
       " \"{'prompt': 'By making forward pass return l2 and the final thing, and taking the l2 into the classifier on the eval step', 'response': 'I have been experimenting on flipping the normalization and linear head in final_output but don‚Äôt know if it‚Äôs a fluke'}\\n\",\n",
       " \"{'prompt': 'Yup yup this sounds right', 'response': 'I‚Äôm running rn after moving the scheduler update thing into the epoch for loop rather than batch'}\\n\",\n",
       " \"{'prompt': 'Just use the default', 'response': 'Why does the loss sometimes become nan'}\\n\",\n",
       " \"{'prompt': 'Why does the loss sometimes become nan', 'response': 'After many epochs of 0 it becomes nan and it breaks'}\\n\",\n",
       " \"{'prompt': 'classifier or supcon?', 'response': 'I mean the mean train loss'}\\n\",\n",
       " \"{'prompt': 'was able to get 71% val acc, and 88% train acc by epoch 7 of training the classifier, just fixing some minor issues on my end right now. I will run a few more experiments and let yall know how it goes.', 'response': 'Messed up a line and my test acc became 0.02 üíÄ'}\\n\",\n",
       " '{\\'prompt\\': \"that\\'s great!\", \\'response\\': \\'Were u able to move the scheduler step out of the batch loop?\\'}\\n',\n",
       " \"{'prompt': 'Were u able to move the scheduler step out of the batch loop?', 'response': 'Seems to be breaking my code'}\\n\",\n",
       " \"{'prompt': 'added a new table just for SimCLR in the results google doc', 'response': 'WAIT'}\\n\",\n",
       " \"{'prompt': 'WAIT', 'response': 'WAIT'}\\n\",\n",
       " \"{'prompt': 'Yes sir', 'response': 'VAL ACC 0.83 on 3 epochs'}\\n\",\n",
       " \"{'prompt': 'Lesgoooo', 'response': 'The train acc is getting to 99%'}\\n\",\n",
       " \"{'prompt': 'What‚Äôs the magic parameter', 'response': 'Let me finish the training'}\\n\",\n",
       " \"{'prompt': 'Okieeeee', 'response': 'It‚Äôs in the process but should take less than the usual 40'}\\n\",\n",
       " \"{'prompt': 'Is ur classifier-input-dim actually the classifier‚Äôs input dim?', 'response': 'oh yeah ill make a branch bc ive been workignon a copy of esters'}\\n\",\n",
       " '{\\'prompt\\': \\'Thanks Jonathan!!\\', \\'response\\': \\'i modifed this line\"classifier = Classifier(args, args.embed_dim, target_size).to(device)\"\\'}\\n',\n",
       " '{\\'prompt\\': \\'i modifed this line\"classifier = Classifier(args, args.embed_dim, target_size).to(device)\"\\', \\'response\\': \\'shoudl be  line ~298 in main.py\\'}\\n',\n",
       " \"{'prompt': 'shoudl be  line ~298 in main.py', 'response': 'bc i was gettign errors from reshaping'}\\n\",\n",
       " \"{'prompt': 'bc i was gettign errors from reshaping', 'response': 'though idk if its sketch'}\\n\",\n",
       " \"{'prompt': 'args.embed_dim will have to be 768', 'response': 'it was modifed to that, it was args.classifier_input_dim'}\\n\",\n",
       " \"{'prompt': 'it was modifed to that, it was args.classifier_input_dim', 'response': 'caused an error bc 256x3500 can do matmul wiht 768x10'}\\n\",\n",
       " \"{'prompt': 'caused an error bc 256x3500 can do matmul wiht 768x10', 'response': 'or 128 in this case'}\\n\",\n",
       " \"{'prompt': 'Niceeeeee wow I‚Äôm so glad we worked this out together üòçüòçüòç', 'response': 'i made my branch but it looks like ester branch from a week ago, im a brnach noob does anyone know how to update it'}\\n\",\n",
       " \"{'prompt': '1. Pull request from Ester to ur branch 2. Then only push ur code to ur branch', 'response': 'oh wait im so dumb'}\\n\",\n",
       " \"{'prompt': 'oh wait im so dumb', 'response': 'im wokring on datahub lol'}\\n\",\n",
       " \"{'prompt': 'im wokring on datahub lol', 'response': 'not the actual repo on my computer'}\\n\",\n",
       " \"{'prompt': 'let me do that rn', 'response': 'ok i pshed to my branch'}\\n\",\n",
       " \"{'prompt': 'ok i pshed to my branch', 'response': 'the comments and stuff r still there so its super messy but u hsould be able to see the changes'}\\n\",\n",
       " \"{'prompt': 'Are we confirmed using that for the report?', 'response': 'I can fill in the values in the chart and in the abstract then'}\\n\",\n",
       " \"{'prompt': 'Should we keep our old set of plots to save time or hyperparameter tune all the models until they are optimized?', 'response': 'I‚Äôm fine with keeping out old set'}\\n\",\n",
       " '{\\'prompt\\': \"to summarize: I\\'ve updated the corresponding report parts for baseline, custom1, and SimCLR. Still waiting on some more SupCon to run for better consistency. Need help on hyperparameter tuning and updating the report for custom3 and custom1and3\", \\'response\\': \\'Do we hv numbers for technique 2 and the combined ones or is it being rerun\\'}\\n',\n",
       " \"{'prompt': 'These are the current best stats but it is being rerun', 'response': 'Also did using the losses rather than the 2 techniques trading faster or was there no significant difference?'}\\n\",\n",
       " \"{'prompt': 'Wdym by using the losses?', 'response': 'Clr and supcon'}\\n\",\n",
       " \"{'prompt': 'Clr and supcon', 'response': 'Like do those model train faster in any significant way?'}\\n\",\n",
       " '{\\'prompt\\': \\'Ester1 should be updated\\', \\'response\\': \\'For this block in the result section: \"For the second technique, we initialize the weights of the last layer in BERT before fine-tuning it.95 Both training and validation accuracies increase very rapidly for the first 4 epochs. The training96 accuracy continues to improve, but the validation accuracy plateaus at around 86%, showing signs of97 overfitting. The final test accuracy is 86.28%. The training stops early at epoch 17 after the validation98 accuracy decreases for 3 epochs in a row. The best model based on the highest validation accuracy99 occurs at epoch 14.100 NEEEEEEEEEEEEED UPDATE!!!!!!!!!!!!!!!!!!!! \" do we still need this or is it jsut misplaced section??\\'}\\n',\n",
       " '{\\'prompt\\': \\'For this block in the result section: \"For the second technique, we initialize the weights of the last layer in BERT before fine-tuning it.95 Both training and validation accuracies increase very rapidly for the first 4 epochs. The training96 accuracy continues to improve, but the validation accuracy plateaus at around 86%, showing signs of97 overfitting. The final test accuracy is 86.28%. The training stops early at epoch 17 after the validation98 accuracy decreases for 3 epochs in a row. The best model based on the highest validation accuracy99 occurs at epoch 14.100 NEEEEEEEEEEEEED UPDATE!!!!!!!!!!!!!!!!!!!! \" do we still need this or is it jsut misplaced section??\\', \\'response\\': \\'like do we need to commnet on each graph or should this be put in the technique 2 section\\'}\\n',\n",
       " \"{'prompt': 'none of the supcon uses the custom techniques, could we say that if the discussion asks us to discuss the results?', 'response': 'I‚Äôm updating the CLR chart and doing the description of the plot - do we have updated params and values for that?'}\\n\",\n",
       " \"{'prompt': 'I‚Äôm updating the CLR chart and doing the description of the plot - do we have updated params and values for that?', 'response': 'Also abstract and intro r up to date'}\\n\",\n",
       " \"{'prompt': 'The table and plot are updated!', 'response': 'Oh wait I misread the graph lol'}\\n\",\n",
       " \"{'prompt': 'oh okok', 'response': 'Jeremy what‚Äôs the line you‚Äôre running'}\\n\",\n",
       " \"{'prompt': 'Wow is mine just too cracked of a seed, somehow the two most popular labels happen to be guessed over and over', 'response': 'I think my code broke or something my baseline has gets acc of 0.8662'}\\n\",\n",
       " \"{'prompt': 'How about Before you run training?', 'response': 'Might have switched up a paean to use a diff model lol'}\\n\",\n",
       " \"{'prompt': 'This is so goofy', 'response': 'I‚Äôll rewrite Q2 to say they match'}\\n\",\n",
       " \"{'prompt': 'I‚Äôll rewrite Q2 to say they match', 'response': 'Wait'}\\n\",\n",
       " \"{'prompt': 'Wait', 'response': 'Not exactly but same order of Magnitude?? I guess'}\\n\",\n",
       " \"{'prompt': 'Not exactly but same order of Magnitude?? I guess', 'response': '1/60 is 1.67% while out test acc is about 0.98%'}\\n\",\n",
       " \"{'prompt': '1/60 is 1.67% while out test acc is about 0.98%', 'response': 'So close enough??'}\\n\",\n",
       " \"{'prompt': 'over 100 different seeds', 'response': 'I‚Äôll out that in Q2'}\\n\",\n",
       " \"{'prompt': 'I‚Äôll out that in Q2', 'response': 'The chart I‚Äôll put the mean number in'}\\n\",\n",
       " \"{'prompt': 'The chart I‚Äôll put the mean number in', 'response': 'Ok updated'}\\n\",\n",
       " \"{'prompt': 'Ok updated', 'response': 'Ready for submit?'}\\n\",\n",
       " \"{'prompt': 'for my parts yea', 'response': 'I‚Äôm gonna submit for now, if there‚Äôs any more changes lmk'}\\n\",\n",
       " '{\\'prompt\\': \"It\\'s pretty easy to get Messenger chat data from Facebook\", \\'response\\': \\'Are you able to download a specific group chat with the date range? Or does it download everything in the same interval only?\\'}\\n',\n",
       " \"{'prompt': 'I can download all of my chat data for a specific time range. Every chat is a different file', 'response': 'Yeah just fogured that out it‚Äôs really cool'}\\n\",\n",
       " \"{'prompt': 'I seeeee', 'response': 'The addition of the emoji in the response is so funny'}\\n\",\n",
       " \"{'prompt': 'lol ya', 'response': 'Spam with what lol'}\\n\",\n",
       " \"{'prompt': 'Spam with what lol', 'response': 'Emojis??? üò≥üò≥üò≥üò≥üò≥üò≥'}\\n\",\n",
       " '{\\'prompt\\': \"Can one of you guys download your facebook messenger data and upload the json file for this groupchat to the github repo? I can\\'t download the 3 month one for some reason\", \\'response\\': \\'I‚Äôll try\\'}\\n']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'Jonathan'\n",
    "create_individual_data(groupchat, name, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566bb957",
   "metadata": {},
   "source": [
    "# Samuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ba4c6e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\\'prompt\\': \\'I‚Äôve done literally nothing but I‚Äôll try for part 4\\', \\'response\\': \"I also haven\\'t done anything but I\\'ll start today\"}\\n',\n",
       " \"{'prompt': 'O yeah small typo', 'response': 'okay'}\\n\",\n",
       " \"{'prompt': 'It‚Äôs config[‚Äúmodel_type‚Äù]', 'response': 'okay thaks'}\\n\",\n",
       " \"{'prompt': 'okay thaks', 'response': 'i got the same error that jonathan got and you were getting earlier'}\\n\",\n",
       " \"{'prompt': 'i got the same error that jonathan got and you were getting earlier', 'response': 'using datahub tho. i could try ssh'}\\n\",\n",
       " \"{'prompt': 'Piazza someone said they created a new conda environment and installed pytorch and it worked', 'response': 'oh okay. ill try that'}\\n\",\n",
       " \"{'prompt': 'oh okay. ill try that', 'response': 'lol im getting a new error after using a conda env:'}\\n\",\n",
       " '{\\'prompt\\': \\'ok I pushed latest changes!\\', \\'response\\': \"i tried ssh\\'ing into capstone gpu but that gave me the same error also\"}\\n',\n",
       " \"{'prompt': 'To clarify the most updated branch is ester2! Ignore the original Ester', 'response': '@Winfrey Kong do you think you help me fix the gpu error i‚Äôm having with capstone gpu later today?'}\\n\",\n",
       " \"{'prompt': 'Is 8.30 ok?', 'response': 'hmm i might not be home til 9-9:15üò¨'}\\n\",\n",
       " \"{'prompt': 'hmm i might not be home til 9-9:15üò¨', 'response': 'would 9:15 work?'}\\n\",\n",
       " \"{'prompt': 'Okiee', 'response': 'thanks'}\\n\",\n",
       " '{\\'prompt\\': \\'thanks\\', \\'response\\': \"Samuel Chu is inviting you to a scheduled Zoom meeting.  Topic: Samuel Chu\\'s Personal Meeting Room  Join Zoom Meeting \"}\\n',\n",
       " \"{'prompt': 'Okie coming', 'response': 'do you want to set a new meeting?'}\\n\",\n",
       " \"{'prompt': 'Okok! You could still write the report using the plot results on my branch if needed üëåüëå', 'response': 'okay is there stuff that you tested that is not on the report already?'}\\n\",\n",
       " \"{'prompt': 'Now running config_250_neurons and the RNN next if it‚Äôs not done already', 'response': 'im kind of confused on temperature can affect how deterministic the model is. bc if we run these values: 5, 10, 15 though a softmax no matter what we divide each element by, when we pick the element with the highest softmax value it will always be the 2nd index(15) right?'}\\n\",\n",
       " \"{'prompt': 'Torch.multinomial randomly selects a character to output', 'response': 'ohhh'}\\n\",\n",
       " \"{'prompt': 'ohhh', 'response': 'ok'}\\n\",\n",
       " \"{'prompt': 'overfitting on training', 'response': 'what does increasing the number of neurons do in the lstm? cuz i thought the lstm just used gates'}\\n\",\n",
       " \"{'prompt': 'what does increasing the number of neurons do in the lstm? cuz i thought the lstm just used gates', 'response': 'does it increase the amount of information that can be stored in the cell state or something?'}\\n\",\n",
       " \"{'prompt': 'For this PA, neurons = number of features in hidden state', 'response': 'ok'}\\n\",\n",
       " \"{'prompt': 'ok', 'response': 'hidden state = cell state?'}\\n\",\n",
       " \"{'prompt': 'Our LSTM is a single layer one, so it‚Äôs only one LSTM', 'response': 'oh really'}\\n\",\n",
       " \"{'prompt': 'oh really', 'response': 'ok'}\\n\",\n",
       " \"{'prompt': 'In RNN, there‚Äôs only hidden state, no cell state', 'response': 'oh okay'}\\n\",\n",
       " \"{'prompt': 'oh okay', 'response': 'thanks'}\\n\",\n",
       " \"{'prompt': 'We will need to describe it in the methods section', 'response': 'ok'}\\n\",\n",
       " \"{'prompt': 'ok', 'response': 'is it always 30?'}\\n\",\n",
       " \"{'prompt': 'It‚Äôs always 50', 'response': 'has anyone generated plots for the bottom 3 rows?'}\\n\",\n",
       " \"{'prompt': 'mb simult many in many out doesnt work for translation bc u kinda need to know the full sentence before u can translate it', 'response': 'btw i think im done with my part too'}\\n\",\n",
       " '{\\'prompt\\': \"would anyone like to proofread the report before we submit? I think it\\'d be nice to have one or two of us go through and standardize our writeup, e.g. past/present tense etc.   I used present tense for most if not all of my parts but i\\'m good with either past or present.\", \\'response\\': \\'ok i can\\'}\\n',\n",
       " \"{'prompt': 'Not me laughing out loud', 'response': 'okay i proofread the report'}\\n\",\n",
       " \"{'prompt': 'oh btw @Samuel Chu do you have anything to add to the regrade request? if not should I submit it?', 'response': 'i was going to try to do part 4 or 5 also'}\\n\",\n",
       " \"{'prompt': 'i was going to try to do part 4 or 5 also', 'response': 'okay i‚Äôll add to it'}\\n\",\n",
       " \"{'prompt': 'oh right i forgot to mention this but im prob going to submit the regrade request later today', 'response': 'label 50 is calendar set'}\\n\",\n",
       " \"{'prompt': 'label 50 is calendar set', 'response': 'no idea how it knows tho'}\\n\",\n",
       " \"{'prompt': 'no idea how it knows tho', 'response': 'are you that you aren‚Äôt training it at all'}\\n\",\n",
       " \"{'prompt': 'Are you able to download a specific group chat with the date range? Or does it download everything in the same interval only?', 'response': 'we could use gpt model too maybe'}\\n\",\n",
       " \"{'prompt': 'yeah i was just thinking we could finetune a gpt model like this ', 'response': 'oh do we need to do the training? or can we just finetune'}\\n\",\n",
       " \"{'prompt': 'For plugging a pre trained generative model, fine tuning looks like to me is finding the right sequence length, setting the right temperature, trying different pretrained models, top_p, top_k etc  Which we should probably have it as some sort of comparison for evaluation, but if we are only fine tuning, I‚Äôm not sure if that can count as our final project. I can confirm with TA tmr though', 'response': 'ahh i see'}\\n\",\n",
       " \"{'prompt': 'ahh i see', 'response': 'okay'}\\n\",\n",
       " \"{'prompt': 'Kind of like how we train our baseline bert for this round.', 'response': 'cool ok'}\\n\",\n",
       " \"{'prompt': 'you can spam the chat right now for data haha', 'response': 'lol ya'}\\n\",\n",
       " \"{'prompt': 'keep sending texts and we will have better data üò≥üò≥', 'response': 'üòÇ'}\\n\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'Samuel'\n",
    "create_individual_data(groupchat, name, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6429a265",
   "metadata": {},
   "source": [
    "# Jeremy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e7045f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"{'prompt': 'What should our group name be üòé Ester and I used transformers for PA1', 'response': 'lol anything works'}\\n\",\n",
       " \"{'prompt': 'lol anything works', 'response': 'we could be incredibly original and use our names'}\\n\",\n",
       " \"{'prompt': 'OOOH I seee', 'response': 'about the same for me i‚Äôm also open to working on any of it'}\\n\",\n",
       " \"{'prompt': 'Has anyone done 5b yet? If not, I can look into it', 'response': 'i‚Äôm going to look into 5b in a bit'}\\n\",\n",
       " \"{'prompt': 'Oh yeah @Jeremy Tow @Jonathan Cheung when training the models for part 5, id recommend using the current train code implemented up till 4c to check if both IoU and accuracy increase.  Baseline + enhancements up till 4c gives IoU around 0.065 to 0.067 and accuracy 0.72 to 0.74  I saw a piazza post that they expect our models in part 5 to give better iou and accuracy', 'response': 'okok'}\\n\",\n",
       " \"{'prompt': 'okok', 'response': 'will do'}\\n\",\n",
       " \"{'prompt': 'rn transfer learning with 5b is giving iou 0.071 and pixel acc 0.737 with the old train code', 'response': 'im going to test with the new train code rn'}\\n\",\n",
       " '{\\'prompt\\': \\'im going to test with the new train code rn\\', \\'response\\': \"just tried 5b with @Winfrey Kong\\'s train_4_c, it took 15 minutes to run and the pixel acc was 0.79 with an iou of 0.2\"}\\n',\n",
       " \"{'prompt': 'rly good numbers', 'response': 'oh btw where / how are we meeting tmr'}\\n\",\n",
       " \"{'prompt': 'oh btw where / how are we meeting tmr', 'response': 'or ig today'}\\n\",\n",
       " '{\\'prompt\\': \"@Jeremy Tow could you push your train_5_b code sometime today when you\\'re done?\", \\'response\\': \\'yep sounds good\\'}\\n',\n",
       " '{\\'prompt\\': \\'yep sounds good\\', \\'response\\': \"its literally just winfrey\\'s train_4_c code lol\"}\\n',\n",
       " '{\\'prompt\\': \"its literally just winfrey\\'s train_4_c code lol\", \\'response\\': \\'i only modified the basic_fcn file\\'}\\n',\n",
       " '{\\'prompt\\': \\'Ester do u plan to push ur code to master? The seed and plot that u added to each train file\\', \\'response\\': \"i think i\\'ll just push the fasic_fcn file to my branch and you can pull it from there?\"}\\n',\n",
       " '{\\'prompt\\': \"i think i\\'ll just push the fasic_fcn file to my branch and you can pull it from there?\", \\'response\\': \\'idk what the easiest way to do this is\\'}\\n',\n",
       " \"{'prompt': 'sure I can try!', 'response': 'ok I just pushed to my branch'}\\n\",\n",
       " \"{'prompt': '@Jeremy Tow if you have extra time, could you double check the architecture for UNet? For some reason it isn‚Äôt doing well, we got IoU 0.0556 and accuracy 0.75', 'response': 'yea fs i‚Äôll check it in a bit'}\\n\",\n",
       " \"{'prompt': 'Oohhh might be cause there are too many people. I had to wait for quite a bit until I get assign a node (I used ssh)', 'response': 'if u need to i can give u access to my server'}\\n\",\n",
       " \"{'prompt': 'I have zero prior knowledge about IoU though, just purely based on articles that I read they put it in decimal', 'response': 'i think it makes more sense in decimal'}\\n\",\n",
       " \"{'prompt': 'Sure!', 'response': 'ig its like the intersection is x percent of the union?'}\\n\",\n",
       " \"{'prompt': 'ig its like the intersection is x percent of the union?', 'response': 'but idk'}\\n\",\n",
       " \"{'prompt': 'Hmm set seed is actually not working fully. The results are different after rerunning', 'response': 'yea i was running into this too'}\\n\",\n",
       " \"{'prompt': 'yea i was running into this too', 'response': 'i thought urs would work cuz u included the cuda bit but mb there‚Äôs another random source we‚Äôre not considering'}\\n\",\n",
       " \"{'prompt': 'i thought urs would work cuz u included the cuda bit but mb there‚Äôs another random source we‚Äôre not considering', 'response': 'like source of randomness lol'}\\n\",\n",
       " \"{'prompt': 'yeah the angles come out deterministic', 'response': 'i tried doing the batch size of 1 thing mentioned in the unet paper and im getting better pixel acc of 0.73 but worse iou of 0.05432'}\\n\",\n",
       " '{\\'prompt\\': \\'I‚Äôm thinking - when batch size is 1, we will need a lot more epochs. How many epochs did you train on just now?\\', \\'response\\': \\'when i did a batch size of 1 i augmented the training data so that each \"img\" is actually a composite of 16 images, so the amount of epochs needed is the same??\\'}\\n',\n",
       " \"{'prompt': 'In this case, is the backward based on one image or 16 images?', 'response': '16 i think'}\\n\",\n",
       " \"{'prompt': '16 i think', 'response': 'well like technically its 1 image that is a composite of 16 imgs'}\\n\",\n",
       " \"{'prompt': 'Yeah, like 15 different transformations of the same image + the original?', 'response': 'umm i think its 16 different images'}\\n\",\n",
       " \"{'prompt': 'Oh as in, one image is segmented into 16 pieces?', 'response': 'yea'}\\n\",\n",
       " \"{'prompt': 'yea', 'response': 'so 1 image is like a 4x4 grid represnting 16 imgs'}\\n\",\n",
       " \"{'prompt': 'so 1 image is like a 4x4 grid represnting 16 imgs', 'response': 'i tried sgd as well and its worse'}\\n\",\n",
       " \"{'prompt': 'Maybe I can run it on your branch? And see if I get the same', 'response': 'yea give me a bit to make it suitable for human viewing first lol'}\\n\",\n",
       " \"{'prompt': 'Yeah take ur time!', 'response': 'ok i just pushed its under jeremy-unet, the code is still vv scuffed but its kinda legible now. mainly i moved the batch_size argument to the voc, and i use the voc to combine the images. as a result batch_size has to be a perfect square'}\\n\",\n",
       " \"{'prompt': 'ok i just pushed its under jeremy-unet, the code is still vv scuffed but its kinda legible now. mainly i moved the batch_size argument to the voc, and i use the voc to combine the images. as a result batch_size has to be a perfect square', 'response': 'also idk why yet but im consistently getting iou of 0.49 now'}\\n\",\n",
       " \"{'prompt': 'For UNet ?', 'response': 'yea'}\\n\",\n",
       " \"{'prompt': 'Ok I‚Äôll look into it in the morning ü§ì thanksss!!', 'response': 'just tried using batch sizes of 4 in the dataloader and 4 in the voc, validation in each epoch was giving good numbers but the final test iou was not as good'}\\n\",\n",
       " \"{'prompt': 'I can help cross check the architecture table when you are done @Jeremy Tow @Jonathan Cheung   @Jeremy Tow do you plan to add ResNet under related works? Just checking!', 'response': 'i‚Äôll add the paper in'}\\n\",\n",
       " \"{'prompt': 'Hmm I think I did implement it according to the TA‚Äôs description', 'response': 'isn‚Äôt the iou function supposed to be of a single prediction and target'}\\n\",\n",
       " \"{'prompt': 'isn‚Äôt the iou function supposed to be of a single prediction and target', 'response': 'and then u average it in the train file'}\\n\",\n",
       " \"{'prompt': 'I‚Äôll aim to be done by 9:30pm but', 'response': 'oh ok i‚Äôll start working on it rn'}\\n\",\n",
       " \"{'prompt': 'thankies!', 'response': 'tysm tysm'}\\n\",\n",
       " \"{'prompt': 'Quick question, is there supposed to be a separate model Python file for 5a and 5b?', 'response': 'oh shoot i didn‚Äôt make a sepearte train file for 5b'}\\n\",\n",
       " \"{'prompt': 'oh shoot i didn‚Äôt make a sepearte train file for 5b', 'response': 'so it kinda overwrites the 4c one'}\\n\",\n",
       " \"{'prompt': 'Like it looks like it‚Äôs much worse than the IOU and accuracy would say', 'response': 'i just added the resnet paper to related work'}\\n\",\n",
       " \"{'prompt': 'i just added the resnet paper to related work', 'response': 'and i finished the architecture table for resnet'}\\n\",\n",
       " \"{'prompt': '@Jeremy Tow there are three layers numbered 31, is that intentional?', 'response': 'oh shoot good catch'}\\n\",\n",
       " \"{'prompt': 'oh shoot good catch', 'response': 'fixed ty'}\\n\",\n",
       " \"{'prompt': '@Jeremy Tow could you check the code submission? I am not sure if the transfer learning file is actually reflecting the architecture in the report', 'response': 'ok yea looking rn'}\\n\",\n",
       " \"{'prompt': 'oh wait it means the same thing lol', 'response': 'just checked, it looks good'}\\n\",\n",
       " \"{'prompt': 'thanks for confirming!', 'response': 'yea the encoder is rollled up into the forward section of the FCN model'}\\n\",\n",
       " \"{'prompt': 'good work!', 'response': 'tysm tysm'}\\n\",\n",
       " \"{'prompt': 'YAY', 'response': 'tytyty'}\\n\",\n",
       " \"{'prompt': 'Okie I can check with you tonight when I start!', 'response': 'i‚Äôm prob going to start working tonight or tmr, if i do start today i‚Äôll lyk'}\\n\",\n",
       " '{\\'prompt\\': \"I also haven\\'t done anything but I\\'ll start today\", \\'response\\': \\'little bit, just slowly working through\\'}\\n',\n",
       " \"{'prompt': 'Ooohhh I seeeee', 'response': 'i‚Äôm curious what gpu it is lol'}\\n\",\n",
       " \"{'prompt': 'might be helpful to make a branch from my branch', 'response': 'ur so amazing'}\\n\",\n",
       " \"{'prompt': 'what time works for you?', 'response': 'like in 5 mknjtes'}\\n\",\n",
       " \"{'prompt': 'I think we can go ahead with sequence length of 70, and the training speed is acceptable', 'response': 'i trained with a sequence length of 300 and let it run the whole day, i‚Äôll let you guys know the loss after i get back to my computer'}\\n\",\n",
       " \"{'prompt': 'I‚Äôm thinking if it‚Äôs overfitting', 'response': 'yea :('}\\n\",\n",
       " \"{'prompt': 'Just very very slightly higher', 'response': 'def not worth doing'}\\n\",\n",
       " \"{'prompt': 'Would you like to try sequence length of 50? to make sure my GPU is not overly optimistic haha', 'response': 'ok yea fs let me do a run rn'}\\n\",\n",
       " \"{'prompt': 'Okkk', 'response': 'oh how many neurons are u using'}\\n\",\n",
       " \"{'prompt': 'Epochs 260', 'response': 'oh wait no im stupid i ran it with 30 instead of 200 i forgot to update the config file fully uhhhhhhh'}\\n\",\n",
       " \"{'prompt': 'oh wait no im stupid i ran it with 30 instead of 200 i forgot to update the config file fully uhhhhhhh', 'response': 'i will run it again lol'}\\n\",\n",
       " '{\\'prompt\\': \\'i will run it again lol\\', \\'response\\': \"i\\'ll run both 200 and 50 again and report back\"}\\n',\n",
       " '{\\'prompt\\': \"i\\'ll run both 200 and 50 again and report back\", \\'response\\': \\'skulling\\'}\\n',\n",
       " \"{'prompt': 'And I‚Äôll generate the plots for hyperparamter tuning using sequence length of 50 and all else constant except for the specific hyperparamter , I‚Äôll take the neurons config, and would you like to take the dropout configs?', 'response': 'okok'}\\n\",\n",
       " \"{'prompt': 'okok', 'response': 'do i even try 200 lol the training time is obscene'}\\n\",\n",
       " \"{'prompt': 'I mean.. only if u want to keep it running overnight üòÇ', 'response': 'like fully 4 minutes per epoch i think'}\\n\",\n",
       " \"{'prompt': 'I tried 150 and it got too miserable at one point I forgot how long I waited so I just interrupted it üòÇüòÇüòÇ', 'response': 'so like 17 hrs'}\\n\",\n",
       " \"{'prompt': 'Ouchy, wait how is ur GPU billed?', 'response': 'i have my own gpu lol'}\\n\",\n",
       " \"{'prompt': 'Ohhhhh like u own it?', 'response': 'its like ever so slightly slower than the datahub ones i think? but like more reliable and i dont have to worry about my connection dropping'}\\n\",\n",
       " \"{'prompt': 'its like ever so slightly slower than the datahub ones i think? but like more reliable and i dont have to worry about my connection dropping', 'response': 'yea its sitting in my computer next to me rn lol'}\\n\",\n",
       " \"{'prompt': 'üòÇ that‚Äôs very cool', 'response': 'oh man i could never aws is so expensive'}\\n\",\n",
       " \"{'prompt': 'OH REALLY dang I thought they always sell themselves as a cheaper option or like pay as you go, no hidden fee, low cost etc', 'response': 'like idk i dont think id want to spend any extra money for class lol'}\\n\",\n",
       " \"{'prompt': 'like idk i dont think id want to spend any extra money for class lol', 'response': 'yea i think im just going to stop at 50'}\\n\",\n",
       " \"{'prompt': 'yea i think im just going to stop at 50', 'response': '300 is taking soo long ive gone through 3 epochs so far'}\\n\",\n",
       " \"{'prompt': '300 is taking soo long ive gone through 3 epochs so far', 'response': 'training 50 takes me about 1:10 per epoch'}\\n\",\n",
       " '{\\'prompt\\': \\'Yeah does take a while, I think datahub might be faster but not really too significant? I haven‚Äôt really timed it properly. Anyway, if we train the full 260 epochs each time, we can probably only do like 3-4 rounds per person per day (for the amount of time that we are awake üò¨\\', \\'response\\': \"if it takes around 2 hrs for datahub then its like twice as fast lol mb i\\'ll switch\"}\\n',\n",
       " '{\\'prompt\\': \"if it takes around 2 hrs for datahub then its like twice as fast lol mb i\\'ll switch\", \\'response\\': \"mb i\\'ll do runs on datahub and my personal gpu concurrently\"}\\n',\n",
       " \"{'prompt': 'I could be wrong cause I just start it and work on other stuffs üòÇüòÇ it could actually be 3-4 hours but yeah I think running on both is a good idea if that‚Äôs manageable', 'response': 'i just timed a single epoch and extrapolated'}\\n\",\n",
       " \"{'prompt': 'The last hyperparameter tuning will be ready shortly! 48 epochs left üò™', 'response': 'i have 200 epochs left on the rnn run and after that i‚Äôll be finished i think'}\\n\",\n",
       " \"{'prompt': 'I ran the rnn earlier actually! but you can verify the results with mine', 'response': 'oh even better'}\\n\",\n",
       " \"{'prompt': 'oh even better', 'response': 'ig ill do generation with different temperatures'}\\n\",\n",
       " \"{'prompt': 'ig ill do generation with different temperatures', 'response': 'and then fill out that part of the report'}\\n\",\n",
       " \"{'prompt': 'and then fill out that part of the report', 'response': 'i‚Äôll just take the model with the best loss?'}\\n\",\n",
       " \"{'prompt': 'it should be, but the numbers look kinda weird so i‚Äôm planning to rerun it overnight', 'response': 'i might have mistyped a parameter or smth'}\\n\",\n",
       " \"{'prompt': '@Ester Tsai is there anything that I should do to adjust the spacing..?', 'response': 'the loss ones?'}\\n\",\n",
       " \"{'prompt': 'the loss plots', 'response': 'ok will do'}\\n\",\n",
       " \"{'prompt': 'Okie thanks', 'response': 'i uploaded the dropout plots under figures, the sequence length is wrong so i‚Äôll update them by tmr morning but for now if anyone needs them they‚Äôre there'}\\n\",\n",
       " \"{'prompt': 'ok I push the best model to branch ester2, path is checkpoint/best_0219', 'response': 'tysm'}\\n\",\n",
       " \"{'prompt': 'not rly an explanation but heres the site where i think they stole the graphics from lol', 'response': 'mb simult many in many out doesnt work for translation bc u kinda need to know the full sentence before u can translate it'}\\n\",\n",
       " \"{'prompt': 'btw i think im done with my part too', 'response': 'what were the hyperparameters u used when training this model?'}\\n\",\n",
       " \"{'prompt': 'u can find it in this config. I remember the more general ones like 200 neurons, Lr 1e-3, dropout 0.1 and LSTM model', 'response': 'oh i didn‚Äôt see that message oops'}\\n\",\n",
       " \"{'prompt': 'oh i didn‚Äôt see that message oops', 'response': 'tyty'}\\n\",\n",
       " \"{'prompt': '@Jeremy Tow could you add your contributions at the end of the report?', 'response': 'yep right after i finish running the temperature expirements'}\\n\",\n",
       " \"{'prompt': 'thank you very much!', 'response': 'im almost done with my part, just generating a bunch of songs with different temperatures to get statistics'}\\n\",\n",
       " \"{'prompt': 'ooo were there any notes that the converter flagged as invalid or unrecognized notes?', 'response': 'so many'}\\n\",\n",
       " \"{'prompt': 'so many', 'response': 'so so many'}\\n\",\n",
       " \"{'prompt': 'so so many', 'response': 'the higher the temperature the more errors'}\\n\",\n",
       " \"{'prompt': 'AHAHHAHA', 'response': 'i tried a bunch of generations and at temp = 2 it was like multiple pages of errors'}\\n\",\n",
       " \"{'prompt': 'right, as it is more creative (or hallucinates more) haha', 'response': 'artists on drugs be like'}\\n\",\n",
       " '{\\'prompt\\': \\'okay i proofread the report\\', \\'response\\': \"ok my part is done, i\\'m just a little iffy on this one bit that i wrote\"}\\n',\n",
       " '{\\'prompt\\': \\'Can someone double check the requirement here? I might have understood it wrongly but it looks like they asked for the ABC notation (which we have) and the music representation/notation (which we don‚Äôt have at the moment)\\', \\'response\\': \"oh shoot yea i\\'ll add screenshots rn\"}\\n',\n",
       " '{\\'prompt\\': \"oh shoot yea i\\'ll add screenshots rn\", \\'response\\': \\'give me like 5 min\\'}\\n',\n",
       " \"{'prompt': 'In the submission', 'response': 'i feel like it kinda reads like we need the music representation tho'}\\n\",\n",
       " '{\\'prompt\\': \\'Ok thats fine\\', \\'response\\': \\'\"provide their abc notation and music representation from <url>\"\\'}\\n',\n",
       " '{\\'prompt\\': \\'\"provide their abc notation and music representation from <url>\"\\', \\'response\\': \\'i just missed it the first time i read it oop\\'}\\n',\n",
       " \"{'prompt': 'Oh yeah this is the thing that I‚Äôm not sure', 'response': 'i think they wanted it uploaded seperately? i vaguely remember reading about this somewhere'}\\n\",\n",
       " \"{'prompt': 'okok', 'response': 'ok i updated the abstract, also added in the pictures of the music'}\\n\",\n",
       " '{\\'prompt\\': \\'All good! Just submitted\\', \\'response\\': \"the pa just came out -- i\\'ve created a team called transformative\"}\\n',\n",
       " \"{'prompt': '@Jeremy Tow would you like to push your code to your branch üòÆ I‚Äôm having a hard time making model.py work oop haha', 'response': 'oh ok yea i‚Äôll push mine but it has low acc'}\\n\",\n",
       " \"{'prompt': 'oh ok yea i‚Äôll push mine but it has low acc', 'response': 'ok pushed'}\\n\",\n",
       " '{\\'prompt\\': \"yeah :0  it\\'ll need to be under 200\", \\'response\\': \\'oh oop\\'}\\n',\n",
       " \"{'prompt': 'Actually the code on ester1 has some odd issue rn, but Jeremy will push his working code soon', 'response': 'the code on the ester1 branch works fine for me lol'}\\n\",\n",
       " \"{'prompt': 'What command do you run on the terminal', 'response': 'mb try a different server?'}\\n\",\n",
       " \"{'prompt': 'mb try a different server?', 'response': 'i pushed the code btw'}\\n\",\n",
       " \"{'prompt': '@Ester Tsai @Jeremy Tow roughly how long does it take to run?', 'response': '< 1 min per epoch'}\\n\",\n",
       " \"{'prompt': 'also confirmed this. Yesterday when I ran it for loss was 800+ at some epoch < 10, today I ran the same branch and loss is 2706 after 50 epochs', 'response': 'uhhhhh wait let me run it a few times locally'}\\n\",\n",
       " \"{'prompt': 'uhhhhh wait let me run it a few times locally', 'response': 'cuz i don‚Äôt think i have this issue'}\\n\",\n",
       " \"{'prompt': 'ooo okok', 'response': 'i just ran it twice and i do not have this issue'}\\n\",\n",
       " '{\\'prompt\\': \"this doesn\\'t work. I still get validation acc: 0.06443679291687161 for every epocj\", \\'response\\': \\'is it possible to use the captive machines?\\'}\\n',\n",
       " \"{'prompt': 'is it possible to use the captive machines?', 'response': 'capstone'}\\n\",\n",
       " \"{'prompt': 'what command line prompt do you use? @Jeremy Tow', 'response': 'i tried the one that you sent'}\\n\",\n",
       " '{\\'prompt\\': \\'Also I don\\\\\\'t wanna see you guys lose points for not coding! I think it\\\\\\'s worth arguing that you guys weren\\\\\\'t able to code for PA3 because of unfixable cuDNN version incompatibility issue, and that you will make sure to code for PA4. This is what our prof wrote for PA3 feedback: \"Hi folks - Based on your description of who did what, it sounds like Jeremy just did hyperparameter tuning, Jonathan only helped with analysis, and Samuel just helped write the report. You know that we expect everyone to code! Please argue with me in a regrade request as to why I should not give Samuel 50% of the points, Jonathan 70%, and Jeremy 80% of the points. In general, I highly recommend using pair or triple programming, trading of who is the driver to avoid this in the future.\"\\', \\'response\\': \\'oh ummmm that‚Äôs kind awkward i kinda did the baseline too I just didn‚Äôt want to write that in my contributions cuz my work for that didn‚Äôt show up in the report\\'}\\n',\n",
       " '{\\'prompt\\': \"you can argue for that! It\\'s ok if it didn\\'t show up in the report\", \\'response\\': \\'uhh shuld i directly send an email or should we create a doc or smth and then send everything all at once\\'}\\n',\n",
       " '{\\'prompt\\': \\'We just need to send one reply through Gradescope regrade request. Would you like to start a doc?\\', \\'response\\': \"ok yea i\\'ll make it\"}\\n',\n",
       " '{\\'prompt\\': \\'ok heres the link\\', \\'response\\': \"ok my bit is finished. i don\\'t really know about the issue that you guys faced with cuDNN, so I couldn\\'t really write about it\"}\\n',\n",
       " \"{'prompt': 'Hello hello, to plan our tasks and time for the rest of this week, is anyone planning to work on or currently working on part 4 or part 5?', 'response': 'i‚Äôll work on part 5'}\\n\",\n",
       " \"{'prompt': 'i‚Äôll work on part 5', 'response': 'oh btw @Samuel Chu do you have anything to add to the regrade request? if not should I submit it?'}\\n\",\n",
       " \"{'prompt': 'Any progress on part 5?', 'response': 'nothing significant yet, gonna work on it later tonight'}\\n\",\n",
       " \"{'prompt': 'ester1 branch has the newest updates', 'response': 'oh oop im still working on it rn'}\\n\",\n",
       " \"{'prompt': 'oh oop im still working on it rn', 'response': 'oh btw did we submit the regrade request yet and if not shuld i do it'}\\n\",\n",
       " \"{'prompt': 'I can check', 'response': 'tyty'}\\n\",\n",
       " \"{'prompt': 'Wowwwww', 'response': 'ur actually amazing'}\\n\",\n",
       " \"{'prompt': 'better sanity check', 'response': 'when i finally figure out how to do it individually i‚Äôll also check ig'}\\n\",\n",
       " \"{'prompt': 'when i finally figure out how to do it individually i‚Äôll also check ig', 'response': 'but it actually putting in so much work it‚Äôs crazy tysm'}\\n\",\n",
       " '{\\'prompt\\': \\'Thank you!\\', \\'response\\': \"i\\'ll run it, i need a break anyways\"}\\n',\n",
       " \"{'prompt': 'Has anyone read the SimCLR slides?', 'response': 'i‚Äôve read the paper'}\\n\",\n",
       " \"{'prompt': 'i‚Äôve read the paper', 'response': 'i‚Äôm in class rn but i‚Äôll update after i fj ish'}\\n\",\n",
       " \"{'prompt': 'Or maybe how does the classifier know which label is the most popular ?', 'response': 'oh right i forgot to mention this but im prob going to submit the regrade request later today'}\\n\",\n",
       " '{\\'prompt\\': \\'I went to his before! and I remember the session I went wasn‚Äôt very helpful too\\', \\'response\\': \"oh shoot sorry i\\'ve been afk for most of today. i jst added a bit to the related works and i can help do runs as well\"}\\n',\n",
       " '{\\'prompt\\': \"oh shoot sorry i\\'ve been afk for most of today. i jst added a bit to the related works and i can help do runs as well\", \\'response\\': \\'which exact ones need reruns?\\'}\\n',\n",
       " \"{'prompt': 'yes, and the combined', 'response': 'okok i will run those'}\\n\",\n",
       " '{\\'prompt\\': \"I also haven\\'t gotten to run any custom1 and custom3 commands\", \\'response\\': \"i\\'ll run the custom3 commands\"}\\n',\n",
       " \"{'prompt': 'Nice nice mine is still running and ntg is higher so far', 'response': '^ yea same'}\\n\",\n",
       " \"{'prompt': 'updated github main readme', 'response': 'the best i got was all 0.88 so yea'}\\n\",\n",
       " \"{'prompt': 'the best i got was all 0.88 so yea', 'response': 'ok I edited the introduction + expanded on the discussion section and finished the related works section'}\\n\",\n",
       " \"{'prompt': 'I don‚Äôt get it lol, our un-tuned model is not predicting randomly', 'response': 'mb our model is plainly just better'}\\n\",\n",
       " \"{'prompt': 'mb our model is plainly just better', 'response': 'built different'}\\n\",\n",
       " \"{'prompt': 'seed of 2', 'response': 'oh okok'}\\n\",\n",
       " \"{'prompt': 'What matters is that run_eval comes before baseline_train', 'response': 'it might have been random chance'}\\n\",\n",
       " \"{'prompt': 'it might have been random chance', 'response': 'we can prob do some statistical analysis to see how probable that is?'}\\n\",\n",
       " \"{'prompt': 'So close enough??', 'response': 'im running stats let me give u numbers in a sec'}\\n\",\n",
       " \"{'prompt': 'im running stats let me give u numbers in a sec', 'response': 'im going to run it 100 times and get means + stddevs'}\\n\",\n",
       " \"{'prompt': 'our seed was actually just built different', 'response': 'two percent chance'}\\n\",\n",
       " \"{'prompt': 'üòÇüòÇüòÇüòÇ', 'response': 'mean: 0.01862474781439139 std: 0.015202989747957751'}\\n\",\n",
       " \"{'prompt': 'mean: 0.01862474781439139 std: 0.015202989747957751', 'response': '@Jonathan Cheung you can put these numbers into the report'}\\n\",\n",
       " \"{'prompt': '@Jonathan Cheung you can put these numbers into the report', 'response': 'over 100 different seeds'}\\n\",\n",
       " \"{'prompt': 'Ready for submit?', 'response': 'for my parts yea'}\\n\",\n",
       " '{\\'prompt\\': \\'This is super useful info!\\', \\'response\\': \"@Winfrey Kong@Ester Tsai for the photo focal length idea we talked about earlier in person: 24-30 mm: 3k 30-40 mm: 6k 40-55 mm: 2k 55-70 mm: 6k > 70 mm: 400  This is just the number of photos taken with my current camera so I have a lot more but they\\'re harder to find stats on rn. I think it might be better to pull photos from the internet so there is more variety, and maybe supplement that data with mine, if we do go through with this idea  link to github repo: \"}\\n',\n",
       " \"{'prompt': 'Lol thanks', 'response': 'oh thats a good idea. i think the main thing we would have to figure out would be how to make it different tho'}\\n\",\n",
       " '{\\'prompt\\': \\'Also, just saying that, we are not set set on the idea so feel free to explore other ideas and we can go over all ideas together tomorrow\\', \\'response\\': \"i feel like it\\'d be pretty easy to find code for this idea, so we\\'d need to apply it to a new dataset or a different problem. idk if just using a different set of text messages for training would be sufficient to get past the tas lol\"}\\n']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'Jeremy'\n",
    "create_individual_data(groupchat, name, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a88f90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
